https://galax.dev/posts/00_lr_scheduler_from_scratchhttps://galax.dev/apps.htmlhttps://galax.dev/blog.htmlhttps://galax.dev/posts/00_lr_scheduler_from_scratch.htmlhttps://galax.dev/index.htmlhttps://galax.dev/posts/00_Image-to-Image.html***URL: https://viktoranchutin.github.io/blog.html***

Engineering notes - Blog Spark RDD with distributed machine learning
Understanding Spark computational model with logistic regression and
clustering Applying vision models to audio data Fine-tuning ViTs and ConvNets
on spectrogram data Gradient accumulation Toy example for gradient
accumulation understanding Training with variable length data Building a
dataloader to train deep learning models on variable length data No matching
items

***URL: https://viktoranchutin.github.io/blog/variable_length_training.html***

Engineering notes - Training with variable length data There are several
different ways we can deal with variable length data when training deep
learning models: Cut or pad all the samples to the maximum length in the whole
dataset Cut or pad samples to the maximum length within a mini-batch Split the
dataset into multiple buckets with samples of similar length. I will describe
the third option as it imposes the least memory and computational overhead.
This option can be used to train CNNs, RNNs or transformers with relative
positional encoding, since they can be trained on variable length data. For
example we can train Wav2Vec 2.0 model with audio samples of different length
as it encodes audio with CNN and is using convolutional relative positional
encoding as well. 1\. Splitting the data Generating dataset with variable
length items. from random import randint import torch import pandas as pd #
generate dataset min_length = 2 max_length = 20 size = 1000 dummy_y = 0
dataset = [(torch.randn(randint(min_length,max_length)),dummy_y) for _ in
range (size)] Create a dataframe with information about items length df =
pd.DataFrame([( id , len (x)) for id ,(x,y) in enumerate (dataset)], columns =
[ 'id' , 'length' ]) df.length.plot(kind = 'hist' ,title = "Length
distribution" ) ; Split data into bukets nbuckets = 10 df[ 'bucket' ] =
pd.cut(df.length, bins = nbuckets, labels = range (nbuckets)) 2\. Create
dataloaders Create DataSet class, which is using a dataframe with items ids to
retrieve them from the original dataset. from torch.utils.data import
DataLoader from torch.nn.functional import pad class DataSet: def __init__ (
self ,dataframe,data): self .df = dataframe.reset_index(drop = True ) # items
ids self .data = data def __getitem__ ( self ,index): id = self
.df.iloc[index]. id # get item by id from the original dataset return self
.data[ id ] def __len__ ( self ): return len ( self .df) Collate function adds
padding according to the maximum length in a batch def collate_fn(batch):
xs,ys = [ list (b) for b in ( zip ( * batch))] maxl = max ([ len (x) for x in
xs]) # maxl in a batch for i in range ( len (xs)): xs[i] = pad(xs[i],( 0 ,maxl
\- len (xs[i]))) # pad to maxl x = torch.stack(xs) y = torch.tensor(ys) return
(x,y) Create dataloaders for each bucket def create_dataloader(dataframe,bs =
4 ): return DataLoader(DataSet(dataframe, dataset), bs, shuffle = True ,
collate_fn = collate_fn) dataloaders = [] for bucket_id in df.bucket.unique():
dl = create_dataloader(df[df.bucket == bucket_id]) dataloaders.append(dl) 3\.
Create random iterator The iterator takes iterators from the dataloaders and
randomly chooses one at the each next call from random import choice class
DLIterator: def __init__ ( self , dls) -> None : self .iters = [ iter (dl) for
dl in dls] def __iter__ ( self ): return self def __next__ ( self ): for _ in
range ( len ( self .iters)): # iterate in case some are empty try : it =
choice( self .iters) return next (it) except StopIteration : self
.iters.remove(it) raise StopIteration class MultiDataLoader: '''Combining
multiple dataloaders.''' def __init__ ( self ,dataloaders) -> None : self .dls
= dataloaders def __iter__ ( self ): return DLIterator( self .dls) def __len__
( self ): return sum ( map ( len , self .dls)) Check the distribution of batch
lengths for the obtained dataloader import matplotlib.pyplot as plt
batch_sizes = [xb.shape[ 1 ] for xb,_ in MultiDataLoader(dataloaders)]
plt.hist(batch_sizes) ; Visualize batch lengths: it = iter
(MultiDataLoader(dataloaders)) _,ax = plt.subplots( 5 ) for i in range ( 5 ):
ax[i].imshow( next (it)[ 0 ])

***URL: https://viktoranchutin.github.io/index.html***

Engineering notes - Viktor Anchutin LinkedIn GitHub Resume About Passionate
software engineer with a love for machine learning, data-driven applications
and robots I have 2 master’s degrees: Data Science and Robotics. Committed to
learning and growing in the field of AI I started as a robotics software
developer, using C and C++, and later transitioned to backend engineering with
Kotlin/Java. I’ve played a key role in building various data-intensive
systems, showcasing my skills in system thinking and design. Blog Projects No
matching items

***URL: https://viktoranchutin.github.io/projects.html***

Engineering notes - Projects Detecting active emotions in speech Training 5
different models to perform active emotion recognition from speech ResNet from
scratch for rice disease classification Building a custom ResNet model using
Pytorch Heart disease prediction Data analysis, Random Forest and Logistic
Regression with heart disease dataset Controlling magnetic field in electric
motors How I built BLDC motor control system from scratch in my Robotics
masters degree No matching items

***URL: https://viktoranchutin.github.io/projects/motor_control.html***

Engineering notes - Controlling magnetic field in electric motors During my
time in Robotics master’s program I worked on building software to control
electric motors position. All projects were built in C and C++. Some hardware
parts had to be developed from scratch as well. The basic elements of a system
were: Microcontroller (STM32) . Executing software to control magnetic field.
(STM32) Hardware module (Driver) . Performing commands from the
microcontroller for controlling voltage. Magnetic encoder . Measuring position
of the rotor and sending it back to the microcontroller. Motor (BLDC) By the
end of my master’s degree I had built various systems from controlling a
single motor to controlling a feedback loop stabilization system. The first
project implemented vector control or also called field oriented control (FOC)
in C language. Later I reimplemented it in C++. One of the fun things to
implement was slow rotation of the motor, which was done using sinusoidal
voltage control , meaning that the magnetic field was slowly and gradually
rotated. Eventually the robotics club in my university built a platform with a
more powerful motor from a hoverboard, and I implemented the same system for
it as well. In my last year I put all the pieces together and built a
stabilization platform. It used a gyroscope and accelerometer to measure its
position. Finally I gave a seminar about motor control algorithms

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://galax.dev/apps.html***

galax.dev - apps playground Logo Header Categories All (0) afterhoursbilly
GitHub Twitter [email protected] No matching items Back to top

***URL: https://galax.dev/index.html***

galax.dev My name is Szymon and, I am an aspiring Machine/Deep Learning
Engineer, currently in my third year of Computer Science. If you are already
here, you can check out my blog posts and demo apps in the playground below.
You can find most of the code for my projects on my GitHub. blog Click here to
check out the all blog posts. LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Nov 13, 2023 An Image-to-Image
Implementation Demonstation of an image-to-image implementation of the Stable
Diffusion model. Oct 3, 2023 No matching items playground Click here to play
more in the playground. No matching items Contact Information afterhoursbilly
GitHub Twitter [email protected]

***URL: https://galax.dev/posts/00_Image-to-Image.html***

galax.dev - An Image-to-Image Implementation Text-Guided: Image-to-Image
Implementation This Python code demonstrates the implementation of the Image-
to-Image technique, allowing you to generate new images from existing ones
with the help of textual prompts. Explore how this innovative approach
combines images and text to create visually compelling artworks. Dive into the
code to understand the mechanics behind this cutting-edge image generation
technique. Pip install necessary libraries (click to show/hide) ! pip install
\- Uq diffusers transformers fastcore fastdownload Importing utilities (click
to show/hide) from transformers import CLIPTextModel, CLIPTokenizer import
torch from diffusers import LMSDiscreteScheduler from PIL import Image from
tqdm.auto import tqdm from diffusers import AutoencoderKL,
UNet2DConditionModel import logging from fastdownload import FastDownload from
pathlib import Path from huggingface_hub import notebook_login import
matplotlib.pyplot as plt from torchvision import transforms if not
(Path.home() / '.cache/huggingface' / 'token' ).exists(): notebook_login()
logging.disable(logging.WARNING) We need to load in the required libraries and
set up the models. tokenizer = CLIPTokenizer.from_pretrained( "openai/clip-
vit-large-patch14" , torch_dtype = torch.float16) text_encoder =
CLIPTextModel.from_pretrained( "openai/clip-vit-large-patch14" , torch_dtype =
torch.float16).to( "cuda" ) # Here we use a different VAE to the original
release, which has been fine-tuned for more steps vae =
AutoencoderKL.from_pretrained( "stabilityai/sd-vae-ft-ema" , torch_dtype =
torch.float16).to( "cuda" ) unet = UNet2DConditionModel.from_pretrained(
"CompVis/stable-diffusion-v1-4" , subfolder = "unet" , torch_dtype =
torch.float16).to( "cuda" ) Define the parameters. height = 512 width = 512
num_inference_steps = 70 guidance_scale = 7.5 batch_size = 1
beta_start,beta_end = 0.00085 , 0.012 scheduler =
LMSDiscreteScheduler(beta_start = beta_start, beta_end = beta_end,
beta_schedule = "scaled_linear" , num_train_timesteps = 1000 )
plt.plot(scheduler.sigmas) plt.title( 'Noise Schedule' ) plt.xlabel( 'Sampling
step' ) plt.ylabel( 'sigma' ) plt.show() def prep_img(img_link : str ) ->
torch.Tensor: """ Preprocesses an image from a given link. Args: img_link
(str): The URL or path to the image file. Returns: torch.Tensor: A tensor
representing the preprocessed image. """ p = FastDownload().download(img_link)
init_image = Image. open (p).convert( "RGB" ).resize(( 512 , 512 )) return
transforms.ToTensor()(init_image) The image we will use as a starting point.
Downloading the image (click to show/hide) link = "https://cdn-
uploads.huggingface.co/production/uploads/1664665907257-noauth.png"
transformed_image = prep_img(link) # show image in notebook. p =
FastDownload().download(link) display(Image. open (p).convert( "RGB"
).resize(( 512 , 512 ))) def tokenization(prompt: list , max_len : int = None
) \- > torch.Tensor: """ Tokenizes a text prompt and returns the corresponding
encoded tensor. Args: prompt (list): The input text prompt to be tokenized.
max_len (int, optional): The maximum length of the tokenized sequence. If not
specified, it defaults to the maximum length allowed by the tokenizer.
Returns: torch.Tensor: A tensor containing the encoded representation of the
tokenized prompt. """ if max_len is None : max_len =
tokenizer.model_max_length tokenized_prompt = tokenizer(prompt, padding =
"max_length" , max_length = max_len, truncation = True , return_tensors = 'pt'
) return text_encoder(tokenized_prompt.input_ids.to( 'cuda' ))[ 0 ].half() def
make_image(latent: torch.Tensor): """ Converts a tensor representation of an
image into a PIL Image. Args: latent (torch.Tensor): A tensor representing an
image. Returns: PIL.Image.Image: A PIL Image representing the image. """ image
= (latent / 2 \+ 0.5 ).clamp( 0 , 1 ).detach().cpu().permute( 1 , 2 , 0
).numpy() return Image.fromarray((image * 255 ). round ().astype( "uint8" ))
Denoising loop To ensure the effectiveness of this solution, it is essential
to incorporate the “start_step” parameter. Essentially, we aim to prevent
excessive noise from being added to the input image, particularly avoiding the
most intense noise additions. After this initial step, we can proceed with the
looping process. In summary, the key to success here is to introduce the
“start_step” parameter, which helps us avoid excessive noise in the early
stages and then continue with the loop as intended. def create_sample(prompt:
list ,transformed_image: torch.Tensor ,guidance_scale: float = 7.5 , seed: int
= 5 , steps: int = 70 ,start_step: int = 10 ): ''' Generate a sample image
based on a text prompt, provided image and guidance parameters. Args: prompt
(list): A list of text prompts. transformed_image (torch.Tensor): A tensor
representing the transformed image. guidance_scale (float, optional): The
scale factor for guiding the generation process. seed (int, optional): Seed
for random number generation. Default is 5. steps (int, optional): The total
number of steps for the generation process. Default is 70. start_step (int,
optional): The step at which the generation process starts. Default is 10.
Returns: torch.Tensor: A tensor representing the generated sample. This
function generates an image based on the provided text prompts , transformed
image and parametrs.It uses a predefined VAE model to encode the image and
then applies noise and guidance to generate the sample.It iteratively refines
the image by adding noise and updating the latent representation. The guidance
scale controls the influence of the text prompts on the image. The generated
image is returned as a PyTorch tensor. Example: >>> prompt = ["Translate the
following English sentence to French: 'Hello, how are you?'"] >>>
transformed_image = prep_img(image_link) >>> generated_sample =
create_sample(prompt, transformed_image) ''' bs = 1 # Implementation for only
a single prompt. text = tokenization(prompt) uncond = tokenization([ "" ] *
bs, text.shape[ 1 ]) emb = torch.cat([uncond, text]) if seed:
torch.manual_seed(seed) # Encode image image_latent =
vae.encode((transformed_image.unsqueeze( 0 ).half().to( 'cuda'
))).latent_dist.sample() image_latent = vae.config.scaling_factor *
image_latent # Create noise scheduler.set_timesteps(steps) noise_latents =
torch.randn_like(image_latent) latents = scheduler.add_noise(image_latent,
noise_latents, timesteps = torch.tensor([scheduler.timesteps[start_step]]))
for i, ts in enumerate (tqdm(scheduler.timesteps)): if i >= start_step: # Skip
the batches of noise that don't affect the input image. inp =
scheduler.scale_model_input(torch.cat([latents] * 2 ), ts) with
torch.no_grad(): noise_pred_uncond, noise_pred_text = unet(inp, ts,
encoder_hidden_states = emb).sample.chunk( 2 ) pred = noise_pred_uncond \+
guidance_scale * (noise_pred_text \- noise_pred_uncond) latents =
scheduler.step(pred, ts, latents).prev_sample with torch.no_grad(): return
vae.decode( 1 / 0.18215 * latents).sample prompt = [ 'Wolf howling at the
moon, photorealistic 4K' ] #prompt = ['unicorn'] #prompt = ['a kids drawing of
bacteria, cartoon style'] #prompt = [' Horse looking at the morning sun,
photorealistic 4K'] image = create_sample(prompt,transformed_image,steps = 50
,seed = 1000 ) display(make_image(image[ 0 ])) Back to top

***URL: https://galax.dev/blog.html***

galax.dev - blog Categories All (2) Deep Learning (2) Implementation (2)
Stable Diffusion (1) afterhoursbilly GitHub Twitter [email protected] Order By
Default Date - Oldest Date - Newest LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Monday, 13 November 2023 2 min An Image-
to-Image Implementation Demonstation of an image-to-image implementation of
the Stable Diffusion model. Tuesday, 03 October 2023 1 min No matching items
Back to top

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch.html***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://viktoranchutin.github.io/blog.html***

Engineering notes - Blog Spark RDD with distributed machine learning
Understanding Spark computational model with logistic regression and
clustering Applying vision models to audio data Fine-tuning ViTs and ConvNets
on spectrogram data Gradient accumulation Toy example for gradient
accumulation understanding Training with variable length data Building a
dataloader to train deep learning models on variable length data No matching
items

***URL: https://viktoranchutin.github.io/projects.html***

Engineering notes - Projects Detecting active emotions in speech Training 5
different models to perform active emotion recognition from speech ResNet from
scratch for rice disease classification Building a custom ResNet model using
Pytorch Heart disease prediction Data analysis, Random Forest and Logistic
Regression with heart disease dataset Controlling magnetic field in electric
motors How I built BLDC motor control system from scratch in my Robotics
masters degree No matching items

***URL: https://viktoranchutin.github.io/projects/motor_control.html***

Engineering notes - Controlling magnetic field in electric motors During my
time in Robotics master’s program I worked on building software to control
electric motors position. All projects were built in C and C++. Some hardware
parts had to be developed from scratch as well. The basic elements of a system
were: Microcontroller (STM32) . Executing software to control magnetic field.
(STM32) Hardware module (Driver) . Performing commands from the
microcontroller for controlling voltage. Magnetic encoder . Measuring position
of the rotor and sending it back to the microcontroller. Motor (BLDC) By the
end of my master’s degree I had built various systems from controlling a
single motor to controlling a feedback loop stabilization system. The first
project implemented vector control or also called field oriented control (FOC)
in C language. Later I reimplemented it in C++. One of the fun things to
implement was slow rotation of the motor, which was done using sinusoidal
voltage control , meaning that the magnetic field was slowly and gradually
rotated. Eventually the robotics club in my university built a platform with a
more powerful motor from a hoverboard, and I implemented the same system for
it as well. In my last year I put all the pieces together and built a
stabilization platform. It used a gyroscope and accelerometer to measure its
position. Finally I gave a seminar about motor control algorithms

***URL: https://viktoranchutin.github.io/index.html***

Engineering notes - Viktor Anchutin LinkedIn GitHub Resume About Passionate
software engineer with a love for machine learning, data-driven applications
and robots I have 2 master’s degrees: Data Science and Robotics. Committed to
learning and growing in the field of AI I started as a robotics software
developer, using C and C++, and later transitioned to backend engineering with
Kotlin/Java. I’ve played a key role in building various data-intensive
systems, showcasing my skills in system thinking and design. Blog Projects No
matching items

***URL: https://viktoranchutin.github.io/blog/Applying_vision_models_to_audio_classification.html***

Engineering notes - Applying vision models to audio data Several research have
demonstrated that vision models pretrained on large datasets of images can be
successfully applied for audio classification tasks. Both vision transformers
and convolutional neural networks. Vision transformers The are 3 main modules
in the vision transformer which are important for transfer learning: The
embedding module . Transforms images to a sequence of vector embeddings
Transformer encoder module . Contains stacked transformer encoder layers. Head
. Computes the final prediction from the obtained representation vector.
Adjusting the embedding layer Embedding layer processing: Split an image into
patches Compute patch embeddings Add position encoding Computing patch
embeddings To compute an embedding from a patch, vision transformer applies a
convolution operation with 3 input channels and the number of output channels
equal to the specified embedding dimension. Here’s how it is done in the timm
library, from the timm.layers.PatchEmbed class: self .proj =
nn.Conv2d(in_chans, embed_dim, kernel_size = patch_size, stride = patch_size,
bias = bias) When we work with spectrograms we have only 1 channel of
information and that’s why the embeddings layer has to be adjusted. The
adjustment can be done by copying spectrogram channel 2 times, averaging or
taking a sum of the input channels of the embedding layer. Timm library sums
the weights of the convolution channels. From the adapt_input_conv function in
the timm.models._manipulate : conv_weight = conv_weight. sum (dim = 1 ,
keepdim = True ) The same is happening in the Audio Spectrogram Transformer :
new_proj.weight = torch.nn.Parameter(torch. sum ( self
.v.patch_embed.proj.weight, dim = 1 ).unsqueeze( 1 )) Adjusting position
embeddings Position embeddings added to encode spatial information about the
location of each patch in the image. Visual transformer models are pretrained
on a fixed size images, which means they learn positional embeddings of a
fixed size as well. To apply it to a different size images or spectrograms,
position embeddings need to be somehow adjusted to transfer the learned
relationships to different resolutions. A popular solution - 2D interpolation.
From the original ViT paper , ‘Fine-tuning and higher resolution’ section:
Timm library also implements interpolation. From the github issue discussion :
From the timm.layers._pos_embed.resample_abs_pos_embed : posemb =
F.interpolate(posemb, size = new_size, mode = interpolation, antialias =
antialias) Replacing the head The final adjustment is simply changing the
final layer for the specific task. The head layer in ViT consumes CLS token.
So the new head should have the input dimension equal to the CLS token
dimension (or just embeddings dimension). * DeiT models have 2 CLS tokens, so
there are 2 heads to adjust. Example Here’s how we can get a pretrained ViT
model with adjusted embedding layer and the head layer for further fine-
tunung: m = timm.create_model( 'vit_base_patch16_224' , img_size = size,
pretrained = True , in_chans = 1 , num_classes = num_classes) CNNs For CNNs
the process is mostly the same except they can work with variable size inputs,
so we only need to adjust the number of input channels and the head. When we
choose in_chans = 1 in timm library, under the hood adapt_input_conv function
is called to adjust input layer as it was shown earlier: conv_weight =
conv_weight.sum(dim=1, keepdim=True) m = timm.create_model( 'convnext_tiny' ,
pretrained = True , in_chans = 1 , num_classes = num_classes)

***URL: https://viktoranchutin.github.io/blog/Spark RDD.html***

Engineering notes - Spark RDD with distributed machine learning In this blog
post I take a close look into the Spark computational model by implementing 2
machine learning algorithms: Logistic Regression and K-Means clustering. The
code is executed in the Databricks environment using Scala. Python is used for
visualization. Databricks notebook Spark was developed to address iterative
big data algorithms like logistic regression (gradient descent) or k-means
clustering. From the RDDs paper : K-Means clustering Data For K-means
clustering let’s generate 5 clusters of data. Generate data (Python) import
matplotlib.pyplot as plt from sklearn.datasets import make_blobs import numpy
as np import csv n_samples = 10000 n_features = 2 n_clusters = 5 data, labels
= make_blobs(n_samples = n_samples, n_features = n_features, centers =
n_clusters, random_state = 12345 ) def plot_centroids(centroids,ax): for i,
centroid in enumerate (centroids): samples = data[i * n_samples:(i \+ 1 ) *
n_samples] ax.plot( * centroid, markersize = 10 , marker = "x" , color = 'k' ,
mew = 5 ) ax.plot( * centroid, markersize = 5 , marker = "x" , color = 'm' ,
mew = 2 ) _,ax = plt.subplots() ax.scatter(data[:, 0 ], data[:, 1 ], c =
labels, cmap = 'viridis' ) centroids = [] for cluster_label in range
(n_clusters): cluster_points = data[labels == cluster_label] # Select data
points in the current cluster cluster_centroid = np.mean(cluster_points, axis
= 0 ) # Calculate the centroid for the cluster
centroids.append(cluster_centroid) plot_centroids(centroids,ax) print
(centroids) #save data with open ( 'data.csv' , 'w' , newline = '' ) as file :
writer = csv.writer( file ) writer.writerows(data) [array([ 8.57032532,
-3.64137432]), array([-6.34270089, -5.91962725]), array([1.3294662 ,
1.87901003]), array([9.31274039, 3.05338878]), array([4.96798439,
3.09705577])] Let’s read the data and create an RDD of data points, Also
important to make sure that RDD of data points will be cached, so we don’t
need to recompute it for each iteration. import scala . io . Source def
getPointsRDD (): RDD [( Float , Float )] = { val source = Source . fromFile (
"data.csv" ) val linesRDD = sc . parallelize ( source . getLines (). toList )
return linesRDD . map ( _ . split ( ',' ). map ( _ . toFloat )). map ({ case
Array ( x , y ) => ( x , y )}) } val points = getPointsRDD (). cache () We can
see that data points were split by Spark in 8 partitions: points .
getNumPartitions res5: Int = 8 Initialize centroids The first step of k-means
clustering is to initialize the first estimates of the centroids. For this
example I randomly sample 5 points, but in real applications this
initialization step usually involves more sophisticated sampling. val
randomMeans = points . takeSample ( withReplacement = false , num = 5 , seed =
10 ) save centroids for plotting import java . io . PrintWriter def saveResult
( means : Array [( Float , Float )]) = { val writer = new PrintWriter (
"means.txt" ) try { means . foreach ( writer . println ) } finally { writer .
close () } } saveResult ( randomMeans ) Plot centroids (Python) # Open the
file in read mode def readResults() -> list : with open ( 'means.txt' , 'r' )
as file : lines = file .readlines() return [ tuple ( map ( float , line.strip(
'() \n ' ).split( ',' ))) for line in lines] def plot_results(centroids,ax):
for i, centroid in enumerate (centroids): samples = data[i * n_samples:(i \+ 1
) * n_samples] ax.plot( * centroid, markersize = 5 , marker = "*" , color =
'r' , mew = 5 ) first_state = readResults() _,ax = plt.subplots()
plot_results(first_state,ax) plot_centroids(centroids,ax) ax.set_title(
'Initial centroids against true centroids' ) Out[5]: Text(0.5, 1.0, 'Initial
centroids against true centrids') Define computational graph Let’s implement
k-means clustering. The main function update \- conceptually performs 2 steps:
Groups the points by the closest centroids Finds the centre of the groups,
effectively obtaining the new estimate for centroids def euclideanDistance (
v1 : ( Float , Float ), v2 : ( Float , Float )): Double = ( v1 . _1 \- v2 . _1
) * ( v1 . _1 \- v2 . _1 ) \+ ( v1 . _2 \- v2 . _2 ) * ( v1 . _2 \- v2 . _2 )
/** Return the center that is the closest to ` p ` */ def findClosest ( p : (
Float , Float ), centers : Array [( Float , Float )]): ( Float , Float ) =
centers . minBy ( euclideanDistance ( _ , p )) def updateMeans ( means : Array
[( Float , Float )], points : RDD [( Float , Float )]): Array [( Float , Float
)] = return points . map ( point => ( findClosest ( point , means ), point ))
// pair (closest mean, point) . mapValues ( point => ( point , 1 )) // add
counter for aggregation . reduceByKey ({ case (( p1 , cnt1 ),( p2 , cnt2 )) =>
(( p1 . _1 \+ p2 . _1 , p1 . _2 \+ p2 . _2 ), cnt1 \+ cnt2 )}) // sum all the
points around a centroid . mapValues ({ case ( sum , count ) => ( sum . _1 /
count , sum . _2 / count )}) // average aggregated points -> new centroid .
map ({ case ( oldMean , newMean ) => newMean }) . collect () Running the
algorithm Let’s run 10 iterations and look at the result. var means =
randomMeans for ( i <\- 0 to 10 ){ means = updateMeans ( means , points ) }
save results for plotting saveResult ( means ) The algorithm successfully
found true centroids of clusters. Plot results (Python) results =
readResults() _,ax = plt.subplots( 1 , 3 ,figsize = ( 10 , 3 ))
plot_results(results,ax[ 0 ]) plot_centroids(centroids,ax[ 0 ])
plot_results(results,ax[ 1 ]) plot_centroids(centroids,ax[ 2 ]) ax[ 0
].set_title( 'True centroids and estimations' ) ax[ 1 ].set_title(
'Estimations' ) ax[ 2 ].set_title( 'True centroids' ) Out[5]: Text(0.5, 1.0,
'True centroids') Execution analysis First, Spark builds a graph of
computations and only when we call action functions such as .collect() it
executes the graph. Let’s look at the diagram of the execution process. Spark
driver creates closures with centroids and sends them to the executors.
Executors apply closures received by the driver to the partitions. From the
spark paper : “..users provide arguments to RDD operations like map by passing
closures (function literals). Scala represents each closure as a Java object,
and these objects can be serialized and loaded on another node to pass the
closure across the network. Scala also saves any variables bound in the
closure as fields in the Java object.” First each partition of points is
transformed to the pairs of points and the corresponding closest centroid.
Then we have reduceByKey followed by shuffle and the average. It is
conceptually the same as grouping the points by key and taking the average,
but computationally more optimal. If we used groupByKey, then the shuffle
operation would have to send 10000 points over the network in the worst case.
With reduceByKey operation, reduction happens before shuffle occurs,
significantly reducing the amount of data to send. In this case for each
cluster data points are reduced to a single pair of sum and counts, which
means that at most 5 (number of clusters) * 8 (number of partitions) = 40
pairs would need to be sent over the network. The obtained centroids are then
sent back to the driver. Next iteration driver sends updated centroids back to
the executors for recomputation. Logistic regression Data Let’s generate data
for binary classification problem generate data (Python) import numpy as np
import matplotlib.pyplot as plt from sklearn.datasets import
make_classification X, y = make_classification(n_samples = 10000 , n_features
= 2 , n_informative = 2 , n_redundant = 0 , n_clusters_per_class = 1 ,
random_state = 123 ) # Plot the dataset plt.figure(figsize = ( 8 , 6 ))
plt.scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = 'viridis' , marker = 'o' ,
edgecolors = 'k' ) plt.xlabel( 'Feature 1' ) plt.ylabel( 'Feature 2' )
Out[15]: Text(0, 0.5, 'Feature 2') save data (Python) import numpy as np data
= np.column_stack((X.astype( str ), y.astype( str ))) np.savetxt(
'classification_data.txt' , data, fmt = ' %s ' ) reading data import scala .
io . Source case class Point ( coordinates : List [ Double ], label : Int )
def readData (): List [ Point ] = { val filePath = "classification_data.txt"
val lines = Source . fromFile ( filePath ). getLines (). toList val points :
List [ Point ] = lines . map { line => val fields = line . split ( " \\\ s+" )
val x = fields . init . map ( _ . toDouble ). toList val y = fields . last .
toInt Point ( x , y ) } return points } val points = readData () Model For a 2
dimensional problem binary classification model would look like this: \\(p =
\sigma(\beta_0 + \beta_1*x_1 + \beta_2*x_2)\\) where p is the probability of a
data point to belong to class 1, \\(\beta_i\\) \- learnable model parameters
We can implement it in Scala in the following way: import math . exp def
linear ( x : List [ Double ], beta : List [ Double ]): Double = return ( 1.0
:: x ). zip ( beta ). map ({ case ( a , b ) => a * b }). reduce ( _ \+ _ ) //
dot product x*beta, append 1 for beta_0 def sigmoid ( x : Double ): Double = 1
/ ( 1 \+ math . exp (- x ). toFloat ) def model ( x : List [ Double ], beta :
List [ Double ]): Double = sigmoid ( linear ( x , beta )) Cost function and
gradient We need to minimize the cost function \\(J\\) : \\(J = - \frac{1}{N}
* \sum_{k=0}^{N}[y_k*log(p_k) + (1-y_k)*log(1-p_k)]\\) where \\(N\\) \- total
number of points And the corresponding partial derivative with respect to each
parameter is: \\(\frac{dJ}{d{\beta}_i} = \frac{1}{N} * \sum_{k=0}^{N} x_i*(p_k
- y_k)\\) For gradient descent we need to compute gradient at each iteration
and update parameters. Implementation of the gradient
\\([\frac{dJ}{d{{\beta}}_0}, \frac{dJ}{d{{\beta}}_1},
\frac{dJ}{d{\beta}_2}]\\) computation and loss function: def
partial_derivative ( y : Int , pred : Double , x : Double ): Double = ( pred
\- y )* x def gradient ( xs : List [ Double ], pred : Double , y : Int ): List
[ Double ] = { ( 1.0 :: xs ). map ( x => partial_derivative ( y , pred , x ))
// add 1 to xs to acount for b0 } def loss ( pred : Double , y : Int ): Double
= { -( y * log ( pred ) \+ ( 1 \- y )* log ( 1 \- pred )) } Define
computational graph Let’s define our computation graph for one iteration of
logistic regression. We want to compute gradients for all data points and find
their average. For monitoring also let’s compute loss for each iteration as
well. import math . log /* Sum gradients and loss values */ def sum ( a :(
List [ Double ], Double , Int ), b :( List [ Double ], Double , Int )): ( List
[ Double ], Double , Int ) = { val ( grad1 , loss1 , cnt1 ) = a val ( grad2 ,
loss2 , cnt2 ) = b val grad_sum = grad1 . zip ( grad2 ). map ({ case ( g1 , g2
) => g1 \+ g2 }) val loss_sum = loss1 \+ loss2 val count = cnt1 \+ cnt2 (
grad_sum , loss_sum , count ) } /*Compute average for gradients and the loss
value*/ def average ( grad_sum : List [ Double ], loss_sum : Double , counts :
Int ): ( List [ Double ], Double ) = { ( grad_sum . map ( _ / counts ),
loss_sum / counts ) } /*Compute gradient and loss value*/ def compute ( points
: RDD [ Point ], params : List [ Double ]): ( List [ Double ], Double , Int )
= points . map ({ case Point ( xs , y ) => ( Point ( xs , y ), model ( xs ,
params ))}) // get predictions . map ({ case ( Point ( xs , y ), pred ) => (
gradient ( xs , pred , y ), loss ( pred , y ), 1 )}) // compute gradient and
loss . reduce ( sum ) Running the training Running the training. 10
iterations. Compute average gradient and loss for each iteration and perform
gradient descent step \\(parameters_n = parameters_{n-1} - \alpha * \nabla
J\\) val pointsRDD = sc . parallelize ( points ) var parameters = List ( 0.1 ,
0.1 , 0.1 ) // b0,b1,b2 random val step_size = 3 for ( i <\- 0 to 10 ){ val (
gradSum , lossSum , cnt ) = compute ( pointsRDD , parameters ) val ( grad ,
loss ) = average ( gradSum , lossSum , cnt ) parameters = parameters . zip (
grad ). map ({ case ( param , g ) => param \- step_size * g }) println ( loss
) } loss output 0.7985616355198335 0.07500786607886242 0.06578866845350452
0.05999996600024983 0.05603349371988779 0.05315514097244599
0.05097938292867146 0.04928355967304257 0.04792987072323888
0.046828371238424885 0.04591785644633872 Computing accuracy: val accuracy =
pointsRDD . map ( point => ( point . label , model ( point . coordinates ,
parameters ))) . map ({ case ( y , prediction ) => if ( ( prediction > 0.5 )
== ( y == 1 ) ) 1 else 0 }) . mean accuracy = 0.9945 Let’s plot the decision
boundary for the obtained model: save results import java . io . PrintWriter
def saveParameters ( parameters : List [ Double ]) = { new PrintWriter (
"parameters.txt" ) { // Iterate through the array and write each element to
the file parameters . foreach ( println ) close () } } saveParameters (
parameters ) plot results (Python) with open ( 'parameters.txt' , 'r' ) as
file : parameters = [ float (line.strip()) for line in file ] def
get_decision_line(b0,b1,b2): c = \- b0 / b2 m = \- b1 / b2 return lambda x: m
* x \+ c xs = np.linspace( \- 5 , 5 ) ys = get_decision_line( *
parameters)(xs) plt.figure(figsize = ( 8 , 6 )) plt.scatter(X[:, 0 ], X[:, 1
], c = y, cmap = 'viridis' , marker = 'o' , edgecolors = 'k' ) plt.plot(xs,
ys) Out[17]: [] Execution analysis Logistic regression iteration doesn’t
require shuffle operation. We simply apply the sequence of map operations to
compute gradients and losses for each point and then do reduce to aggregate.
The final step of the iteration happens on the driver. The Driver receives
aggregated gradients and losses from the executors, computes the average and
performs gradient descent update step. The updated parameters are then sent to
the executors for the next iteration.

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://galax.dev/index.html***

galax.dev My name is Szymon and, I am an aspiring Machine/Deep Learning
Engineer, currently in my third year of Computer Science. If you are already
here, you can check out my blog posts and demo apps in the playground below.
You can find most of the code for my projects on my GitHub. blog Click here to
check out the all blog posts. LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Nov 13, 2023 An Image-to-Image
Implementation Demonstation of an image-to-image implementation of the Stable
Diffusion model. Oct 3, 2023 No matching items playground Click here to play
more in the playground. No matching items Contact Information afterhoursbilly
GitHub Twitter [email protected]

***URL: https://galax.dev/posts/00_Image-to-Image.html***

galax.dev - An Image-to-Image Implementation Text-Guided: Image-to-Image
Implementation This Python code demonstrates the implementation of the Image-
to-Image technique, allowing you to generate new images from existing ones
with the help of textual prompts. Explore how this innovative approach
combines images and text to create visually compelling artworks. Dive into the
code to understand the mechanics behind this cutting-edge image generation
technique. Pip install necessary libraries (click to show/hide) ! pip install
\- Uq diffusers transformers fastcore fastdownload Importing utilities (click
to show/hide) from transformers import CLIPTextModel, CLIPTokenizer import
torch from diffusers import LMSDiscreteScheduler from PIL import Image from
tqdm.auto import tqdm from diffusers import AutoencoderKL,
UNet2DConditionModel import logging from fastdownload import FastDownload from
pathlib import Path from huggingface_hub import notebook_login import
matplotlib.pyplot as plt from torchvision import transforms if not
(Path.home() / '.cache/huggingface' / 'token' ).exists(): notebook_login()
logging.disable(logging.WARNING) We need to load in the required libraries and
set up the models. tokenizer = CLIPTokenizer.from_pretrained( "openai/clip-
vit-large-patch14" , torch_dtype = torch.float16) text_encoder =
CLIPTextModel.from_pretrained( "openai/clip-vit-large-patch14" , torch_dtype =
torch.float16).to( "cuda" ) # Here we use a different VAE to the original
release, which has been fine-tuned for more steps vae =
AutoencoderKL.from_pretrained( "stabilityai/sd-vae-ft-ema" , torch_dtype =
torch.float16).to( "cuda" ) unet = UNet2DConditionModel.from_pretrained(
"CompVis/stable-diffusion-v1-4" , subfolder = "unet" , torch_dtype =
torch.float16).to( "cuda" ) Define the parameters. height = 512 width = 512
num_inference_steps = 70 guidance_scale = 7.5 batch_size = 1
beta_start,beta_end = 0.00085 , 0.012 scheduler =
LMSDiscreteScheduler(beta_start = beta_start, beta_end = beta_end,
beta_schedule = "scaled_linear" , num_train_timesteps = 1000 )
plt.plot(scheduler.sigmas) plt.title( 'Noise Schedule' ) plt.xlabel( 'Sampling
step' ) plt.ylabel( 'sigma' ) plt.show() def prep_img(img_link : str ) ->
torch.Tensor: """ Preprocesses an image from a given link. Args: img_link
(str): The URL or path to the image file. Returns: torch.Tensor: A tensor
representing the preprocessed image. """ p = FastDownload().download(img_link)
init_image = Image. open (p).convert( "RGB" ).resize(( 512 , 512 )) return
transforms.ToTensor()(init_image) The image we will use as a starting point.
Downloading the image (click to show/hide) link = "https://cdn-
uploads.huggingface.co/production/uploads/1664665907257-noauth.png"
transformed_image = prep_img(link) # show image in notebook. p =
FastDownload().download(link) display(Image. open (p).convert( "RGB"
).resize(( 512 , 512 ))) def tokenization(prompt: list , max_len : int = None
) \- > torch.Tensor: """ Tokenizes a text prompt and returns the corresponding
encoded tensor. Args: prompt (list): The input text prompt to be tokenized.
max_len (int, optional): The maximum length of the tokenized sequence. If not
specified, it defaults to the maximum length allowed by the tokenizer.
Returns: torch.Tensor: A tensor containing the encoded representation of the
tokenized prompt. """ if max_len is None : max_len =
tokenizer.model_max_length tokenized_prompt = tokenizer(prompt, padding =
"max_length" , max_length = max_len, truncation = True , return_tensors = 'pt'
) return text_encoder(tokenized_prompt.input_ids.to( 'cuda' ))[ 0 ].half() def
make_image(latent: torch.Tensor): """ Converts a tensor representation of an
image into a PIL Image. Args: latent (torch.Tensor): A tensor representing an
image. Returns: PIL.Image.Image: A PIL Image representing the image. """ image
= (latent / 2 \+ 0.5 ).clamp( 0 , 1 ).detach().cpu().permute( 1 , 2 , 0
).numpy() return Image.fromarray((image * 255 ). round ().astype( "uint8" ))
Denoising loop To ensure the effectiveness of this solution, it is essential
to incorporate the “start_step” parameter. Essentially, we aim to prevent
excessive noise from being added to the input image, particularly avoiding the
most intense noise additions. After this initial step, we can proceed with the
looping process. In summary, the key to success here is to introduce the
“start_step” parameter, which helps us avoid excessive noise in the early
stages and then continue with the loop as intended. def create_sample(prompt:
list ,transformed_image: torch.Tensor ,guidance_scale: float = 7.5 , seed: int
= 5 , steps: int = 70 ,start_step: int = 10 ): ''' Generate a sample image
based on a text prompt, provided image and guidance parameters. Args: prompt
(list): A list of text prompts. transformed_image (torch.Tensor): A tensor
representing the transformed image. guidance_scale (float, optional): The
scale factor for guiding the generation process. seed (int, optional): Seed
for random number generation. Default is 5. steps (int, optional): The total
number of steps for the generation process. Default is 70. start_step (int,
optional): The step at which the generation process starts. Default is 10.
Returns: torch.Tensor: A tensor representing the generated sample. This
function generates an image based on the provided text prompts , transformed
image and parametrs.It uses a predefined VAE model to encode the image and
then applies noise and guidance to generate the sample.It iteratively refines
the image by adding noise and updating the latent representation. The guidance
scale controls the influence of the text prompts on the image. The generated
image is returned as a PyTorch tensor. Example: >>> prompt = ["Translate the
following English sentence to French: 'Hello, how are you?'"] >>>
transformed_image = prep_img(image_link) >>> generated_sample =
create_sample(prompt, transformed_image) ''' bs = 1 # Implementation for only
a single prompt. text = tokenization(prompt) uncond = tokenization([ "" ] *
bs, text.shape[ 1 ]) emb = torch.cat([uncond, text]) if seed:
torch.manual_seed(seed) # Encode image image_latent =
vae.encode((transformed_image.unsqueeze( 0 ).half().to( 'cuda'
))).latent_dist.sample() image_latent = vae.config.scaling_factor *
image_latent # Create noise scheduler.set_timesteps(steps) noise_latents =
torch.randn_like(image_latent) latents = scheduler.add_noise(image_latent,
noise_latents, timesteps = torch.tensor([scheduler.timesteps[start_step]]))
for i, ts in enumerate (tqdm(scheduler.timesteps)): if i >= start_step: # Skip
the batches of noise that don't affect the input image. inp =
scheduler.scale_model_input(torch.cat([latents] * 2 ), ts) with
torch.no_grad(): noise_pred_uncond, noise_pred_text = unet(inp, ts,
encoder_hidden_states = emb).sample.chunk( 2 ) pred = noise_pred_uncond \+
guidance_scale * (noise_pred_text \- noise_pred_uncond) latents =
scheduler.step(pred, ts, latents).prev_sample with torch.no_grad(): return
vae.decode( 1 / 0.18215 * latents).sample prompt = [ 'Wolf howling at the
moon, photorealistic 4K' ] #prompt = ['unicorn'] #prompt = ['a kids drawing of
bacteria, cartoon style'] #prompt = [' Horse looking at the morning sun,
photorealistic 4K'] image = create_sample(prompt,transformed_image,steps = 50
,seed = 1000 ) display(make_image(image[ 0 ])) Back to top

***URL: https://galax.dev/apps.html***

galax.dev - apps playground Logo Header Categories All (0) afterhoursbilly
GitHub Twitter [email protected] No matching items Back to top

***URL: https://galax.dev/blog.html***

galax.dev - blog Categories All (2) Deep Learning (2) Implementation (2)
Stable Diffusion (1) afterhoursbilly GitHub Twitter [email protected] Order By
Default Date - Oldest Date - Newest LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Monday, 13 November 2023 2 min An Image-
to-Image Implementation Demonstation of an image-to-image implementation of
the Stable Diffusion model. Tuesday, 03 October 2023 1 min No matching items
Back to top

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch.html***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://viktoranchutin.github.io/blog.html***

Engineering notes - Blog Spark RDD with distributed machine learning
Understanding Spark computational model with logistic regression and
clustering Applying vision models to audio data Fine-tuning ViTs and ConvNets
on spectrogram data Gradient accumulation Toy example for gradient
accumulation understanding Training with variable length data Building a
dataloader to train deep learning models on variable length data No matching
items

***URL: https://viktoranchutin.github.io/blog/variable_length_training.html***

Engineering notes - Training with variable length data There are several
different ways we can deal with variable length data when training deep
learning models: Cut or pad all the samples to the maximum length in the whole
dataset Cut or pad samples to the maximum length within a mini-batch Split the
dataset into multiple buckets with samples of similar length. I will describe
the third option as it imposes the least memory and computational overhead.
This option can be used to train CNNs, RNNs or transformers with relative
positional encoding, since they can be trained on variable length data. For
example we can train Wav2Vec 2.0 model with audio samples of different length
as it encodes audio with CNN and is using convolutional relative positional
encoding as well. 1\. Splitting the data Generating dataset with variable
length items. from random import randint import torch import pandas as pd #
generate dataset min_length = 2 max_length = 20 size = 1000 dummy_y = 0
dataset = [(torch.randn(randint(min_length,max_length)),dummy_y) for _ in
range (size)] Create a dataframe with information about items length df =
pd.DataFrame([( id , len (x)) for id ,(x,y) in enumerate (dataset)], columns =
[ 'id' , 'length' ]) df.length.plot(kind = 'hist' ,title = "Length
distribution" ) ; Split data into bukets nbuckets = 10 df[ 'bucket' ] =
pd.cut(df.length, bins = nbuckets, labels = range (nbuckets)) 2\. Create
dataloaders Create DataSet class, which is using a dataframe with items ids to
retrieve them from the original dataset. from torch.utils.data import
DataLoader from torch.nn.functional import pad class DataSet: def __init__ (
self ,dataframe,data): self .df = dataframe.reset_index(drop = True ) # items
ids self .data = data def __getitem__ ( self ,index): id = self
.df.iloc[index]. id # get item by id from the original dataset return self
.data[ id ] def __len__ ( self ): return len ( self .df) Collate function adds
padding according to the maximum length in a batch def collate_fn(batch):
xs,ys = [ list (b) for b in ( zip ( * batch))] maxl = max ([ len (x) for x in
xs]) # maxl in a batch for i in range ( len (xs)): xs[i] = pad(xs[i],( 0 ,maxl
\- len (xs[i]))) # pad to maxl x = torch.stack(xs) y = torch.tensor(ys) return
(x,y) Create dataloaders for each bucket def create_dataloader(dataframe,bs =
4 ): return DataLoader(DataSet(dataframe, dataset), bs, shuffle = True ,
collate_fn = collate_fn) dataloaders = [] for bucket_id in df.bucket.unique():
dl = create_dataloader(df[df.bucket == bucket_id]) dataloaders.append(dl) 3\.
Create random iterator The iterator takes iterators from the dataloaders and
randomly chooses one at the each next call from random import choice class
DLIterator: def __init__ ( self , dls) -> None : self .iters = [ iter (dl) for
dl in dls] def __iter__ ( self ): return self def __next__ ( self ): for _ in
range ( len ( self .iters)): # iterate in case some are empty try : it =
choice( self .iters) return next (it) except StopIteration : self
.iters.remove(it) raise StopIteration class MultiDataLoader: '''Combining
multiple dataloaders.''' def __init__ ( self ,dataloaders) -> None : self .dls
= dataloaders def __iter__ ( self ): return DLIterator( self .dls) def __len__
( self ): return sum ( map ( len , self .dls)) Check the distribution of batch
lengths for the obtained dataloader import matplotlib.pyplot as plt
batch_sizes = [xb.shape[ 1 ] for xb,_ in MultiDataLoader(dataloaders)]
plt.hist(batch_sizes) ; Visualize batch lengths: it = iter
(MultiDataLoader(dataloaders)) _,ax = plt.subplots( 5 ) for i in range ( 5 ):
ax[i].imshow( next (it)[ 0 ])

***URL: https://viktoranchutin.github.io/blog/Spark RDD.html***

Engineering notes - Spark RDD with distributed machine learning In this blog
post I take a close look into the Spark computational model by implementing 2
machine learning algorithms: Logistic Regression and K-Means clustering. The
code is executed in the Databricks environment using Scala. Python is used for
visualization. Databricks notebook Spark was developed to address iterative
big data algorithms like logistic regression (gradient descent) or k-means
clustering. From the RDDs paper : K-Means clustering Data For K-means
clustering let’s generate 5 clusters of data. Generate data (Python) import
matplotlib.pyplot as plt from sklearn.datasets import make_blobs import numpy
as np import csv n_samples = 10000 n_features = 2 n_clusters = 5 data, labels
= make_blobs(n_samples = n_samples, n_features = n_features, centers =
n_clusters, random_state = 12345 ) def plot_centroids(centroids,ax): for i,
centroid in enumerate (centroids): samples = data[i * n_samples:(i \+ 1 ) *
n_samples] ax.plot( * centroid, markersize = 10 , marker = "x" , color = 'k' ,
mew = 5 ) ax.plot( * centroid, markersize = 5 , marker = "x" , color = 'm' ,
mew = 2 ) _,ax = plt.subplots() ax.scatter(data[:, 0 ], data[:, 1 ], c =
labels, cmap = 'viridis' ) centroids = [] for cluster_label in range
(n_clusters): cluster_points = data[labels == cluster_label] # Select data
points in the current cluster cluster_centroid = np.mean(cluster_points, axis
= 0 ) # Calculate the centroid for the cluster
centroids.append(cluster_centroid) plot_centroids(centroids,ax) print
(centroids) #save data with open ( 'data.csv' , 'w' , newline = '' ) as file :
writer = csv.writer( file ) writer.writerows(data) [array([ 8.57032532,
-3.64137432]), array([-6.34270089, -5.91962725]), array([1.3294662 ,
1.87901003]), array([9.31274039, 3.05338878]), array([4.96798439,
3.09705577])] Let’s read the data and create an RDD of data points, Also
important to make sure that RDD of data points will be cached, so we don’t
need to recompute it for each iteration. import scala . io . Source def
getPointsRDD (): RDD [( Float , Float )] = { val source = Source . fromFile (
"data.csv" ) val linesRDD = sc . parallelize ( source . getLines (). toList )
return linesRDD . map ( _ . split ( ',' ). map ( _ . toFloat )). map ({ case
Array ( x , y ) => ( x , y )}) } val points = getPointsRDD (). cache () We can
see that data points were split by Spark in 8 partitions: points .
getNumPartitions res5: Int = 8 Initialize centroids The first step of k-means
clustering is to initialize the first estimates of the centroids. For this
example I randomly sample 5 points, but in real applications this
initialization step usually involves more sophisticated sampling. val
randomMeans = points . takeSample ( withReplacement = false , num = 5 , seed =
10 ) save centroids for plotting import java . io . PrintWriter def saveResult
( means : Array [( Float , Float )]) = { val writer = new PrintWriter (
"means.txt" ) try { means . foreach ( writer . println ) } finally { writer .
close () } } saveResult ( randomMeans ) Plot centroids (Python) # Open the
file in read mode def readResults() -> list : with open ( 'means.txt' , 'r' )
as file : lines = file .readlines() return [ tuple ( map ( float , line.strip(
'() \n ' ).split( ',' ))) for line in lines] def plot_results(centroids,ax):
for i, centroid in enumerate (centroids): samples = data[i * n_samples:(i \+ 1
) * n_samples] ax.plot( * centroid, markersize = 5 , marker = "*" , color =
'r' , mew = 5 ) first_state = readResults() _,ax = plt.subplots()
plot_results(first_state,ax) plot_centroids(centroids,ax) ax.set_title(
'Initial centroids against true centroids' ) Out[5]: Text(0.5, 1.0, 'Initial
centroids against true centrids') Define computational graph Let’s implement
k-means clustering. The main function update \- conceptually performs 2 steps:
Groups the points by the closest centroids Finds the centre of the groups,
effectively obtaining the new estimate for centroids def euclideanDistance (
v1 : ( Float , Float ), v2 : ( Float , Float )): Double = ( v1 . _1 \- v2 . _1
) * ( v1 . _1 \- v2 . _1 ) \+ ( v1 . _2 \- v2 . _2 ) * ( v1 . _2 \- v2 . _2 )
/** Return the center that is the closest to ` p ` */ def findClosest ( p : (
Float , Float ), centers : Array [( Float , Float )]): ( Float , Float ) =
centers . minBy ( euclideanDistance ( _ , p )) def updateMeans ( means : Array
[( Float , Float )], points : RDD [( Float , Float )]): Array [( Float , Float
)] = return points . map ( point => ( findClosest ( point , means ), point ))
// pair (closest mean, point) . mapValues ( point => ( point , 1 )) // add
counter for aggregation . reduceByKey ({ case (( p1 , cnt1 ),( p2 , cnt2 )) =>
(( p1 . _1 \+ p2 . _1 , p1 . _2 \+ p2 . _2 ), cnt1 \+ cnt2 )}) // sum all the
points around a centroid . mapValues ({ case ( sum , count ) => ( sum . _1 /
count , sum . _2 / count )}) // average aggregated points -> new centroid .
map ({ case ( oldMean , newMean ) => newMean }) . collect () Running the
algorithm Let’s run 10 iterations and look at the result. var means =
randomMeans for ( i <\- 0 to 10 ){ means = updateMeans ( means , points ) }
save results for plotting saveResult ( means ) The algorithm successfully
found true centroids of clusters. Plot results (Python) results =
readResults() _,ax = plt.subplots( 1 , 3 ,figsize = ( 10 , 3 ))
plot_results(results,ax[ 0 ]) plot_centroids(centroids,ax[ 0 ])
plot_results(results,ax[ 1 ]) plot_centroids(centroids,ax[ 2 ]) ax[ 0
].set_title( 'True centroids and estimations' ) ax[ 1 ].set_title(
'Estimations' ) ax[ 2 ].set_title( 'True centroids' ) Out[5]: Text(0.5, 1.0,
'True centroids') Execution analysis First, Spark builds a graph of
computations and only when we call action functions such as .collect() it
executes the graph. Let’s look at the diagram of the execution process. Spark
driver creates closures with centroids and sends them to the executors.
Executors apply closures received by the driver to the partitions. From the
spark paper : “..users provide arguments to RDD operations like map by passing
closures (function literals). Scala represents each closure as a Java object,
and these objects can be serialized and loaded on another node to pass the
closure across the network. Scala also saves any variables bound in the
closure as fields in the Java object.” First each partition of points is
transformed to the pairs of points and the corresponding closest centroid.
Then we have reduceByKey followed by shuffle and the average. It is
conceptually the same as grouping the points by key and taking the average,
but computationally more optimal. If we used groupByKey, then the shuffle
operation would have to send 10000 points over the network in the worst case.
With reduceByKey operation, reduction happens before shuffle occurs,
significantly reducing the amount of data to send. In this case for each
cluster data points are reduced to a single pair of sum and counts, which
means that at most 5 (number of clusters) * 8 (number of partitions) = 40
pairs would need to be sent over the network. The obtained centroids are then
sent back to the driver. Next iteration driver sends updated centroids back to
the executors for recomputation. Logistic regression Data Let’s generate data
for binary classification problem generate data (Python) import numpy as np
import matplotlib.pyplot as plt from sklearn.datasets import
make_classification X, y = make_classification(n_samples = 10000 , n_features
= 2 , n_informative = 2 , n_redundant = 0 , n_clusters_per_class = 1 ,
random_state = 123 ) # Plot the dataset plt.figure(figsize = ( 8 , 6 ))
plt.scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = 'viridis' , marker = 'o' ,
edgecolors = 'k' ) plt.xlabel( 'Feature 1' ) plt.ylabel( 'Feature 2' )
Out[15]: Text(0, 0.5, 'Feature 2') save data (Python) import numpy as np data
= np.column_stack((X.astype( str ), y.astype( str ))) np.savetxt(
'classification_data.txt' , data, fmt = ' %s ' ) reading data import scala .
io . Source case class Point ( coordinates : List [ Double ], label : Int )
def readData (): List [ Point ] = { val filePath = "classification_data.txt"
val lines = Source . fromFile ( filePath ). getLines (). toList val points :
List [ Point ] = lines . map { line => val fields = line . split ( " \\\ s+" )
val x = fields . init . map ( _ . toDouble ). toList val y = fields . last .
toInt Point ( x , y ) } return points } val points = readData () Model For a 2
dimensional problem binary classification model would look like this: \\(p =
\sigma(\beta_0 + \beta_1*x_1 + \beta_2*x_2)\\) where p is the probability of a
data point to belong to class 1, \\(\beta_i\\) \- learnable model parameters
We can implement it in Scala in the following way: import math . exp def
linear ( x : List [ Double ], beta : List [ Double ]): Double = return ( 1.0
:: x ). zip ( beta ). map ({ case ( a , b ) => a * b }). reduce ( _ \+ _ ) //
dot product x*beta, append 1 for beta_0 def sigmoid ( x : Double ): Double = 1
/ ( 1 \+ math . exp (- x ). toFloat ) def model ( x : List [ Double ], beta :
List [ Double ]): Double = sigmoid ( linear ( x , beta )) Cost function and
gradient We need to minimize the cost function \\(J\\) : \\(J = - \frac{1}{N}
* \sum_{k=0}^{N}[y_k*log(p_k) + (1-y_k)*log(1-p_k)]\\) where \\(N\\) \- total
number of points And the corresponding partial derivative with respect to each
parameter is: \\(\frac{dJ}{d{\beta}_i} = \frac{1}{N} * \sum_{k=0}^{N} x_i*(p_k
- y_k)\\) For gradient descent we need to compute gradient at each iteration
and update parameters. Implementation of the gradient
\\([\frac{dJ}{d{{\beta}}_0}, \frac{dJ}{d{{\beta}}_1},
\frac{dJ}{d{\beta}_2}]\\) computation and loss function: def
partial_derivative ( y : Int , pred : Double , x : Double ): Double = ( pred
\- y )* x def gradient ( xs : List [ Double ], pred : Double , y : Int ): List
[ Double ] = { ( 1.0 :: xs ). map ( x => partial_derivative ( y , pred , x ))
// add 1 to xs to acount for b0 } def loss ( pred : Double , y : Int ): Double
= { -( y * log ( pred ) \+ ( 1 \- y )* log ( 1 \- pred )) } Define
computational graph Let’s define our computation graph for one iteration of
logistic regression. We want to compute gradients for all data points and find
their average. For monitoring also let’s compute loss for each iteration as
well. import math . log /* Sum gradients and loss values */ def sum ( a :(
List [ Double ], Double , Int ), b :( List [ Double ], Double , Int )): ( List
[ Double ], Double , Int ) = { val ( grad1 , loss1 , cnt1 ) = a val ( grad2 ,
loss2 , cnt2 ) = b val grad_sum = grad1 . zip ( grad2 ). map ({ case ( g1 , g2
) => g1 \+ g2 }) val loss_sum = loss1 \+ loss2 val count = cnt1 \+ cnt2 (
grad_sum , loss_sum , count ) } /*Compute average for gradients and the loss
value*/ def average ( grad_sum : List [ Double ], loss_sum : Double , counts :
Int ): ( List [ Double ], Double ) = { ( grad_sum . map ( _ / counts ),
loss_sum / counts ) } /*Compute gradient and loss value*/ def compute ( points
: RDD [ Point ], params : List [ Double ]): ( List [ Double ], Double , Int )
= points . map ({ case Point ( xs , y ) => ( Point ( xs , y ), model ( xs ,
params ))}) // get predictions . map ({ case ( Point ( xs , y ), pred ) => (
gradient ( xs , pred , y ), loss ( pred , y ), 1 )}) // compute gradient and
loss . reduce ( sum ) Running the training Running the training. 10
iterations. Compute average gradient and loss for each iteration and perform
gradient descent step \\(parameters_n = parameters_{n-1} - \alpha * \nabla
J\\) val pointsRDD = sc . parallelize ( points ) var parameters = List ( 0.1 ,
0.1 , 0.1 ) // b0,b1,b2 random val step_size = 3 for ( i <\- 0 to 10 ){ val (
gradSum , lossSum , cnt ) = compute ( pointsRDD , parameters ) val ( grad ,
loss ) = average ( gradSum , lossSum , cnt ) parameters = parameters . zip (
grad ). map ({ case ( param , g ) => param \- step_size * g }) println ( loss
) } loss output 0.7985616355198335 0.07500786607886242 0.06578866845350452
0.05999996600024983 0.05603349371988779 0.05315514097244599
0.05097938292867146 0.04928355967304257 0.04792987072323888
0.046828371238424885 0.04591785644633872 Computing accuracy: val accuracy =
pointsRDD . map ( point => ( point . label , model ( point . coordinates ,
parameters ))) . map ({ case ( y , prediction ) => if ( ( prediction > 0.5 )
== ( y == 1 ) ) 1 else 0 }) . mean accuracy = 0.9945 Let’s plot the decision
boundary for the obtained model: save results import java . io . PrintWriter
def saveParameters ( parameters : List [ Double ]) = { new PrintWriter (
"parameters.txt" ) { // Iterate through the array and write each element to
the file parameters . foreach ( println ) close () } } saveParameters (
parameters ) plot results (Python) with open ( 'parameters.txt' , 'r' ) as
file : parameters = [ float (line.strip()) for line in file ] def
get_decision_line(b0,b1,b2): c = \- b0 / b2 m = \- b1 / b2 return lambda x: m
* x \+ c xs = np.linspace( \- 5 , 5 ) ys = get_decision_line( *
parameters)(xs) plt.figure(figsize = ( 8 , 6 )) plt.scatter(X[:, 0 ], X[:, 1
], c = y, cmap = 'viridis' , marker = 'o' , edgecolors = 'k' ) plt.plot(xs,
ys) Out[17]: [] Execution analysis Logistic regression iteration doesn’t
require shuffle operation. We simply apply the sequence of map operations to
compute gradients and losses for each point and then do reduce to aggregate.
The final step of the iteration happens on the driver. The Driver receives
aggregated gradients and losses from the executors, computes the average and
performs gradient descent update step. The updated parameters are then sent to
the executors for the next iteration.

***URL: https://viktoranchutin.github.io/projects.html***

Engineering notes - Projects Detecting active emotions in speech Training 5
different models to perform active emotion recognition from speech ResNet from
scratch for rice disease classification Building a custom ResNet model using
Pytorch Heart disease prediction Data analysis, Random Forest and Logistic
Regression with heart disease dataset Controlling magnetic field in electric
motors How I built BLDC motor control system from scratch in my Robotics
masters degree No matching items

***URL: https://viktoranchutin.github.io/index.html***

Engineering notes - Viktor Anchutin LinkedIn GitHub Resume About Passionate
software engineer with a love for machine learning, data-driven applications
and robots I have 2 master’s degrees: Data Science and Robotics. Committed to
learning and growing in the field of AI I started as a robotics software
developer, using C and C++, and later transitioned to backend engineering with
Kotlin/Java. I’ve played a key role in building various data-intensive
systems, showcasing my skills in system thinking and design. Blog Projects No
matching items

***URL: https://viktoranchutin.github.io/projects/Heart_disease_project.html***

Engineering notes - Heart disease prediction Imports import pandas as pd
import numpy as np import seaborn as sns import matplotlib.pyplot as plt
import scipy.stats as stats import palettable import seaborn as sns from
sklearn.ensemble import RandomForestRegressor from sklearn.impute import
SimpleImputer from sklearn.model_selection import cross_val_score Data The
data is available at UCI machine learning repository . The website contains 4
datasets concerning heart disease diagnosis.The data for these datasets was
collected from the four following locations: 1\. Hungarian Institute of
Cardiology. Budapest: Andras Janosi, M.D. 2\. University Hospital, Zurich,
Switzerland: William Steinbrunn, M.D. 3\. University Hospital, Basel,
Switzerland: Matthias Pfisterer, M.D. 4\. V.A. Medical Center, Long Beach and
Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D. There are 2 versions
of each dataset: Full dataset with 76 attributes Dataset with 14 attributes
The reduced dataset exists because only the subset of 14 attributes has been
used in prior research and experiments. Files used for this project:
processed.switzerland.data processed.cleveland.data processed.hungarian.data
processed.va.data I create a single dataset by combining these four. Download
data ! wget https: // archive.ics.uci.edu / static / public / 45 / heart \+
disease. zip ! mkdir heart_disease_data ! unzip heart \+ disease. zip \- d
heart_disease_data Creating a single dataset The datasets have the same
columns, they don’t have headers, and missing values are provided as ? . Code
columns = [ "age" , "sex" , "cp" , "trestbps" , "chol" , "fbs" , "restecg" ,
"thalach" , "exang" , "oldpeak" , "slope" , "ca" , "thal" , "disease" ] sdf =
pd.read_csv( "heart_disease_data/processed.switzerland.data" , header = None ,
names = columns, na_values = '?' ) cdf = pd.read_csv(
"heart_disease_data/processed.cleveland.data" , header = None , names =
columns, na_values = '?' ) hdf = pd.read_csv(
"heart_disease_data/processed.hungarian.data" , header = None , names =
columns, na_values = '?' ) vdf = pd.read_csv(
"heart_disease_data/processed.va.data" , header = None , names = columns,
na_values = '?' ) df = pd.concat([sdf, cdf, vdf, hdf], ignore_index = True )
df.disease = df[ 'disease' ]. apply ( lambda x: 1 if x > 0 else 0 ) df age sex
cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal disease 0
32.0 1.0 1.0 95.0 0.0 NaN 0.0 127.0 0.0 0.7 1.0 NaN NaN 1 1 34.0 1.0 4.0 115.0
0.0 NaN NaN 154.0 0.0 0.2 1.0 NaN NaN 1 2 35.0 1.0 4.0 NaN 0.0 NaN 0.0 130.0
1.0 NaN NaN NaN 7.0 1 3 36.0 1.0 4.0 110.0 0.0 NaN 0.0 125.0 1.0 1.0 2.0 NaN
6.0 1 4 38.0 0.0 4.0 105.0 0.0 NaN 0.0 166.0 0.0 2.8 1.0 NaN NaN 1 ... ... ...
... ... ... ... ... ... ... ... ... ... ... ... 915 52.0 1.0 4.0 160.0 331.0
0.0 0.0 94.0 1.0 2.5 NaN NaN NaN 1 916 54.0 0.0 3.0 130.0 294.0 0.0 1.0 100.0
1.0 0.0 2.0 NaN NaN 1 917 56.0 1.0 4.0 155.0 342.0 1.0 0.0 150.0 1.0 3.0 2.0
NaN NaN 1 918 58.0 0.0 2.0 180.0 393.0 0.0 0.0 110.0 1.0 1.0 2.0 NaN 7.0 1 919
65.0 1.0 4.0 130.0 275.0 0.0 1.0 115.0 1.0 1.0 2.0 NaN NaN 1 920 rows × 14
columns Attributes Numerical attributes age : age in years, numerical trestbps
\- resting blood pressure (in mm Hg on admission to the hospital) chol \-
cholesterol in mg/dl thalach \- maximum heart rate achieved oldpeak \- ST
depression induced by exercise relative to rest. ‘ST’ relates to the positions
on the electrocardiographic (ECG) plot. ca \- number of major vessels (0-3)
colored by flouroscopy. Fluoroscopy is one of the most popular non-invasive
coronary artery disease diagnosis. It enables the doctor to see the flow of
blood through the coronary arteries in order to evaluate the presence of
arterial blockages. Categorical attributes sex : sex 1 = male 0 = female cp :
chest pain type 1: typical angina 2: atypical angina 3: non-anginal pain 4:
asymptomatic fbs \- fasting blood sugar > 120 mg/dl 1 = true 0 = false restecg
\- resting electrocardiographic (ECG) results 0: normal 1: having ST-T wave
abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
2: showing probable or definite left ventricular hypertrophy by Estes’
criteria exang \- exercise induced angina. Angina is a type of chest pain
caused by reduced blood flow to the heart. 1 - yes 0 - no slope \- the slope
of the peak exercise ST segment. (ECG) 1: upsloping 2: flat 3: downsloping
thal \- A blood disorder called thalassemia 3: normal blood flow 6: fixed
defect (no blood flow in some part of the heart) 7: reversable defect (a blood
flow is observed but it is not normal) disease \- refers to the presence of
heart disease in the patient. It is integer valued from 0 (no presence) to 4.
Exploratory data analysis EDA helper functinos df_eda = df.copy() def
plot_categorical(data = pd.DataFrame, column = str , labels = [], target =
'disease' , target_labels = [ 'healthy' , 'heart disease' ], title = '' , font
= 10 , ax = None ): crosstab = pd.crosstab(data[column], data[target]) ax =
crosstab.plot(kind = 'bar' , figsize = ( 15 , 7 ), rot = 0 , fontsize = font,
ax = ax) x = np.arange( len (labels)) ax.set_xticks(x)
ax.set_xticklabels(labels) ax.legend(target_labels) plt.ylabel( "count" , size
= 14 ) plt.title(title) bars = ax.patches #compute percents total_by_category
= crosstab. sum (axis = 1 ) healthy_perc = round ((crosstab[ 0 ] /
total_by_category ) * 100 ) for (i, bar) in enumerate (bars): prc =
healthy_perc.iloc[i] if i < len (healthy_perc) else 100 \- healthy_perc.iloc[i
% len (healthy_perc)] plt.annotate( str ( int (prc)) \+ '%' , (bar.get_x() \+
bar.get_width() / 2\. , bar.get_height()), ha = 'center' , va = 'center' ,
xytext = ( 0 , 9 ), textcoords = 'offset points' ) def plot_numeric(data =
pd.DataFrame, column = str , title = str ): fig, ax = plt.subplots( 1 , 2 )
fig.set_size_inches( 20 , 7 ) #with respect to the target healthy =
data.loc[data.disease == 0 , column] sick = data.loc[data.disease == 1 ,
column] healthy.plot.density(ax = ax[ 0 ]) sick.plot.density(ax = ax[ 0 ]) ax[
0 ].legend([ 'healthy' , 'heart disease' ]) data.boxplot(by = 'disease' ,
column = [column], ax = ax[ 1 ]) fig.suptitle(title, fontsize = 19 ) def
describe_numeric(data = pd.DataFrame, column = str ): temp = data[[column,
'disease' ]].copy() temp[ 'healthy' ] = data[data.disease == 0 ][column] temp[
'sick' ] = data[data.disease == 1 ][column] return temp[[column, 'healthy' ,
'sick' ]].describe() def plot_missing(data = pd.DataFrame): na_values_percent
= data.isna(). sum ().sort_values(ascending = False ) \ . apply ( lambda x:
(x, round (x / data.index.size * 100 , 2 ))) # (count, %) na_values_percent.
apply ( lambda x: x[ 1 ]).plot.bar() # (plot %) plt.ylabel( "Percentage" ,
size = 14 ) plt.title( 'Missing values' ) Numeric data summary Code df_numeric
= df_eda.loc[:,[ 'age' , 'trestbps' , 'chol' , 'thalach' , 'oldpeak' ]]
df_numeric.describe() age trestbps chol thalach oldpeak count 920.000000
861.000000 890.000000 865.000000 858.000000 mean 53.510870 132.132404
199.130337 137.545665 0.878788 std 9.424685 19.066070 110.780810 25.926276
1.091226 min 28.000000 0.000000 0.000000 60.000000 -2.600000 25% 47.000000
120.000000 175.000000 120.000000 0.000000 50% 54.000000 130.000000 223.000000
140.000000 0.500000 75% 60.000000 140.000000 268.000000 157.000000 1.500000
max 77.000000 200.000000 603.000000 202.000000 6.200000 Code dcorr =
df_numeric.corr(method = 'pearson' ) sns.heatmap(data = dcorr,annot = True
,fmt = ".2f" )  Categorical data summary Code plt.figure(figsize = ( 5 , 5
),dpi = 200 ) for (i, col) in enumerate ([ 'sex' , 'cp' , 'fbs' , 'restecg' ,
'exang' , 'disease' , 'thal' , 'slope' ]): plt.subplot( 3 , 3 ,i \+ 1 )
df_eda[col].value_counts().sort_index().plot(kind = 'pie' , figsize = ( 7 , 5
), autopct = ' %1.1f%% ' , title = col, textprops = { 'fontsize' : 5 })
Analysing risk factors for heart disease Cholesterol (‘chol’) Cholesterol has
a lot of 0 values, which is not possible. Code df_eda.chol.hist(bins = 20 )
df_eda[ 'chol' ] = df_eda[ 'chol' ].replace({ 0 :np.nan}) Analysis People with
heart disease on average have a higher cholesterol level. Code
plot_numeric(df_eda, 'chol' , 'Cholesterol' ) Code describe_numeric(df_eda,
'chol' ) chol healthy sick count 718.000000 372.000000 346.000000 mean
246.832869 240.158602 254.008671 std 58.527062 55.767559 60.620439 min
85.000000 85.000000 100.000000 25% 210.000000 204.000000 216.000000 50%
239.500000 233.000000 248.000000 75% 276.750000 270.250000 284.750000 max
603.000000 564.000000 603.000000 Binning shows that people with cholesterol
level more than 254 mg/dl have more than 50% chance of having a heart disease.
Code df_eda[ 'chol_range' ] = pd.qcut(df_eda[ 'chol' ], 10 , duplicates =
'drop' ) plot_categorical(df_eda, 'chol_range' ,
df_eda.chol_range.unique().sort_values(), title = 'Cholesterol intervals' )
Age People with heart disease on average are about 5 years older than healthy
people. Code df_eda.age.hist(bins = 20 )  Code describe_numeric(df_eda, 'age'
) age healthy sick count 920.000000 411.000000 509.000000 mean 53.510870
50.547445 55.903733 std 9.424685 9.433700 8.718959 min 28.000000 28.000000
31.000000 25% 47.000000 43.000000 51.000000 50% 54.000000 51.000000 57.000000
75% 60.000000 57.000000 62.000000 max 77.000000 76.000000 77.000000 Code
plot_numeric(df_eda, 'age' , 'age' ) We can see that the risk of getting heart
disease increases after the age of 54. Code df_eda[ 'age_range' ] =
pd.qcut(df_eda[ 'age' ], 10 ) plot_categorical(df_eda, 'age_range' ,
df_eda.age_range.unique().sort_values()) Resting blood pressure (‘trestbps’)
Code df_eda.trestbps.hist(bins = 20 )  There’s one unrealistic value of 0, I
replace it with na for analysis. Code df_eda.trestbps.replace({ 0 :np.nan},
inplace = True ) People with heart disease on average have a slightly higher
blood pressure than healthy patients. Code plot_numeric(df_eda, 'trestbps' ,
'Resting blood pressure' ) Code describe_numeric(df_eda, 'trestbps' ) trestbps
healthy sick count 860.000000 391.000000 469.000000 mean 132.286047 129.913043
134.264392 std 18.536175 16.869867 19.617889 min 80.000000 80.000000 92.000000
25% 120.000000 120.000000 120.000000 50% 130.000000 130.000000 130.000000 75%
140.000000 140.000000 145.000000 max 200.000000 190.000000 200.000000 Most
patients with normal blood pressure do not have heart disease, while most
patients with elevated blood pressure have heart disease. Code bins = [ 0 ,
120 , 130 , 140 , np.inf] labels = [ 'Normal' , 'Elevated' , 'High' ,
'Critically high' ] df_eda[ 'bp_range' ] = pd.cut(df_eda[ 'trestbps' ], bins)
plot_categorical(df_eda, 'bp_range' , labels) Exercise induced angina
(‘exang’) Most of the patients with heart disease experienced angina during
the exercise, while the majority in the healthy group had no such symptoms.
Code plot_categorical(df_eda, 'exang' , [ 'no' , 'yes' ]) Number of Blocked
Vessels (‘ca’) The chance of having heart disease increases proportionally to
the number of blocked vessels. Patients with 0 blocked vessels have only 27%
chance of having heart disease. The value reaches 85% chance for the group
with 3 blocked vessels. Code labels = [ "0" , "1" , "2" , "3" ]
plot_categorical(df_eda, 'ca' , labels) Gender The majority of men in the
dataset have heart disease, while only 26% of women are unhealthy. Code
plot_categorical(df_eda, 'sex' , [ 'women' , 'men' ]) Chest pain type (‘cp’)
Amongst the patients with no chest pain almost 80% had heart disease. Patients
with the atypical angina had the lowest level of heart disease rate. Overall,
we can not say if a chest pain can be considered as a risk factor for a heart
disease. Code labels = [ 'typical angina' , 'atypical angina' , 'non-anginal
pain' , 'no pain' ] plot_categorical(df_eda, 'cp' , labels) Fasting blood
sugar In the dataset most people don’t have a high sugar level. But among
those who do, almost 70% have heart disease. Code plot_categorical(df_eda,
'fbs' , [ 'Normal' , 'High' ]) Resting electrocardiographic results
(‘restecg’) Patients in the categories with type 1 and type 2 abnormalities
have higher chance of getting a heart disease than the group with normal ECG.
Code labels = [ 'normal' , 'type 1' , 'type 2' ] plot_categorical(df_eda,
'restecg' , labels) Maximum heart rate achieved (‘thalach’) Code
df_eda.thalach.plot.hist(bins = 20 )  Patients without heart disease are able
to reach a higher maximum heart rate than patients with the disease. Code
plot_numeric(df_eda, 'thalach' , 'Maximum heart rate achieved' ) ST depression
(oldpeak) Code df_eda.oldpeak.hist()  There are some negative values.
Replacing with nan. Code df_eda[ 'oldpeak' ] = df_eda[ 'oldpeak' ]. apply (
lambda x: np.nan if x < 0 else x) df_eda.oldpeak.hist()  Oldpeak is another
ECG parameter measuring ST depression during the exercise. It represents a
distance on the ECG plot between specific points. There’s a notable difference
in distributions between the groups. Sick people on average have higher value
of the parameter. Code plot_numeric(df_eda, 'oldpeak' , 'oldpeak' ) Code
describe_numeric(df_eda, 'oldpeak' ) oldpeak healthy sick count 846.000000
387.000000 459.000000 mean 0.906265 0.425840 1.311329 std 1.071192 0.712184
1.153295 min 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 50%
0.500000 0.000000 1.200000 75% 1.500000 0.800000 2.000000 max 6.200000
4.200000 6.200000 Patients with high olpeak values have higher chance of
having herat disease. Code df_eda[ 'oldpeak_range' ] = pd.qcut(df_eda[
'oldpeak' ], 5 , duplicates = 'drop' ) plot_categorical(df_eda,
'oldpeak_range' , df_eda.oldpeak_range.unique().sort_values(), title =
'oldpeak' ) The slope of the peak exercise ST segment (‘slope’) This is
another ECG parameter, measured during the exercise. Almost 80% of the
patients with ‘flat’ or ‘downslopping’ slope parameter had heart disease. Most
of the people with the ‘upslopping’ slope were healthy. Code labels = [
"upslopping" , "flat" , "downslopping" ] plot_categorical(df_eda, 'slope' ,
labels) Thalassemia blood disorder (‘thal’) People with this blood disorder
are at risk of having heart disease with almost 80% for both type 1 and type 2
disorders. Code labels = [ 'normal' , 'type 1' , 'type 2' ]
plot_categorical(df_eda, 'thal' , labels) Numeric attributes relationships
Younger patients are able to achieve higher maximum heart rate. Patients with
heart disease are older and have lower maximum heart rate. Code
sns.pairplot(df_eda, hue = 'disease' , vars = [ 'age' , 'thalach' ]) ; Blood
pressure is higher in older patients. But these factors do not form a clear
separation between healthy and sick patients. Code sns.pairplot(df_eda, hue =
'disease' , vars = [ 'age' , 'trestbps' ]) ; EDA colclusion Exploratory data
analysis identified the following groups with a high risk of heart disease:
Patients with high cholesterol Patients older than 54 years old Male patients
Patients with high blood sugar Patients with abnormalities in their ECG
Patient low maximum heart rate Patients with high oldpeak value Patients with
slope that flat or downsloping Patients with blocked heart vessels Patients
with thalassemia blood disorder Patients with high blood pressure, more than
120 mmHg Data quality Incorrect values There are some incorrect values in the
dataset as it was discovered. Cholesterol (‘chol’) has lots of zeros. ST
depression (‘oldpeak’) has some negative values. In the existing research this
parameter is >= 0. Resting blood pressure (‘trestbps’) has one zero value.
Missing values Count incorrect values as missing values. Code df_missing =
df.copy() df_missing.chol = df.chol.replace({ 0 :np.nan}) df_missing.trestbps
= df.trestbps.replace({ 0 :np.nan}) df_missing.loc[df.oldpeak < 0 , 'oldpeak'
] = np.nan plot_missing(df_missing) Data Preparation Data cleaning Removing
incorrect values. Code def remove_incorrect(data = pd.DataFrame): copy =
df.copy() copy.chol.replace( 0 ,np.nan,inplace = True ) copy.trestbps.replace(
0 ,np.nan,inplace = True ) copy.loc[copy[ 'oldpeak' ] < 0 , "oldpeak" ] =
np.nan return copy df_clean = remove_incorrect(df) #plot the result fig, axs =
plt.subplots( 2 , 2 , figsize = ( 10 , 10 )) #chol sns.histplot(data = df, x =
"chol" , ax = axs[ 0 , 0 ]) axs[ 0 , 0 ].title.set_text( 'Cholesterol raw
values' ) sns.histplot(data = df_clean, x = "chol" , ax = axs[ 0 , 1 ]) axs[ 0
, 1 ].title.set_text( 'Cholesterol removed zeros' ) #oldpeak sns.histplot(data
= df, x = "oldpeak" , ax = axs[ 1 , 0 ]) axs[ 1 , 0 ].title.set_text( 'Oldpeak
raw values' ) sns.histplot(data = df_clean, x = "oldpeak" , ax = axs[ 1 , 1 ])
axs[ 1 , 1 ].title.set_text( 'Oldpeak removed negative values' ) Deleting
columns with more than 30% missing data Code df_reduced = df_clean.drop([ 'ca'
, 'thal' , 'slope' ],axis = 1 ) df_reduced.head( 1 ) age sex cp trestbps chol
fbs restecg thalach exang oldpeak disease 0 32.0 1.0 1.0 95.0 NaN NaN 0.0
127.0 0.0 0.7 1 Removing duplicate rows. Code duplicates =
df_reduced.loc[df_reduced.duplicated(), :] df_no_duplicates =
df_reduced.drop_duplicates(keep = 'first' ) duplicates age sex cp trestbps
chol fbs restecg thalach exang oldpeak disease 613 58.0 1.0 3.0 150.0 219.0
0.0 1.0 118.0 1.0 0.0 1 728 49.0 0.0 2.0 110.0 NaN 0.0 0.0 160.0 0.0 0.0 0
Outliers Code def find_outliers(data = pd.DataFrame): return [col for col in
data.columns if has_outliers(data[col])] def get_box_limits(col = pd.Series):
q1 = col.quantile( .25 ) q3 = col.quantile( .75 ) IQR = q3 \- q1 ll = q1 \- (
1.5 * IQR) ul = q3 \+ ( 1.5 * IQR) return (ll, ul) def has_outliers(col =
pd.Series): ll, ul = get_box_limits(col) upper_outliers = col[col >
ul].count() > 0 lower_outliers = col[col < ll].count() > 0 return
upper_outliers or lower_outliers outliers_columns =
find_outliers(df_no_duplicates[[ 'age' , 'trestbps' , 'chol' , 'thalach' ,
'oldpeak' ]]) fig, axs = plt.subplots( 1 , len (outliers_columns), figsize = (
20 , 7 )) for (i, col) in enumerate (outliers_columns):
df_no_duplicates[col].plot.box(ax = axs[i], fontsize = 18 ) These values are
real, and removing them might affect the modeling, so I keep them. Statistical
significance testing The best resource about univariate and bivariate
statistical analysis basics: univariate bivariate Chi square test for
categorical data Code from pandas.core.strings.accessor import isna from
itertools import combinations from scipy.stats import chi2_contingency def
test(col1 = pd.Series, col2 = pd.Series, alpha = float ): data_cont =
pd.crosstab(col1, col2) p_value = chi2_contingency(data_cont)[ 1 ] return
p_value > alpha def color_df(x): if x == True : return 'color: %s ' % 'red'
elif isna(x): return None else : return 'color: %s ' % 'green' def
show_chi_square_results(data = pd.DataFrame): categorical = [ 'sex' , 'cp' ,
'fbs' , 'restecg' , 'exang' , 'disease' ] cmb = list
(combinations(categorical, 2 )) chi_square_results_df = pd.DataFrame(index =
categorical, columns = categorical) for c in cmb: res = test(data[c[ 0 ]],
data[c[ 1 ]], 0.05 ) chi_square_results_df.loc[c[ 0 ], c[ 1 ]] = res
chi_square_results_df.loc[c[ 1 ], c[ 0 ]] = res return
chi_square_results_df.style.applymap(color_df)
show_chi_square_results(df_no_duplicates) sex cp fbs restecg exang disease sex
nan False False True False False cp False nan True False False False fbs False
True nan False True False restecg True False False nan False False exang False
False True False nan False disease False False False False False nan The table
demonstrates if a variable is statistically independent from another
variable(label ‘True’). There are no independent variables for the target
(disease) column. T-test for numeric features Two-tailed two sample t-testing
is performed. Null Hypothesis: The means of features for patients with heart
disease and healthy patients are the same. Alternative hypothesis: There are
statistically significant differences in the feature means for the healthy and
sick patients. Code from scipy.stats import ttest_ind def test_numeric(data =
pd.DataFrame, col = str ): data = data[ ~ data[col].isna()] _, p =
ttest_ind(data[col], data[ 'disease' ], equal_var = False ) return p def
show_ttest_results(data = pd.DataFrame): results = [(col, test_numeric(data,
col)) for col in [ 'chol' , 'trestbps' , 'age' , 'oldpeak' , 'thalach' ]]
return pd.DataFrame(results, columns = [ 'Column' , 'p-value' ]) Code
show_ttest_results(df_no_duplicates) Column p-value 0 chol 0.000000e+00 1
trestbps 0.000000e+00 2 age 0.000000e+00 3 oldpeak 9.389716e-19 4 thalach
0.000000e+00 I reject null hypothesis and state that there are statistically
significant differences in the feature means for the healthy and sick
patients. Keeping all the features. Split train/test 70% train, 30% test Code
train_data = df_no_duplicates.sample(frac = 0.7 , random_state = 123 )
test_data = df_no_duplicates[ ~ df_no_duplicates.index.isin(train_data.index)]
Data imputation Code from scipy.stats import mode from functools import
partial def impute(data,cols,impute_fn): imputed = data.copy() for col in
cols: imputed[col \+ "_missing" ] = imputed[col].isna().astype( int ) # flag
variable for missing imputed[col] = impute_fn(imputed[col])
#.fillna(imputed[col].mode()[0]) return imputed def impute_categorical(data =
pd.DataFrame) -> pd.DataFrame: cols = [ 'restecg' , 'exang' , 'fbs' ]
impute_fn = lambda x: x.fillna(x.mode()[ 0 ]) return
impute(data,cols,impute_fn) def impute_numeric(data = pd.DataFrame, cols =
list ) -> pd.DataFrame: cols = [ 'oldpeak' , 'trestbps' , 'thalach' , 'chol' ]
impute_fn = lambda x: x.fillna(x.median()) return impute(data,cols,impute_fn)
def plot_numeric_imputation(data_before = pd.DataFrame, data_after =
pd.DataFrame): cols = [ 'oldpeak' , 'trestbps' , 'thalach' , 'chol' ] fig, axs
= plt.subplots( 1 , len (cols), figsize = ( 25 , 5 )) for i, col in enumerate
(cols): sns.kdeplot(data_before[col], color = 'b' , fill = True , ax = axs[i],
alpha = 0.07 ) sns.kdeplot(data_after[col], color = 'r' , fill = True , ax =
axs[i], alpha = 0.07 ) axs[i].legend([ 'Before imputation' , 'After
imputation' ]) def plot_categorical_imputation(data_before = pd.DataFrame,
data_after = pd.DataFrame): cols = [ 'restecg' , 'exang' , 'fbs' ] fig, axs =
plt.subplots( 1 , len (cols), figsize = ( 25 , 5 )) for i, col in enumerate
(cols): sns.histplot(data = data_before, x = col, ax = axs[i], color = 'b' )
sns.histplot(data = data_after, x = col, ax = axs[i], color = 'r' , alpha =
0.2 ) plot_numeric_imputation(train_data, impute_numeric(train_data))
plot_categorical_imputation(train_data, impute_categorical(train_data)) One
hot encoding Code def encode_dummy(data = pd.DataFrame, drop_first = True ) ->
pd.DataFrame: df = data.copy() for col in [ 'cp' , 'restecg' ]: dummy =
pd.get_dummies(df[col], prefix = col,drop_first = drop_first) df =
pd.concat([df, dummy], axis = 1 ) df.drop(col, axis = 1 , inplace = True )
return df encode_dummy(train_data).head( 5 ) age sex trestbps chol fbs thalach
exang oldpeak disease cp_2.0 cp_3.0 cp_4.0 restecg_1.0 restecg_2.0 349 47.0
1.0 112.0 204.0 0.0 143.0 0.0 0.1 0 0 0 1 0 0 654 38.0 1.0 140.0 297.0 0.0
150.0 0.0 0.0 0 1 0 0 0 0 7 38.0 1.0 115.0 NaN 0.0 128.0 1.0 0.0 1 0 1 0 0 0
571 54.0 1.0 NaN 182.0 0.0 NaN NaN NaN 0 1 0 0 1 0 171 65.0 0.0 140.0 417.0
1.0 157.0 0.0 0.8 0 0 1 0 0 1 Feature Scaling Normalization/Standardization
Machine learning algorithms like linear regression, logistic regression,
neural network, etc. that use gradient descent as an optimization technique
require data to be scaled. Distance algorithms like KNN, K-means, and SVM are
most affected by the range of features
Normalization/Standardization.Therefore, I scale the data before employing a
distance based algorithm so that all the features contribute equally to the
result. Code from sklearn.preprocessing import MinMaxScaler def scale(data =
pd.DataFrame): return pd.DataFrame(MinMaxScaler().fit_transform(data), columns
= data.columns) def plot_normalization(data_before = pd.DataFrame, data_after
= pd.DataFrame, cols = str ): fig, axs = plt.subplots( len (cols), 2 , figsize
= ( 25 , 15 )) for i, col in enumerate (cols): sns.histplot(data_before[col],
color = 'b' , ax = axs[i, 0 ]) axs[i, 0 ].title.set_text(col \+ ' before
scaling' ) sns.histplot(data_after[col], color = 'r' , ax = axs[i, 1 ]) axs[i,
1 ].title.set_text(col \+ ' after scaling' ) plot_normalization(train_data,
scale(train_data), [ 'chol' , 'age' , 'sex' ]) Modeling Helper functions from
sklearn.ensemble import RandomForestClassifier, RandomForestRegressor from
sklearn.model_selection import train_test_split from sklearn.metrics import
accuracy_score from sklearn.linear_model import LogisticRegression def
run_modeling(model,train_data,test_data): x_train = train_data.drop( 'disease'
, axis = 1 ) y_train = train_data[ 'disease' ] model.fit(x_train, y_train)
x_test = test_data.drop( 'disease' , axis = 1 ) y_test = test_data[ 'disease'
] y_model = model.predict(x_test) return accuracy_score(y_test, y_model),
model def impute_all(data): return impute_categorical(impute_numeric(data))
def full_pipeline(data): data = impute_all(data) data = encode_dummy(data)
return scale(data) Random forest rf_train_data = impute_all(train_data)
rf_test_data = impute_all(test_data) acc, rf_model =
run_modeling(RandomForestClassifier(random_state = 0 ),
rf_train_data,rf_test_data) print ( f'accuracy: { acc } ' ) accuracy:
0.7527272727272727 Logistic regression lr_train_data =
full_pipeline(train_data) lr_test_data = full_pipeline(test_data) acc, model =
run_modeling(LogisticRegression(), lr_train_data, lr_test_data) print (
f'accuracy: { acc } ' ) accuracy: 0.7781818181818182 SVM There are many model
parameters and they are not easy to choose, so I use the GridSearchCV tool in
sklearn to help to complete the parameter selection. The main parameters
include kernel, C and gamma from sklearn import svm from
sklearn.model_selection import GridSearchCV parameters = { 'kernel' :(
'linear' , 'rbf' ), 'C' :[ 1 ], 'gamma' :[ 0.05 , 0.07 , 0.125 , 0.25 , 0.5 ]}
model = GridSearchCV(svm.SVC(), parameters, scoring = 'accuracy' )
svm_train_data = full_pipeline(train_data) svm_test_data =
full_pipeline(test_data) acc, model = run_modeling(model, svm_train_data,
svm_test_data) print ( f'accuracy: { acc } ' ) accuracy: 0.7781818181818182
Interpretation I use information from the random forest feature importance to
do interpretation. From the random forest feature importance information I
conclude that the most important variables for predicting heart disease are:
Chest pain type (cp) Maximum heart rate achieved (thalach) Oldpeak Age
Cholesterol Meanwhile such parameters as blood sugar and the results of the
electrocardiogram contributed the least to the heart disease prediction in the
random forest model. The most of the missing values flags were not important
for the model. Code (pd.Series(rf_model.feature_importances_, index =
rf_train_data.drop([ 'disease' ], axis = 1 ).columns).sort_values().plot(kind
= 'barh' , figsize = ( 10 , 10 )))

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://galax.dev/apps.html***

galax.dev - apps playground Logo Header Categories All (0) afterhoursbilly
GitHub Twitter [email protected] No matching items Back to top

***URL: https://galax.dev/index.html***

galax.dev My name is Szymon and, I am an aspiring Machine/Deep Learning
Engineer, currently in my third year of Computer Science. If you are already
here, you can check out my blog posts and demo apps in the playground below.
You can find most of the code for my projects on my GitHub. blog Click here to
check out the all blog posts. LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Nov 13, 2023 An Image-to-Image
Implementation Demonstation of an image-to-image implementation of the Stable
Diffusion model. Oct 3, 2023 No matching items playground Click here to play
more in the playground. No matching items Contact Information afterhoursbilly
GitHub Twitter [email protected]

***URL: https://galax.dev/posts/00_Image-to-Image.html***

galax.dev - An Image-to-Image Implementation Text-Guided: Image-to-Image
Implementation This Python code demonstrates the implementation of the Image-
to-Image technique, allowing you to generate new images from existing ones
with the help of textual prompts. Explore how this innovative approach
combines images and text to create visually compelling artworks. Dive into the
code to understand the mechanics behind this cutting-edge image generation
technique. Pip install necessary libraries (click to show/hide) ! pip install
\- Uq diffusers transformers fastcore fastdownload Importing utilities (click
to show/hide) from transformers import CLIPTextModel, CLIPTokenizer import
torch from diffusers import LMSDiscreteScheduler from PIL import Image from
tqdm.auto import tqdm from diffusers import AutoencoderKL,
UNet2DConditionModel import logging from fastdownload import FastDownload from
pathlib import Path from huggingface_hub import notebook_login import
matplotlib.pyplot as plt from torchvision import transforms if not
(Path.home() / '.cache/huggingface' / 'token' ).exists(): notebook_login()
logging.disable(logging.WARNING) We need to load in the required libraries and
set up the models. tokenizer = CLIPTokenizer.from_pretrained( "openai/clip-
vit-large-patch14" , torch_dtype = torch.float16) text_encoder =
CLIPTextModel.from_pretrained( "openai/clip-vit-large-patch14" , torch_dtype =
torch.float16).to( "cuda" ) # Here we use a different VAE to the original
release, which has been fine-tuned for more steps vae =
AutoencoderKL.from_pretrained( "stabilityai/sd-vae-ft-ema" , torch_dtype =
torch.float16).to( "cuda" ) unet = UNet2DConditionModel.from_pretrained(
"CompVis/stable-diffusion-v1-4" , subfolder = "unet" , torch_dtype =
torch.float16).to( "cuda" ) Define the parameters. height = 512 width = 512
num_inference_steps = 70 guidance_scale = 7.5 batch_size = 1
beta_start,beta_end = 0.00085 , 0.012 scheduler =
LMSDiscreteScheduler(beta_start = beta_start, beta_end = beta_end,
beta_schedule = "scaled_linear" , num_train_timesteps = 1000 )
plt.plot(scheduler.sigmas) plt.title( 'Noise Schedule' ) plt.xlabel( 'Sampling
step' ) plt.ylabel( 'sigma' ) plt.show() def prep_img(img_link : str ) ->
torch.Tensor: """ Preprocesses an image from a given link. Args: img_link
(str): The URL or path to the image file. Returns: torch.Tensor: A tensor
representing the preprocessed image. """ p = FastDownload().download(img_link)
init_image = Image. open (p).convert( "RGB" ).resize(( 512 , 512 )) return
transforms.ToTensor()(init_image) The image we will use as a starting point.
Downloading the image (click to show/hide) link = "https://cdn-
uploads.huggingface.co/production/uploads/1664665907257-noauth.png"
transformed_image = prep_img(link) # show image in notebook. p =
FastDownload().download(link) display(Image. open (p).convert( "RGB"
).resize(( 512 , 512 ))) def tokenization(prompt: list , max_len : int = None
) \- > torch.Tensor: """ Tokenizes a text prompt and returns the corresponding
encoded tensor. Args: prompt (list): The input text prompt to be tokenized.
max_len (int, optional): The maximum length of the tokenized sequence. If not
specified, it defaults to the maximum length allowed by the tokenizer.
Returns: torch.Tensor: A tensor containing the encoded representation of the
tokenized prompt. """ if max_len is None : max_len =
tokenizer.model_max_length tokenized_prompt = tokenizer(prompt, padding =
"max_length" , max_length = max_len, truncation = True , return_tensors = 'pt'
) return text_encoder(tokenized_prompt.input_ids.to( 'cuda' ))[ 0 ].half() def
make_image(latent: torch.Tensor): """ Converts a tensor representation of an
image into a PIL Image. Args: latent (torch.Tensor): A tensor representing an
image. Returns: PIL.Image.Image: A PIL Image representing the image. """ image
= (latent / 2 \+ 0.5 ).clamp( 0 , 1 ).detach().cpu().permute( 1 , 2 , 0
).numpy() return Image.fromarray((image * 255 ). round ().astype( "uint8" ))
Denoising loop To ensure the effectiveness of this solution, it is essential
to incorporate the “start_step” parameter. Essentially, we aim to prevent
excessive noise from being added to the input image, particularly avoiding the
most intense noise additions. After this initial step, we can proceed with the
looping process. In summary, the key to success here is to introduce the
“start_step” parameter, which helps us avoid excessive noise in the early
stages and then continue with the loop as intended. def create_sample(prompt:
list ,transformed_image: torch.Tensor ,guidance_scale: float = 7.5 , seed: int
= 5 , steps: int = 70 ,start_step: int = 10 ): ''' Generate a sample image
based on a text prompt, provided image and guidance parameters. Args: prompt
(list): A list of text prompts. transformed_image (torch.Tensor): A tensor
representing the transformed image. guidance_scale (float, optional): The
scale factor for guiding the generation process. seed (int, optional): Seed
for random number generation. Default is 5. steps (int, optional): The total
number of steps for the generation process. Default is 70. start_step (int,
optional): The step at which the generation process starts. Default is 10.
Returns: torch.Tensor: A tensor representing the generated sample. This
function generates an image based on the provided text prompts , transformed
image and parametrs.It uses a predefined VAE model to encode the image and
then applies noise and guidance to generate the sample.It iteratively refines
the image by adding noise and updating the latent representation. The guidance
scale controls the influence of the text prompts on the image. The generated
image is returned as a PyTorch tensor. Example: >>> prompt = ["Translate the
following English sentence to French: 'Hello, how are you?'"] >>>
transformed_image = prep_img(image_link) >>> generated_sample =
create_sample(prompt, transformed_image) ''' bs = 1 # Implementation for only
a single prompt. text = tokenization(prompt) uncond = tokenization([ "" ] *
bs, text.shape[ 1 ]) emb = torch.cat([uncond, text]) if seed:
torch.manual_seed(seed) # Encode image image_latent =
vae.encode((transformed_image.unsqueeze( 0 ).half().to( 'cuda'
))).latent_dist.sample() image_latent = vae.config.scaling_factor *
image_latent # Create noise scheduler.set_timesteps(steps) noise_latents =
torch.randn_like(image_latent) latents = scheduler.add_noise(image_latent,
noise_latents, timesteps = torch.tensor([scheduler.timesteps[start_step]]))
for i, ts in enumerate (tqdm(scheduler.timesteps)): if i >= start_step: # Skip
the batches of noise that don't affect the input image. inp =
scheduler.scale_model_input(torch.cat([latents] * 2 ), ts) with
torch.no_grad(): noise_pred_uncond, noise_pred_text = unet(inp, ts,
encoder_hidden_states = emb).sample.chunk( 2 ) pred = noise_pred_uncond \+
guidance_scale * (noise_pred_text \- noise_pred_uncond) latents =
scheduler.step(pred, ts, latents).prev_sample with torch.no_grad(): return
vae.decode( 1 / 0.18215 * latents).sample prompt = [ 'Wolf howling at the
moon, photorealistic 4K' ] #prompt = ['unicorn'] #prompt = ['a kids drawing of
bacteria, cartoon style'] #prompt = [' Horse looking at the morning sun,
photorealistic 4K'] image = create_sample(prompt,transformed_image,steps = 50
,seed = 1000 ) display(make_image(image[ 0 ])) Back to top

***URL: https://galax.dev/blog.html***

galax.dev - blog Categories All (2) Deep Learning (2) Implementation (2)
Stable Diffusion (1) afterhoursbilly GitHub Twitter [email protected] Order By
Default Date - Oldest Date - Newest LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Monday, 13 November 2023 2 min An Image-
to-Image Implementation Demonstation of an image-to-image implementation of
the Stable Diffusion model. Tuesday, 03 October 2023 1 min No matching items
Back to top

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch.html***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://galax.dev/blog.html***

galax.dev - blog Categories All (2) Deep Learning (2) Implementation (2)
Stable Diffusion (1) afterhoursbilly GitHub Twitter [email protected] Order By
Default Date - Oldest Date - Newest LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Monday, 13 November 2023 2 min An Image-
to-Image Implementation Demonstation of an image-to-image implementation of
the Stable Diffusion model. Tuesday, 03 October 2023 1 min No matching items
Back to top

***URL: https://galax.dev/posts/00_Image-to-Image.html***

galax.dev - An Image-to-Image Implementation Text-Guided: Image-to-Image
Implementation This Python code demonstrates the implementation of the Image-
to-Image technique, allowing you to generate new images from existing ones
with the help of textual prompts. Explore how this innovative approach
combines images and text to create visually compelling artworks. Dive into the
code to understand the mechanics behind this cutting-edge image generation
technique. Pip install necessary libraries (click to show/hide) ! pip install
\- Uq diffusers transformers fastcore fastdownload Importing utilities (click
to show/hide) from transformers import CLIPTextModel, CLIPTokenizer import
torch from diffusers import LMSDiscreteScheduler from PIL import Image from
tqdm.auto import tqdm from diffusers import AutoencoderKL,
UNet2DConditionModel import logging from fastdownload import FastDownload from
pathlib import Path from huggingface_hub import notebook_login import
matplotlib.pyplot as plt from torchvision import transforms if not
(Path.home() / '.cache/huggingface' / 'token' ).exists(): notebook_login()
logging.disable(logging.WARNING) We need to load in the required libraries and
set up the models. tokenizer = CLIPTokenizer.from_pretrained( "openai/clip-
vit-large-patch14" , torch_dtype = torch.float16) text_encoder =
CLIPTextModel.from_pretrained( "openai/clip-vit-large-patch14" , torch_dtype =
torch.float16).to( "cuda" ) # Here we use a different VAE to the original
release, which has been fine-tuned for more steps vae =
AutoencoderKL.from_pretrained( "stabilityai/sd-vae-ft-ema" , torch_dtype =
torch.float16).to( "cuda" ) unet = UNet2DConditionModel.from_pretrained(
"CompVis/stable-diffusion-v1-4" , subfolder = "unet" , torch_dtype =
torch.float16).to( "cuda" ) Define the parameters. height = 512 width = 512
num_inference_steps = 70 guidance_scale = 7.5 batch_size = 1
beta_start,beta_end = 0.00085 , 0.012 scheduler =
LMSDiscreteScheduler(beta_start = beta_start, beta_end = beta_end,
beta_schedule = "scaled_linear" , num_train_timesteps = 1000 )
plt.plot(scheduler.sigmas) plt.title( 'Noise Schedule' ) plt.xlabel( 'Sampling
step' ) plt.ylabel( 'sigma' ) plt.show() def prep_img(img_link : str ) ->
torch.Tensor: """ Preprocesses an image from a given link. Args: img_link
(str): The URL or path to the image file. Returns: torch.Tensor: A tensor
representing the preprocessed image. """ p = FastDownload().download(img_link)
init_image = Image. open (p).convert( "RGB" ).resize(( 512 , 512 )) return
transforms.ToTensor()(init_image) The image we will use as a starting point.
Downloading the image (click to show/hide) link = "https://cdn-
uploads.huggingface.co/production/uploads/1664665907257-noauth.png"
transformed_image = prep_img(link) # show image in notebook. p =
FastDownload().download(link) display(Image. open (p).convert( "RGB"
).resize(( 512 , 512 ))) def tokenization(prompt: list , max_len : int = None
) \- > torch.Tensor: """ Tokenizes a text prompt and returns the corresponding
encoded tensor. Args: prompt (list): The input text prompt to be tokenized.
max_len (int, optional): The maximum length of the tokenized sequence. If not
specified, it defaults to the maximum length allowed by the tokenizer.
Returns: torch.Tensor: A tensor containing the encoded representation of the
tokenized prompt. """ if max_len is None : max_len =
tokenizer.model_max_length tokenized_prompt = tokenizer(prompt, padding =
"max_length" , max_length = max_len, truncation = True , return_tensors = 'pt'
) return text_encoder(tokenized_prompt.input_ids.to( 'cuda' ))[ 0 ].half() def
make_image(latent: torch.Tensor): """ Converts a tensor representation of an
image into a PIL Image. Args: latent (torch.Tensor): A tensor representing an
image. Returns: PIL.Image.Image: A PIL Image representing the image. """ image
= (latent / 2 \+ 0.5 ).clamp( 0 , 1 ).detach().cpu().permute( 1 , 2 , 0
).numpy() return Image.fromarray((image * 255 ). round ().astype( "uint8" ))
Denoising loop To ensure the effectiveness of this solution, it is essential
to incorporate the “start_step” parameter. Essentially, we aim to prevent
excessive noise from being added to the input image, particularly avoiding the
most intense noise additions. After this initial step, we can proceed with the
looping process. In summary, the key to success here is to introduce the
“start_step” parameter, which helps us avoid excessive noise in the early
stages and then continue with the loop as intended. def create_sample(prompt:
list ,transformed_image: torch.Tensor ,guidance_scale: float = 7.5 , seed: int
= 5 , steps: int = 70 ,start_step: int = 10 ): ''' Generate a sample image
based on a text prompt, provided image and guidance parameters. Args: prompt
(list): A list of text prompts. transformed_image (torch.Tensor): A tensor
representing the transformed image. guidance_scale (float, optional): The
scale factor for guiding the generation process. seed (int, optional): Seed
for random number generation. Default is 5. steps (int, optional): The total
number of steps for the generation process. Default is 70. start_step (int,
optional): The step at which the generation process starts. Default is 10.
Returns: torch.Tensor: A tensor representing the generated sample. This
function generates an image based on the provided text prompts , transformed
image and parametrs.It uses a predefined VAE model to encode the image and
then applies noise and guidance to generate the sample.It iteratively refines
the image by adding noise and updating the latent representation. The guidance
scale controls the influence of the text prompts on the image. The generated
image is returned as a PyTorch tensor. Example: >>> prompt = ["Translate the
following English sentence to French: 'Hello, how are you?'"] >>>
transformed_image = prep_img(image_link) >>> generated_sample =
create_sample(prompt, transformed_image) ''' bs = 1 # Implementation for only
a single prompt. text = tokenization(prompt) uncond = tokenization([ "" ] *
bs, text.shape[ 1 ]) emb = torch.cat([uncond, text]) if seed:
torch.manual_seed(seed) # Encode image image_latent =
vae.encode((transformed_image.unsqueeze( 0 ).half().to( 'cuda'
))).latent_dist.sample() image_latent = vae.config.scaling_factor *
image_latent # Create noise scheduler.set_timesteps(steps) noise_latents =
torch.randn_like(image_latent) latents = scheduler.add_noise(image_latent,
noise_latents, timesteps = torch.tensor([scheduler.timesteps[start_step]]))
for i, ts in enumerate (tqdm(scheduler.timesteps)): if i >= start_step: # Skip
the batches of noise that don't affect the input image. inp =
scheduler.scale_model_input(torch.cat([latents] * 2 ), ts) with
torch.no_grad(): noise_pred_uncond, noise_pred_text = unet(inp, ts,
encoder_hidden_states = emb).sample.chunk( 2 ) pred = noise_pred_uncond \+
guidance_scale * (noise_pred_text \- noise_pred_uncond) latents =
scheduler.step(pred, ts, latents).prev_sample with torch.no_grad(): return
vae.decode( 1 / 0.18215 * latents).sample prompt = [ 'Wolf howling at the
moon, photorealistic 4K' ] #prompt = ['unicorn'] #prompt = ['a kids drawing of
bacteria, cartoon style'] #prompt = [' Horse looking at the morning sun,
photorealistic 4K'] image = create_sample(prompt,transformed_image,steps = 50
,seed = 1000 ) display(make_image(image[ 0 ])) Back to top

***URL: https://galax.dev/apps.html***

galax.dev - apps playground Logo Header Categories All (0) afterhoursbilly
GitHub Twitter [email protected] No matching items Back to top

***URL: https://galax.dev/index.html***

galax.dev My name is Szymon and, I am an aspiring Machine/Deep Learning
Engineer, currently in my third year of Computer Science. If you are already
here, you can check out my blog posts and demo apps in the playground below.
You can find most of the code for my projects on my GitHub. blog Click here to
check out the all blog posts. LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Nov 13, 2023 An Image-to-Image
Implementation Demonstation of an image-to-image implementation of the Stable
Diffusion model. Oct 3, 2023 No matching items playground Click here to play
more in the playground. No matching items Contact Information afterhoursbilly
GitHub Twitter [email protected]

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch.html***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://galax.dev/blog.html***

galax.dev - blog Categories All (2) Deep Learning (2) Implementation (2)
Stable Diffusion (1) afterhoursbilly GitHub Twitter [email protected] Order By
Default Date - Oldest Date - Newest LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Monday, 13 November 2023 2 min An Image-
to-Image Implementation Demonstation of an image-to-image implementation of
the Stable Diffusion model. Tuesday, 03 October 2023 1 min No matching items
Back to top

***URL: https://galax.dev/posts/00_Image-to-Image.html***

galax.dev - An Image-to-Image Implementation Text-Guided: Image-to-Image
Implementation This Python code demonstrates the implementation of the Image-
to-Image technique, allowing you to generate new images from existing ones
with the help of textual prompts. Explore how this innovative approach
combines images and text to create visually compelling artworks. Dive into the
code to understand the mechanics behind this cutting-edge image generation
technique. Pip install necessary libraries (click to show/hide) ! pip install
\- Uq diffusers transformers fastcore fastdownload Importing utilities (click
to show/hide) from transformers import CLIPTextModel, CLIPTokenizer import
torch from diffusers import LMSDiscreteScheduler from PIL import Image from
tqdm.auto import tqdm from diffusers import AutoencoderKL,
UNet2DConditionModel import logging from fastdownload import FastDownload from
pathlib import Path from huggingface_hub import notebook_login import
matplotlib.pyplot as plt from torchvision import transforms if not
(Path.home() / '.cache/huggingface' / 'token' ).exists(): notebook_login()
logging.disable(logging.WARNING) We need to load in the required libraries and
set up the models. tokenizer = CLIPTokenizer.from_pretrained( "openai/clip-
vit-large-patch14" , torch_dtype = torch.float16) text_encoder =
CLIPTextModel.from_pretrained( "openai/clip-vit-large-patch14" , torch_dtype =
torch.float16).to( "cuda" ) # Here we use a different VAE to the original
release, which has been fine-tuned for more steps vae =
AutoencoderKL.from_pretrained( "stabilityai/sd-vae-ft-ema" , torch_dtype =
torch.float16).to( "cuda" ) unet = UNet2DConditionModel.from_pretrained(
"CompVis/stable-diffusion-v1-4" , subfolder = "unet" , torch_dtype =
torch.float16).to( "cuda" ) Define the parameters. height = 512 width = 512
num_inference_steps = 70 guidance_scale = 7.5 batch_size = 1
beta_start,beta_end = 0.00085 , 0.012 scheduler =
LMSDiscreteScheduler(beta_start = beta_start, beta_end = beta_end,
beta_schedule = "scaled_linear" , num_train_timesteps = 1000 )
plt.plot(scheduler.sigmas) plt.title( 'Noise Schedule' ) plt.xlabel( 'Sampling
step' ) plt.ylabel( 'sigma' ) plt.show() def prep_img(img_link : str ) ->
torch.Tensor: """ Preprocesses an image from a given link. Args: img_link
(str): The URL or path to the image file. Returns: torch.Tensor: A tensor
representing the preprocessed image. """ p = FastDownload().download(img_link)
init_image = Image. open (p).convert( "RGB" ).resize(( 512 , 512 )) return
transforms.ToTensor()(init_image) The image we will use as a starting point.
Downloading the image (click to show/hide) link = "https://cdn-
uploads.huggingface.co/production/uploads/1664665907257-noauth.png"
transformed_image = prep_img(link) # show image in notebook. p =
FastDownload().download(link) display(Image. open (p).convert( "RGB"
).resize(( 512 , 512 ))) def tokenization(prompt: list , max_len : int = None
) \- > torch.Tensor: """ Tokenizes a text prompt and returns the corresponding
encoded tensor. Args: prompt (list): The input text prompt to be tokenized.
max_len (int, optional): The maximum length of the tokenized sequence. If not
specified, it defaults to the maximum length allowed by the tokenizer.
Returns: torch.Tensor: A tensor containing the encoded representation of the
tokenized prompt. """ if max_len is None : max_len =
tokenizer.model_max_length tokenized_prompt = tokenizer(prompt, padding =
"max_length" , max_length = max_len, truncation = True , return_tensors = 'pt'
) return text_encoder(tokenized_prompt.input_ids.to( 'cuda' ))[ 0 ].half() def
make_image(latent: torch.Tensor): """ Converts a tensor representation of an
image into a PIL Image. Args: latent (torch.Tensor): A tensor representing an
image. Returns: PIL.Image.Image: A PIL Image representing the image. """ image
= (latent / 2 \+ 0.5 ).clamp( 0 , 1 ).detach().cpu().permute( 1 , 2 , 0
).numpy() return Image.fromarray((image * 255 ). round ().astype( "uint8" ))
Denoising loop To ensure the effectiveness of this solution, it is essential
to incorporate the “start_step” parameter. Essentially, we aim to prevent
excessive noise from being added to the input image, particularly avoiding the
most intense noise additions. After this initial step, we can proceed with the
looping process. In summary, the key to success here is to introduce the
“start_step” parameter, which helps us avoid excessive noise in the early
stages and then continue with the loop as intended. def create_sample(prompt:
list ,transformed_image: torch.Tensor ,guidance_scale: float = 7.5 , seed: int
= 5 , steps: int = 70 ,start_step: int = 10 ): ''' Generate a sample image
based on a text prompt, provided image and guidance parameters. Args: prompt
(list): A list of text prompts. transformed_image (torch.Tensor): A tensor
representing the transformed image. guidance_scale (float, optional): The
scale factor for guiding the generation process. seed (int, optional): Seed
for random number generation. Default is 5. steps (int, optional): The total
number of steps for the generation process. Default is 70. start_step (int,
optional): The step at which the generation process starts. Default is 10.
Returns: torch.Tensor: A tensor representing the generated sample. This
function generates an image based on the provided text prompts , transformed
image and parametrs.It uses a predefined VAE model to encode the image and
then applies noise and guidance to generate the sample.It iteratively refines
the image by adding noise and updating the latent representation. The guidance
scale controls the influence of the text prompts on the image. The generated
image is returned as a PyTorch tensor. Example: >>> prompt = ["Translate the
following English sentence to French: 'Hello, how are you?'"] >>>
transformed_image = prep_img(image_link) >>> generated_sample =
create_sample(prompt, transformed_image) ''' bs = 1 # Implementation for only
a single prompt. text = tokenization(prompt) uncond = tokenization([ "" ] *
bs, text.shape[ 1 ]) emb = torch.cat([uncond, text]) if seed:
torch.manual_seed(seed) # Encode image image_latent =
vae.encode((transformed_image.unsqueeze( 0 ).half().to( 'cuda'
))).latent_dist.sample() image_latent = vae.config.scaling_factor *
image_latent # Create noise scheduler.set_timesteps(steps) noise_latents =
torch.randn_like(image_latent) latents = scheduler.add_noise(image_latent,
noise_latents, timesteps = torch.tensor([scheduler.timesteps[start_step]]))
for i, ts in enumerate (tqdm(scheduler.timesteps)): if i >= start_step: # Skip
the batches of noise that don't affect the input image. inp =
scheduler.scale_model_input(torch.cat([latents] * 2 ), ts) with
torch.no_grad(): noise_pred_uncond, noise_pred_text = unet(inp, ts,
encoder_hidden_states = emb).sample.chunk( 2 ) pred = noise_pred_uncond \+
guidance_scale * (noise_pred_text \- noise_pred_uncond) latents =
scheduler.step(pred, ts, latents).prev_sample with torch.no_grad(): return
vae.decode( 1 / 0.18215 * latents).sample prompt = [ 'Wolf howling at the
moon, photorealistic 4K' ] #prompt = ['unicorn'] #prompt = ['a kids drawing of
bacteria, cartoon style'] #prompt = [' Horse looking at the morning sun,
photorealistic 4K'] image = create_sample(prompt,transformed_image,steps = 50
,seed = 1000 ) display(make_image(image[ 0 ])) Back to top

***URL: https://galax.dev/apps.html***

galax.dev - apps playground Logo Header Categories All (0) afterhoursbilly
GitHub Twitter [email protected] No matching items Back to top

***URL: https://galax.dev/index.html***

galax.dev My name is Szymon and, I am an aspiring Machine/Deep Learning
Engineer, currently in my third year of Computer Science. If you are already
here, you can check out my blog posts and demo apps in the playground below.
You can find most of the code for my projects on my GitHub. blog Click here to
check out the all blog posts. LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Nov 13, 2023 An Image-to-Image
Implementation Demonstation of an image-to-image implementation of the Stable
Diffusion model. Oct 3, 2023 No matching items playground Click here to play
more in the playground. No matching items Contact Information afterhoursbilly
GitHub Twitter [email protected]

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch.html***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

