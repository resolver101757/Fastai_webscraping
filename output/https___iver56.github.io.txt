***URL: https://viktoranchutin.github.io/blog.html***

Engineering notes - Blog Spark RDD with distributed machine learning
Understanding Spark computational model with logistic regression and
clustering Applying vision models to audio data Fine-tuning ViTs and ConvNets
on spectrogram data Gradient accumulation Toy example for gradient
accumulation understanding Training with variable length data Building a
dataloader to train deep learning models on variable length data No matching
items

***URL: https://viktoranchutin.github.io/blog/variable_length_training.html***

Engineering notes - Training with variable length data There are several
different ways we can deal with variable length data when training deep
learning models: Cut or pad all the samples to the maximum length in the whole
dataset Cut or pad samples to the maximum length within a mini-batch Split the
dataset into multiple buckets with samples of similar length. I will describe
the third option as it imposes the least memory and computational overhead.
This option can be used to train CNNs, RNNs or transformers with relative
positional encoding, since they can be trained on variable length data. For
example we can train Wav2Vec 2.0 model with audio samples of different length
as it encodes audio with CNN and is using convolutional relative positional
encoding as well. 1\. Splitting the data Generating dataset with variable
length items. from random import randint import torch import pandas as pd #
generate dataset min_length = 2 max_length = 20 size = 1000 dummy_y = 0
dataset = [(torch.randn(randint(min_length,max_length)),dummy_y) for _ in
range (size)] Create a dataframe with information about items length df =
pd.DataFrame([( id , len (x)) for id ,(x,y) in enumerate (dataset)], columns =
[ 'id' , 'length' ]) df.length.plot(kind = 'hist' ,title = "Length
distribution" ) ; Split data into bukets nbuckets = 10 df[ 'bucket' ] =
pd.cut(df.length, bins = nbuckets, labels = range (nbuckets)) 2\. Create
dataloaders Create DataSet class, which is using a dataframe with items ids to
retrieve them from the original dataset. from torch.utils.data import
DataLoader from torch.nn.functional import pad class DataSet: def __init__ (
self ,dataframe,data): self .df = dataframe.reset_index(drop = True ) # items
ids self .data = data def __getitem__ ( self ,index): id = self
.df.iloc[index]. id # get item by id from the original dataset return self
.data[ id ] def __len__ ( self ): return len ( self .df) Collate function adds
padding according to the maximum length in a batch def collate_fn(batch):
xs,ys = [ list (b) for b in ( zip ( * batch))] maxl = max ([ len (x) for x in
xs]) # maxl in a batch for i in range ( len (xs)): xs[i] = pad(xs[i],( 0 ,maxl
\- len (xs[i]))) # pad to maxl x = torch.stack(xs) y = torch.tensor(ys) return
(x,y) Create dataloaders for each bucket def create_dataloader(dataframe,bs =
4 ): return DataLoader(DataSet(dataframe, dataset), bs, shuffle = True ,
collate_fn = collate_fn) dataloaders = [] for bucket_id in df.bucket.unique():
dl = create_dataloader(df[df.bucket == bucket_id]) dataloaders.append(dl) 3\.
Create random iterator The iterator takes iterators from the dataloaders and
randomly chooses one at the each next call from random import choice class
DLIterator: def __init__ ( self , dls) -> None : self .iters = [ iter (dl) for
dl in dls] def __iter__ ( self ): return self def __next__ ( self ): for _ in
range ( len ( self .iters)): # iterate in case some are empty try : it =
choice( self .iters) return next (it) except StopIteration : self
.iters.remove(it) raise StopIteration class MultiDataLoader: '''Combining
multiple dataloaders.''' def __init__ ( self ,dataloaders) -> None : self .dls
= dataloaders def __iter__ ( self ): return DLIterator( self .dls) def __len__
( self ): return sum ( map ( len , self .dls)) Check the distribution of batch
lengths for the obtained dataloader import matplotlib.pyplot as plt
batch_sizes = [xb.shape[ 1 ] for xb,_ in MultiDataLoader(dataloaders)]
plt.hist(batch_sizes) ; Visualize batch lengths: it = iter
(MultiDataLoader(dataloaders)) _,ax = plt.subplots( 5 ) for i in range ( 5 ):
ax[i].imshow( next (it)[ 0 ])

***URL: https://viktoranchutin.github.io/blog/Spark RDD.html***

Engineering notes - Spark RDD with distributed machine learning In this blog
post I take a close look into the Spark computational model by implementing 2
machine learning algorithms: Logistic Regression and K-Means clustering. The
code is executed in the Databricks environment using Scala. Python is used for
visualization. Databricks notebook Spark was developed to address iterative
big data algorithms like logistic regression (gradient descent) or k-means
clustering. From the RDDs paper : K-Means clustering Data For K-means
clustering let’s generate 5 clusters of data. Generate data (Python) import
matplotlib.pyplot as plt from sklearn.datasets import make_blobs import numpy
as np import csv n_samples = 10000 n_features = 2 n_clusters = 5 data, labels
= make_blobs(n_samples = n_samples, n_features = n_features, centers =
n_clusters, random_state = 12345 ) def plot_centroids(centroids,ax): for i,
centroid in enumerate (centroids): samples = data[i * n_samples:(i \+ 1 ) *
n_samples] ax.plot( * centroid, markersize = 10 , marker = "x" , color = 'k' ,
mew = 5 ) ax.plot( * centroid, markersize = 5 , marker = "x" , color = 'm' ,
mew = 2 ) _,ax = plt.subplots() ax.scatter(data[:, 0 ], data[:, 1 ], c =
labels, cmap = 'viridis' ) centroids = [] for cluster_label in range
(n_clusters): cluster_points = data[labels == cluster_label] # Select data
points in the current cluster cluster_centroid = np.mean(cluster_points, axis
= 0 ) # Calculate the centroid for the cluster
centroids.append(cluster_centroid) plot_centroids(centroids,ax) print
(centroids) #save data with open ( 'data.csv' , 'w' , newline = '' ) as file :
writer = csv.writer( file ) writer.writerows(data) [array([ 8.57032532,
-3.64137432]), array([-6.34270089, -5.91962725]), array([1.3294662 ,
1.87901003]), array([9.31274039, 3.05338878]), array([4.96798439,
3.09705577])] Let’s read the data and create an RDD of data points, Also
important to make sure that RDD of data points will be cached, so we don’t
need to recompute it for each iteration. import scala . io . Source def
getPointsRDD (): RDD [( Float , Float )] = { val source = Source . fromFile (
"data.csv" ) val linesRDD = sc . parallelize ( source . getLines (). toList )
return linesRDD . map ( _ . split ( ',' ). map ( _ . toFloat )). map ({ case
Array ( x , y ) => ( x , y )}) } val points = getPointsRDD (). cache () We can
see that data points were split by Spark in 8 partitions: points .
getNumPartitions res5: Int = 8 Initialize centroids The first step of k-means
clustering is to initialize the first estimates of the centroids. For this
example I randomly sample 5 points, but in real applications this
initialization step usually involves more sophisticated sampling. val
randomMeans = points . takeSample ( withReplacement = false , num = 5 , seed =
10 ) save centroids for plotting import java . io . PrintWriter def saveResult
( means : Array [( Float , Float )]) = { val writer = new PrintWriter (
"means.txt" ) try { means . foreach ( writer . println ) } finally { writer .
close () } } saveResult ( randomMeans ) Plot centroids (Python) # Open the
file in read mode def readResults() -> list : with open ( 'means.txt' , 'r' )
as file : lines = file .readlines() return [ tuple ( map ( float , line.strip(
'() \n ' ).split( ',' ))) for line in lines] def plot_results(centroids,ax):
for i, centroid in enumerate (centroids): samples = data[i * n_samples:(i \+ 1
) * n_samples] ax.plot( * centroid, markersize = 5 , marker = "*" , color =
'r' , mew = 5 ) first_state = readResults() _,ax = plt.subplots()
plot_results(first_state,ax) plot_centroids(centroids,ax) ax.set_title(
'Initial centroids against true centroids' ) Out[5]: Text(0.5, 1.0, 'Initial
centroids against true centrids') Define computational graph Let’s implement
k-means clustering. The main function update \- conceptually performs 2 steps:
Groups the points by the closest centroids Finds the centre of the groups,
effectively obtaining the new estimate for centroids def euclideanDistance (
v1 : ( Float , Float ), v2 : ( Float , Float )): Double = ( v1 . _1 \- v2 . _1
) * ( v1 . _1 \- v2 . _1 ) \+ ( v1 . _2 \- v2 . _2 ) * ( v1 . _2 \- v2 . _2 )
/** Return the center that is the closest to ` p ` */ def findClosest ( p : (
Float , Float ), centers : Array [( Float , Float )]): ( Float , Float ) =
centers . minBy ( euclideanDistance ( _ , p )) def updateMeans ( means : Array
[( Float , Float )], points : RDD [( Float , Float )]): Array [( Float , Float
)] = return points . map ( point => ( findClosest ( point , means ), point ))
// pair (closest mean, point) . mapValues ( point => ( point , 1 )) // add
counter for aggregation . reduceByKey ({ case (( p1 , cnt1 ),( p2 , cnt2 )) =>
(( p1 . _1 \+ p2 . _1 , p1 . _2 \+ p2 . _2 ), cnt1 \+ cnt2 )}) // sum all the
points around a centroid . mapValues ({ case ( sum , count ) => ( sum . _1 /
count , sum . _2 / count )}) // average aggregated points -> new centroid .
map ({ case ( oldMean , newMean ) => newMean }) . collect () Running the
algorithm Let’s run 10 iterations and look at the result. var means =
randomMeans for ( i <\- 0 to 10 ){ means = updateMeans ( means , points ) }
save results for plotting saveResult ( means ) The algorithm successfully
found true centroids of clusters. Plot results (Python) results =
readResults() _,ax = plt.subplots( 1 , 3 ,figsize = ( 10 , 3 ))
plot_results(results,ax[ 0 ]) plot_centroids(centroids,ax[ 0 ])
plot_results(results,ax[ 1 ]) plot_centroids(centroids,ax[ 2 ]) ax[ 0
].set_title( 'True centroids and estimations' ) ax[ 1 ].set_title(
'Estimations' ) ax[ 2 ].set_title( 'True centroids' ) Out[5]: Text(0.5, 1.0,
'True centroids') Execution analysis First, Spark builds a graph of
computations and only when we call action functions such as .collect() it
executes the graph. Let’s look at the diagram of the execution process. Spark
driver creates closures with centroids and sends them to the executors.
Executors apply closures received by the driver to the partitions. From the
spark paper : “..users provide arguments to RDD operations like map by passing
closures (function literals). Scala represents each closure as a Java object,
and these objects can be serialized and loaded on another node to pass the
closure across the network. Scala also saves any variables bound in the
closure as fields in the Java object.” First each partition of points is
transformed to the pairs of points and the corresponding closest centroid.
Then we have reduceByKey followed by shuffle and the average. It is
conceptually the same as grouping the points by key and taking the average,
but computationally more optimal. If we used groupByKey, then the shuffle
operation would have to send 10000 points over the network in the worst case.
With reduceByKey operation, reduction happens before shuffle occurs,
significantly reducing the amount of data to send. In this case for each
cluster data points are reduced to a single pair of sum and counts, which
means that at most 5 (number of clusters) * 8 (number of partitions) = 40
pairs would need to be sent over the network. The obtained centroids are then
sent back to the driver. Next iteration driver sends updated centroids back to
the executors for recomputation. Logistic regression Data Let’s generate data
for binary classification problem generate data (Python) import numpy as np
import matplotlib.pyplot as plt from sklearn.datasets import
make_classification X, y = make_classification(n_samples = 10000 , n_features
= 2 , n_informative = 2 , n_redundant = 0 , n_clusters_per_class = 1 ,
random_state = 123 ) # Plot the dataset plt.figure(figsize = ( 8 , 6 ))
plt.scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = 'viridis' , marker = 'o' ,
edgecolors = 'k' ) plt.xlabel( 'Feature 1' ) plt.ylabel( 'Feature 2' )
Out[15]: Text(0, 0.5, 'Feature 2') save data (Python) import numpy as np data
= np.column_stack((X.astype( str ), y.astype( str ))) np.savetxt(
'classification_data.txt' , data, fmt = ' %s ' ) reading data import scala .
io . Source case class Point ( coordinates : List [ Double ], label : Int )
def readData (): List [ Point ] = { val filePath = "classification_data.txt"
val lines = Source . fromFile ( filePath ). getLines (). toList val points :
List [ Point ] = lines . map { line => val fields = line . split ( " \\\ s+" )
val x = fields . init . map ( _ . toDouble ). toList val y = fields . last .
toInt Point ( x , y ) } return points } val points = readData () Model For a 2
dimensional problem binary classification model would look like this: \\(p =
\sigma(\beta_0 + \beta_1*x_1 + \beta_2*x_2)\\) where p is the probability of a
data point to belong to class 1, \\(\beta_i\\) \- learnable model parameters
We can implement it in Scala in the following way: import math . exp def
linear ( x : List [ Double ], beta : List [ Double ]): Double = return ( 1.0
:: x ). zip ( beta ). map ({ case ( a , b ) => a * b }). reduce ( _ \+ _ ) //
dot product x*beta, append 1 for beta_0 def sigmoid ( x : Double ): Double = 1
/ ( 1 \+ math . exp (- x ). toFloat ) def model ( x : List [ Double ], beta :
List [ Double ]): Double = sigmoid ( linear ( x , beta )) Cost function and
gradient We need to minimize the cost function \\(J\\) : \\(J = - \frac{1}{N}
* \sum_{k=0}^{N}[y_k*log(p_k) + (1-y_k)*log(1-p_k)]\\) where \\(N\\) \- total
number of points And the corresponding partial derivative with respect to each
parameter is: \\(\frac{dJ}{d{\beta}_i} = \frac{1}{N} * \sum_{k=0}^{N} x_i*(p_k
- y_k)\\) For gradient descent we need to compute gradient at each iteration
and update parameters. Implementation of the gradient
\\([\frac{dJ}{d{{\beta}}_0}, \frac{dJ}{d{{\beta}}_1},
\frac{dJ}{d{\beta}_2}]\\) computation and loss function: def
partial_derivative ( y : Int , pred : Double , x : Double ): Double = ( pred
\- y )* x def gradient ( xs : List [ Double ], pred : Double , y : Int ): List
[ Double ] = { ( 1.0 :: xs ). map ( x => partial_derivative ( y , pred , x ))
// add 1 to xs to acount for b0 } def loss ( pred : Double , y : Int ): Double
= { -( y * log ( pred ) \+ ( 1 \- y )* log ( 1 \- pred )) } Define
computational graph Let’s define our computation graph for one iteration of
logistic regression. We want to compute gradients for all data points and find
their average. For monitoring also let’s compute loss for each iteration as
well. import math . log /* Sum gradients and loss values */ def sum ( a :(
List [ Double ], Double , Int ), b :( List [ Double ], Double , Int )): ( List
[ Double ], Double , Int ) = { val ( grad1 , loss1 , cnt1 ) = a val ( grad2 ,
loss2 , cnt2 ) = b val grad_sum = grad1 . zip ( grad2 ). map ({ case ( g1 , g2
) => g1 \+ g2 }) val loss_sum = loss1 \+ loss2 val count = cnt1 \+ cnt2 (
grad_sum , loss_sum , count ) } /*Compute average for gradients and the loss
value*/ def average ( grad_sum : List [ Double ], loss_sum : Double , counts :
Int ): ( List [ Double ], Double ) = { ( grad_sum . map ( _ / counts ),
loss_sum / counts ) } /*Compute gradient and loss value*/ def compute ( points
: RDD [ Point ], params : List [ Double ]): ( List [ Double ], Double , Int )
= points . map ({ case Point ( xs , y ) => ( Point ( xs , y ), model ( xs ,
params ))}) // get predictions . map ({ case ( Point ( xs , y ), pred ) => (
gradient ( xs , pred , y ), loss ( pred , y ), 1 )}) // compute gradient and
loss . reduce ( sum ) Running the training Running the training. 10
iterations. Compute average gradient and loss for each iteration and perform
gradient descent step \\(parameters_n = parameters_{n-1} - \alpha * \nabla
J\\) val pointsRDD = sc . parallelize ( points ) var parameters = List ( 0.1 ,
0.1 , 0.1 ) // b0,b1,b2 random val step_size = 3 for ( i <\- 0 to 10 ){ val (
gradSum , lossSum , cnt ) = compute ( pointsRDD , parameters ) val ( grad ,
loss ) = average ( gradSum , lossSum , cnt ) parameters = parameters . zip (
grad ). map ({ case ( param , g ) => param \- step_size * g }) println ( loss
) } loss output 0.7985616355198335 0.07500786607886242 0.06578866845350452
0.05999996600024983 0.05603349371988779 0.05315514097244599
0.05097938292867146 0.04928355967304257 0.04792987072323888
0.046828371238424885 0.04591785644633872 Computing accuracy: val accuracy =
pointsRDD . map ( point => ( point . label , model ( point . coordinates ,
parameters ))) . map ({ case ( y , prediction ) => if ( ( prediction > 0.5 )
== ( y == 1 ) ) 1 else 0 }) . mean accuracy = 0.9945 Let’s plot the decision
boundary for the obtained model: save results import java . io . PrintWriter
def saveParameters ( parameters : List [ Double ]) = { new PrintWriter (
"parameters.txt" ) { // Iterate through the array and write each element to
the file parameters . foreach ( println ) close () } } saveParameters (
parameters ) plot results (Python) with open ( 'parameters.txt' , 'r' ) as
file : parameters = [ float (line.strip()) for line in file ] def
get_decision_line(b0,b1,b2): c = \- b0 / b2 m = \- b1 / b2 return lambda x: m
* x \+ c xs = np.linspace( \- 5 , 5 ) ys = get_decision_line( *
parameters)(xs) plt.figure(figsize = ( 8 , 6 )) plt.scatter(X[:, 0 ], X[:, 1
], c = y, cmap = 'viridis' , marker = 'o' , edgecolors = 'k' ) plt.plot(xs,
ys) Out[17]: [] Execution analysis Logistic regression iteration doesn’t
require shuffle operation. We simply apply the sequence of map operations to
compute gradients and losses for each point and then do reduce to aggregate.
The final step of the iteration happens on the driver. The Driver receives
aggregated gradients and losses from the executors, computes the average and
performs gradient descent update step. The updated parameters are then sent to
the executors for the next iteration.

***URL: https://viktoranchutin.github.io/projects.html***

Engineering notes - Projects Detecting active emotions in speech Training 5
different models to perform active emotion recognition from speech ResNet from
scratch for rice disease classification Building a custom ResNet model using
Pytorch Heart disease prediction Data analysis, Random Forest and Logistic
Regression with heart disease dataset Controlling magnetic field in electric
motors How I built BLDC motor control system from scratch in my Robotics
masters degree No matching items

***URL: https://viktoranchutin.github.io/index.html***

Engineering notes - Viktor Anchutin LinkedIn GitHub Resume About Passionate
software engineer with a love for machine learning, data-driven applications
and robots I have 2 master’s degrees: Data Science and Robotics. Committed to
learning and growing in the field of AI I started as a robotics software
developer, using C and C++, and later transitioned to backend engineering with
Kotlin/Java. I’ve played a key role in building various data-intensive
systems, showcasing my skills in system thinking and design. Blog Projects No
matching items

***URL: https://viktoranchutin.github.io/projects/Heart_disease_project.html***

Engineering notes - Heart disease prediction Imports import pandas as pd
import numpy as np import seaborn as sns import matplotlib.pyplot as plt
import scipy.stats as stats import palettable import seaborn as sns from
sklearn.ensemble import RandomForestRegressor from sklearn.impute import
SimpleImputer from sklearn.model_selection import cross_val_score Data The
data is available at UCI machine learning repository . The website contains 4
datasets concerning heart disease diagnosis.The data for these datasets was
collected from the four following locations: 1\. Hungarian Institute of
Cardiology. Budapest: Andras Janosi, M.D. 2\. University Hospital, Zurich,
Switzerland: William Steinbrunn, M.D. 3\. University Hospital, Basel,
Switzerland: Matthias Pfisterer, M.D. 4\. V.A. Medical Center, Long Beach and
Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D. There are 2 versions
of each dataset: Full dataset with 76 attributes Dataset with 14 attributes
The reduced dataset exists because only the subset of 14 attributes has been
used in prior research and experiments. Files used for this project:
processed.switzerland.data processed.cleveland.data processed.hungarian.data
processed.va.data I create a single dataset by combining these four. Download
data ! wget https: // archive.ics.uci.edu / static / public / 45 / heart \+
disease. zip ! mkdir heart_disease_data ! unzip heart \+ disease. zip \- d
heart_disease_data Creating a single dataset The datasets have the same
columns, they don’t have headers, and missing values are provided as ? . Code
columns = [ "age" , "sex" , "cp" , "trestbps" , "chol" , "fbs" , "restecg" ,
"thalach" , "exang" , "oldpeak" , "slope" , "ca" , "thal" , "disease" ] sdf =
pd.read_csv( "heart_disease_data/processed.switzerland.data" , header = None ,
names = columns, na_values = '?' ) cdf = pd.read_csv(
"heart_disease_data/processed.cleveland.data" , header = None , names =
columns, na_values = '?' ) hdf = pd.read_csv(
"heart_disease_data/processed.hungarian.data" , header = None , names =
columns, na_values = '?' ) vdf = pd.read_csv(
"heart_disease_data/processed.va.data" , header = None , names = columns,
na_values = '?' ) df = pd.concat([sdf, cdf, vdf, hdf], ignore_index = True )
df.disease = df[ 'disease' ]. apply ( lambda x: 1 if x > 0 else 0 ) df age sex
cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal disease 0
32.0 1.0 1.0 95.0 0.0 NaN 0.0 127.0 0.0 0.7 1.0 NaN NaN 1 1 34.0 1.0 4.0 115.0
0.0 NaN NaN 154.0 0.0 0.2 1.0 NaN NaN 1 2 35.0 1.0 4.0 NaN 0.0 NaN 0.0 130.0
1.0 NaN NaN NaN 7.0 1 3 36.0 1.0 4.0 110.0 0.0 NaN 0.0 125.0 1.0 1.0 2.0 NaN
6.0 1 4 38.0 0.0 4.0 105.0 0.0 NaN 0.0 166.0 0.0 2.8 1.0 NaN NaN 1 ... ... ...
... ... ... ... ... ... ... ... ... ... ... ... 915 52.0 1.0 4.0 160.0 331.0
0.0 0.0 94.0 1.0 2.5 NaN NaN NaN 1 916 54.0 0.0 3.0 130.0 294.0 0.0 1.0 100.0
1.0 0.0 2.0 NaN NaN 1 917 56.0 1.0 4.0 155.0 342.0 1.0 0.0 150.0 1.0 3.0 2.0
NaN NaN 1 918 58.0 0.0 2.0 180.0 393.0 0.0 0.0 110.0 1.0 1.0 2.0 NaN 7.0 1 919
65.0 1.0 4.0 130.0 275.0 0.0 1.0 115.0 1.0 1.0 2.0 NaN NaN 1 920 rows × 14
columns Attributes Numerical attributes age : age in years, numerical trestbps
\- resting blood pressure (in mm Hg on admission to the hospital) chol \-
cholesterol in mg/dl thalach \- maximum heart rate achieved oldpeak \- ST
depression induced by exercise relative to rest. ‘ST’ relates to the positions
on the electrocardiographic (ECG) plot. ca \- number of major vessels (0-3)
colored by flouroscopy. Fluoroscopy is one of the most popular non-invasive
coronary artery disease diagnosis. It enables the doctor to see the flow of
blood through the coronary arteries in order to evaluate the presence of
arterial blockages. Categorical attributes sex : sex 1 = male 0 = female cp :
chest pain type 1: typical angina 2: atypical angina 3: non-anginal pain 4:
asymptomatic fbs \- fasting blood sugar > 120 mg/dl 1 = true 0 = false restecg
\- resting electrocardiographic (ECG) results 0: normal 1: having ST-T wave
abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
2: showing probable or definite left ventricular hypertrophy by Estes’
criteria exang \- exercise induced angina. Angina is a type of chest pain
caused by reduced blood flow to the heart. 1 - yes 0 - no slope \- the slope
of the peak exercise ST segment. (ECG) 1: upsloping 2: flat 3: downsloping
thal \- A blood disorder called thalassemia 3: normal blood flow 6: fixed
defect (no blood flow in some part of the heart) 7: reversable defect (a blood
flow is observed but it is not normal) disease \- refers to the presence of
heart disease in the patient. It is integer valued from 0 (no presence) to 4.
Exploratory data analysis EDA helper functinos df_eda = df.copy() def
plot_categorical(data = pd.DataFrame, column = str , labels = [], target =
'disease' , target_labels = [ 'healthy' , 'heart disease' ], title = '' , font
= 10 , ax = None ): crosstab = pd.crosstab(data[column], data[target]) ax =
crosstab.plot(kind = 'bar' , figsize = ( 15 , 7 ), rot = 0 , fontsize = font,
ax = ax) x = np.arange( len (labels)) ax.set_xticks(x)
ax.set_xticklabels(labels) ax.legend(target_labels) plt.ylabel( "count" , size
= 14 ) plt.title(title) bars = ax.patches #compute percents total_by_category
= crosstab. sum (axis = 1 ) healthy_perc = round ((crosstab[ 0 ] /
total_by_category ) * 100 ) for (i, bar) in enumerate (bars): prc =
healthy_perc.iloc[i] if i < len (healthy_perc) else 100 \- healthy_perc.iloc[i
% len (healthy_perc)] plt.annotate( str ( int (prc)) \+ '%' , (bar.get_x() \+
bar.get_width() / 2\. , bar.get_height()), ha = 'center' , va = 'center' ,
xytext = ( 0 , 9 ), textcoords = 'offset points' ) def plot_numeric(data =
pd.DataFrame, column = str , title = str ): fig, ax = plt.subplots( 1 , 2 )
fig.set_size_inches( 20 , 7 ) #with respect to the target healthy =
data.loc[data.disease == 0 , column] sick = data.loc[data.disease == 1 ,
column] healthy.plot.density(ax = ax[ 0 ]) sick.plot.density(ax = ax[ 0 ]) ax[
0 ].legend([ 'healthy' , 'heart disease' ]) data.boxplot(by = 'disease' ,
column = [column], ax = ax[ 1 ]) fig.suptitle(title, fontsize = 19 ) def
describe_numeric(data = pd.DataFrame, column = str ): temp = data[[column,
'disease' ]].copy() temp[ 'healthy' ] = data[data.disease == 0 ][column] temp[
'sick' ] = data[data.disease == 1 ][column] return temp[[column, 'healthy' ,
'sick' ]].describe() def plot_missing(data = pd.DataFrame): na_values_percent
= data.isna(). sum ().sort_values(ascending = False ) \ . apply ( lambda x:
(x, round (x / data.index.size * 100 , 2 ))) # (count, %) na_values_percent.
apply ( lambda x: x[ 1 ]).plot.bar() # (plot %) plt.ylabel( "Percentage" ,
size = 14 ) plt.title( 'Missing values' ) Numeric data summary Code df_numeric
= df_eda.loc[:,[ 'age' , 'trestbps' , 'chol' , 'thalach' , 'oldpeak' ]]
df_numeric.describe() age trestbps chol thalach oldpeak count 920.000000
861.000000 890.000000 865.000000 858.000000 mean 53.510870 132.132404
199.130337 137.545665 0.878788 std 9.424685 19.066070 110.780810 25.926276
1.091226 min 28.000000 0.000000 0.000000 60.000000 -2.600000 25% 47.000000
120.000000 175.000000 120.000000 0.000000 50% 54.000000 130.000000 223.000000
140.000000 0.500000 75% 60.000000 140.000000 268.000000 157.000000 1.500000
max 77.000000 200.000000 603.000000 202.000000 6.200000 Code dcorr =
df_numeric.corr(method = 'pearson' ) sns.heatmap(data = dcorr,annot = True
,fmt = ".2f" )  Categorical data summary Code plt.figure(figsize = ( 5 , 5
),dpi = 200 ) for (i, col) in enumerate ([ 'sex' , 'cp' , 'fbs' , 'restecg' ,
'exang' , 'disease' , 'thal' , 'slope' ]): plt.subplot( 3 , 3 ,i \+ 1 )
df_eda[col].value_counts().sort_index().plot(kind = 'pie' , figsize = ( 7 , 5
), autopct = ' %1.1f%% ' , title = col, textprops = { 'fontsize' : 5 })
Analysing risk factors for heart disease Cholesterol (‘chol’) Cholesterol has
a lot of 0 values, which is not possible. Code df_eda.chol.hist(bins = 20 )
df_eda[ 'chol' ] = df_eda[ 'chol' ].replace({ 0 :np.nan}) Analysis People with
heart disease on average have a higher cholesterol level. Code
plot_numeric(df_eda, 'chol' , 'Cholesterol' ) Code describe_numeric(df_eda,
'chol' ) chol healthy sick count 718.000000 372.000000 346.000000 mean
246.832869 240.158602 254.008671 std 58.527062 55.767559 60.620439 min
85.000000 85.000000 100.000000 25% 210.000000 204.000000 216.000000 50%
239.500000 233.000000 248.000000 75% 276.750000 270.250000 284.750000 max
603.000000 564.000000 603.000000 Binning shows that people with cholesterol
level more than 254 mg/dl have more than 50% chance of having a heart disease.
Code df_eda[ 'chol_range' ] = pd.qcut(df_eda[ 'chol' ], 10 , duplicates =
'drop' ) plot_categorical(df_eda, 'chol_range' ,
df_eda.chol_range.unique().sort_values(), title = 'Cholesterol intervals' )
Age People with heart disease on average are about 5 years older than healthy
people. Code df_eda.age.hist(bins = 20 )  Code describe_numeric(df_eda, 'age'
) age healthy sick count 920.000000 411.000000 509.000000 mean 53.510870
50.547445 55.903733 std 9.424685 9.433700 8.718959 min 28.000000 28.000000
31.000000 25% 47.000000 43.000000 51.000000 50% 54.000000 51.000000 57.000000
75% 60.000000 57.000000 62.000000 max 77.000000 76.000000 77.000000 Code
plot_numeric(df_eda, 'age' , 'age' ) We can see that the risk of getting heart
disease increases after the age of 54. Code df_eda[ 'age_range' ] =
pd.qcut(df_eda[ 'age' ], 10 ) plot_categorical(df_eda, 'age_range' ,
df_eda.age_range.unique().sort_values()) Resting blood pressure (‘trestbps’)
Code df_eda.trestbps.hist(bins = 20 )  There’s one unrealistic value of 0, I
replace it with na for analysis. Code df_eda.trestbps.replace({ 0 :np.nan},
inplace = True ) People with heart disease on average have a slightly higher
blood pressure than healthy patients. Code plot_numeric(df_eda, 'trestbps' ,
'Resting blood pressure' ) Code describe_numeric(df_eda, 'trestbps' ) trestbps
healthy sick count 860.000000 391.000000 469.000000 mean 132.286047 129.913043
134.264392 std 18.536175 16.869867 19.617889 min 80.000000 80.000000 92.000000
25% 120.000000 120.000000 120.000000 50% 130.000000 130.000000 130.000000 75%
140.000000 140.000000 145.000000 max 200.000000 190.000000 200.000000 Most
patients with normal blood pressure do not have heart disease, while most
patients with elevated blood pressure have heart disease. Code bins = [ 0 ,
120 , 130 , 140 , np.inf] labels = [ 'Normal' , 'Elevated' , 'High' ,
'Critically high' ] df_eda[ 'bp_range' ] = pd.cut(df_eda[ 'trestbps' ], bins)
plot_categorical(df_eda, 'bp_range' , labels) Exercise induced angina
(‘exang’) Most of the patients with heart disease experienced angina during
the exercise, while the majority in the healthy group had no such symptoms.
Code plot_categorical(df_eda, 'exang' , [ 'no' , 'yes' ]) Number of Blocked
Vessels (‘ca’) The chance of having heart disease increases proportionally to
the number of blocked vessels. Patients with 0 blocked vessels have only 27%
chance of having heart disease. The value reaches 85% chance for the group
with 3 blocked vessels. Code labels = [ "0" , "1" , "2" , "3" ]
plot_categorical(df_eda, 'ca' , labels) Gender The majority of men in the
dataset have heart disease, while only 26% of women are unhealthy. Code
plot_categorical(df_eda, 'sex' , [ 'women' , 'men' ]) Chest pain type (‘cp’)
Amongst the patients with no chest pain almost 80% had heart disease. Patients
with the atypical angina had the lowest level of heart disease rate. Overall,
we can not say if a chest pain can be considered as a risk factor for a heart
disease. Code labels = [ 'typical angina' , 'atypical angina' , 'non-anginal
pain' , 'no pain' ] plot_categorical(df_eda, 'cp' , labels) Fasting blood
sugar In the dataset most people don’t have a high sugar level. But among
those who do, almost 70% have heart disease. Code plot_categorical(df_eda,
'fbs' , [ 'Normal' , 'High' ]) Resting electrocardiographic results
(‘restecg’) Patients in the categories with type 1 and type 2 abnormalities
have higher chance of getting a heart disease than the group with normal ECG.
Code labels = [ 'normal' , 'type 1' , 'type 2' ] plot_categorical(df_eda,
'restecg' , labels) Maximum heart rate achieved (‘thalach’) Code
df_eda.thalach.plot.hist(bins = 20 )  Patients without heart disease are able
to reach a higher maximum heart rate than patients with the disease. Code
plot_numeric(df_eda, 'thalach' , 'Maximum heart rate achieved' ) ST depression
(oldpeak) Code df_eda.oldpeak.hist()  There are some negative values.
Replacing with nan. Code df_eda[ 'oldpeak' ] = df_eda[ 'oldpeak' ]. apply (
lambda x: np.nan if x < 0 else x) df_eda.oldpeak.hist()  Oldpeak is another
ECG parameter measuring ST depression during the exercise. It represents a
distance on the ECG plot between specific points. There’s a notable difference
in distributions between the groups. Sick people on average have higher value
of the parameter. Code plot_numeric(df_eda, 'oldpeak' , 'oldpeak' ) Code
describe_numeric(df_eda, 'oldpeak' ) oldpeak healthy sick count 846.000000
387.000000 459.000000 mean 0.906265 0.425840 1.311329 std 1.071192 0.712184
1.153295 min 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 50%
0.500000 0.000000 1.200000 75% 1.500000 0.800000 2.000000 max 6.200000
4.200000 6.200000 Patients with high olpeak values have higher chance of
having herat disease. Code df_eda[ 'oldpeak_range' ] = pd.qcut(df_eda[
'oldpeak' ], 5 , duplicates = 'drop' ) plot_categorical(df_eda,
'oldpeak_range' , df_eda.oldpeak_range.unique().sort_values(), title =
'oldpeak' ) The slope of the peak exercise ST segment (‘slope’) This is
another ECG parameter, measured during the exercise. Almost 80% of the
patients with ‘flat’ or ‘downslopping’ slope parameter had heart disease. Most
of the people with the ‘upslopping’ slope were healthy. Code labels = [
"upslopping" , "flat" , "downslopping" ] plot_categorical(df_eda, 'slope' ,
labels) Thalassemia blood disorder (‘thal’) People with this blood disorder
are at risk of having heart disease with almost 80% for both type 1 and type 2
disorders. Code labels = [ 'normal' , 'type 1' , 'type 2' ]
plot_categorical(df_eda, 'thal' , labels) Numeric attributes relationships
Younger patients are able to achieve higher maximum heart rate. Patients with
heart disease are older and have lower maximum heart rate. Code
sns.pairplot(df_eda, hue = 'disease' , vars = [ 'age' , 'thalach' ]) ; Blood
pressure is higher in older patients. But these factors do not form a clear
separation between healthy and sick patients. Code sns.pairplot(df_eda, hue =
'disease' , vars = [ 'age' , 'trestbps' ]) ; EDA colclusion Exploratory data
analysis identified the following groups with a high risk of heart disease:
Patients with high cholesterol Patients older than 54 years old Male patients
Patients with high blood sugar Patients with abnormalities in their ECG
Patient low maximum heart rate Patients with high oldpeak value Patients with
slope that flat or downsloping Patients with blocked heart vessels Patients
with thalassemia blood disorder Patients with high blood pressure, more than
120 mmHg Data quality Incorrect values There are some incorrect values in the
dataset as it was discovered. Cholesterol (‘chol’) has lots of zeros. ST
depression (‘oldpeak’) has some negative values. In the existing research this
parameter is >= 0. Resting blood pressure (‘trestbps’) has one zero value.
Missing values Count incorrect values as missing values. Code df_missing =
df.copy() df_missing.chol = df.chol.replace({ 0 :np.nan}) df_missing.trestbps
= df.trestbps.replace({ 0 :np.nan}) df_missing.loc[df.oldpeak < 0 , 'oldpeak'
] = np.nan plot_missing(df_missing) Data Preparation Data cleaning Removing
incorrect values. Code def remove_incorrect(data = pd.DataFrame): copy =
df.copy() copy.chol.replace( 0 ,np.nan,inplace = True ) copy.trestbps.replace(
0 ,np.nan,inplace = True ) copy.loc[copy[ 'oldpeak' ] < 0 , "oldpeak" ] =
np.nan return copy df_clean = remove_incorrect(df) #plot the result fig, axs =
plt.subplots( 2 , 2 , figsize = ( 10 , 10 )) #chol sns.histplot(data = df, x =
"chol" , ax = axs[ 0 , 0 ]) axs[ 0 , 0 ].title.set_text( 'Cholesterol raw
values' ) sns.histplot(data = df_clean, x = "chol" , ax = axs[ 0 , 1 ]) axs[ 0
, 1 ].title.set_text( 'Cholesterol removed zeros' ) #oldpeak sns.histplot(data
= df, x = "oldpeak" , ax = axs[ 1 , 0 ]) axs[ 1 , 0 ].title.set_text( 'Oldpeak
raw values' ) sns.histplot(data = df_clean, x = "oldpeak" , ax = axs[ 1 , 1 ])
axs[ 1 , 1 ].title.set_text( 'Oldpeak removed negative values' ) Deleting
columns with more than 30% missing data Code df_reduced = df_clean.drop([ 'ca'
, 'thal' , 'slope' ],axis = 1 ) df_reduced.head( 1 ) age sex cp trestbps chol
fbs restecg thalach exang oldpeak disease 0 32.0 1.0 1.0 95.0 NaN NaN 0.0
127.0 0.0 0.7 1 Removing duplicate rows. Code duplicates =
df_reduced.loc[df_reduced.duplicated(), :] df_no_duplicates =
df_reduced.drop_duplicates(keep = 'first' ) duplicates age sex cp trestbps
chol fbs restecg thalach exang oldpeak disease 613 58.0 1.0 3.0 150.0 219.0
0.0 1.0 118.0 1.0 0.0 1 728 49.0 0.0 2.0 110.0 NaN 0.0 0.0 160.0 0.0 0.0 0
Outliers Code def find_outliers(data = pd.DataFrame): return [col for col in
data.columns if has_outliers(data[col])] def get_box_limits(col = pd.Series):
q1 = col.quantile( .25 ) q3 = col.quantile( .75 ) IQR = q3 \- q1 ll = q1 \- (
1.5 * IQR) ul = q3 \+ ( 1.5 * IQR) return (ll, ul) def has_outliers(col =
pd.Series): ll, ul = get_box_limits(col) upper_outliers = col[col >
ul].count() > 0 lower_outliers = col[col < ll].count() > 0 return
upper_outliers or lower_outliers outliers_columns =
find_outliers(df_no_duplicates[[ 'age' , 'trestbps' , 'chol' , 'thalach' ,
'oldpeak' ]]) fig, axs = plt.subplots( 1 , len (outliers_columns), figsize = (
20 , 7 )) for (i, col) in enumerate (outliers_columns):
df_no_duplicates[col].plot.box(ax = axs[i], fontsize = 18 ) These values are
real, and removing them might affect the modeling, so I keep them. Statistical
significance testing The best resource about univariate and bivariate
statistical analysis basics: univariate bivariate Chi square test for
categorical data Code from pandas.core.strings.accessor import isna from
itertools import combinations from scipy.stats import chi2_contingency def
test(col1 = pd.Series, col2 = pd.Series, alpha = float ): data_cont =
pd.crosstab(col1, col2) p_value = chi2_contingency(data_cont)[ 1 ] return
p_value > alpha def color_df(x): if x == True : return 'color: %s ' % 'red'
elif isna(x): return None else : return 'color: %s ' % 'green' def
show_chi_square_results(data = pd.DataFrame): categorical = [ 'sex' , 'cp' ,
'fbs' , 'restecg' , 'exang' , 'disease' ] cmb = list
(combinations(categorical, 2 )) chi_square_results_df = pd.DataFrame(index =
categorical, columns = categorical) for c in cmb: res = test(data[c[ 0 ]],
data[c[ 1 ]], 0.05 ) chi_square_results_df.loc[c[ 0 ], c[ 1 ]] = res
chi_square_results_df.loc[c[ 1 ], c[ 0 ]] = res return
chi_square_results_df.style.applymap(color_df)
show_chi_square_results(df_no_duplicates) sex cp fbs restecg exang disease sex
nan False False True False False cp False nan True False False False fbs False
True nan False True False restecg True False False nan False False exang False
False True False nan False disease False False False False False nan The table
demonstrates if a variable is statistically independent from another
variable(label ‘True’). There are no independent variables for the target
(disease) column. T-test for numeric features Two-tailed two sample t-testing
is performed. Null Hypothesis: The means of features for patients with heart
disease and healthy patients are the same. Alternative hypothesis: There are
statistically significant differences in the feature means for the healthy and
sick patients. Code from scipy.stats import ttest_ind def test_numeric(data =
pd.DataFrame, col = str ): data = data[ ~ data[col].isna()] _, p =
ttest_ind(data[col], data[ 'disease' ], equal_var = False ) return p def
show_ttest_results(data = pd.DataFrame): results = [(col, test_numeric(data,
col)) for col in [ 'chol' , 'trestbps' , 'age' , 'oldpeak' , 'thalach' ]]
return pd.DataFrame(results, columns = [ 'Column' , 'p-value' ]) Code
show_ttest_results(df_no_duplicates) Column p-value 0 chol 0.000000e+00 1
trestbps 0.000000e+00 2 age 0.000000e+00 3 oldpeak 9.389716e-19 4 thalach
0.000000e+00 I reject null hypothesis and state that there are statistically
significant differences in the feature means for the healthy and sick
patients. Keeping all the features. Split train/test 70% train, 30% test Code
train_data = df_no_duplicates.sample(frac = 0.7 , random_state = 123 )
test_data = df_no_duplicates[ ~ df_no_duplicates.index.isin(train_data.index)]
Data imputation Code from scipy.stats import mode from functools import
partial def impute(data,cols,impute_fn): imputed = data.copy() for col in
cols: imputed[col \+ "_missing" ] = imputed[col].isna().astype( int ) # flag
variable for missing imputed[col] = impute_fn(imputed[col])
#.fillna(imputed[col].mode()[0]) return imputed def impute_categorical(data =
pd.DataFrame) -> pd.DataFrame: cols = [ 'restecg' , 'exang' , 'fbs' ]
impute_fn = lambda x: x.fillna(x.mode()[ 0 ]) return
impute(data,cols,impute_fn) def impute_numeric(data = pd.DataFrame, cols =
list ) -> pd.DataFrame: cols = [ 'oldpeak' , 'trestbps' , 'thalach' , 'chol' ]
impute_fn = lambda x: x.fillna(x.median()) return impute(data,cols,impute_fn)
def plot_numeric_imputation(data_before = pd.DataFrame, data_after =
pd.DataFrame): cols = [ 'oldpeak' , 'trestbps' , 'thalach' , 'chol' ] fig, axs
= plt.subplots( 1 , len (cols), figsize = ( 25 , 5 )) for i, col in enumerate
(cols): sns.kdeplot(data_before[col], color = 'b' , fill = True , ax = axs[i],
alpha = 0.07 ) sns.kdeplot(data_after[col], color = 'r' , fill = True , ax =
axs[i], alpha = 0.07 ) axs[i].legend([ 'Before imputation' , 'After
imputation' ]) def plot_categorical_imputation(data_before = pd.DataFrame,
data_after = pd.DataFrame): cols = [ 'restecg' , 'exang' , 'fbs' ] fig, axs =
plt.subplots( 1 , len (cols), figsize = ( 25 , 5 )) for i, col in enumerate
(cols): sns.histplot(data = data_before, x = col, ax = axs[i], color = 'b' )
sns.histplot(data = data_after, x = col, ax = axs[i], color = 'r' , alpha =
0.2 ) plot_numeric_imputation(train_data, impute_numeric(train_data))
plot_categorical_imputation(train_data, impute_categorical(train_data)) One
hot encoding Code def encode_dummy(data = pd.DataFrame, drop_first = True ) ->
pd.DataFrame: df = data.copy() for col in [ 'cp' , 'restecg' ]: dummy =
pd.get_dummies(df[col], prefix = col,drop_first = drop_first) df =
pd.concat([df, dummy], axis = 1 ) df.drop(col, axis = 1 , inplace = True )
return df encode_dummy(train_data).head( 5 ) age sex trestbps chol fbs thalach
exang oldpeak disease cp_2.0 cp_3.0 cp_4.0 restecg_1.0 restecg_2.0 349 47.0
1.0 112.0 204.0 0.0 143.0 0.0 0.1 0 0 0 1 0 0 654 38.0 1.0 140.0 297.0 0.0
150.0 0.0 0.0 0 1 0 0 0 0 7 38.0 1.0 115.0 NaN 0.0 128.0 1.0 0.0 1 0 1 0 0 0
571 54.0 1.0 NaN 182.0 0.0 NaN NaN NaN 0 1 0 0 1 0 171 65.0 0.0 140.0 417.0
1.0 157.0 0.0 0.8 0 0 1 0 0 1 Feature Scaling Normalization/Standardization
Machine learning algorithms like linear regression, logistic regression,
neural network, etc. that use gradient descent as an optimization technique
require data to be scaled. Distance algorithms like KNN, K-means, and SVM are
most affected by the range of features
Normalization/Standardization.Therefore, I scale the data before employing a
distance based algorithm so that all the features contribute equally to the
result. Code from sklearn.preprocessing import MinMaxScaler def scale(data =
pd.DataFrame): return pd.DataFrame(MinMaxScaler().fit_transform(data), columns
= data.columns) def plot_normalization(data_before = pd.DataFrame, data_after
= pd.DataFrame, cols = str ): fig, axs = plt.subplots( len (cols), 2 , figsize
= ( 25 , 15 )) for i, col in enumerate (cols): sns.histplot(data_before[col],
color = 'b' , ax = axs[i, 0 ]) axs[i, 0 ].title.set_text(col \+ ' before
scaling' ) sns.histplot(data_after[col], color = 'r' , ax = axs[i, 1 ]) axs[i,
1 ].title.set_text(col \+ ' after scaling' ) plot_normalization(train_data,
scale(train_data), [ 'chol' , 'age' , 'sex' ]) Modeling Helper functions from
sklearn.ensemble import RandomForestClassifier, RandomForestRegressor from
sklearn.model_selection import train_test_split from sklearn.metrics import
accuracy_score from sklearn.linear_model import LogisticRegression def
run_modeling(model,train_data,test_data): x_train = train_data.drop( 'disease'
, axis = 1 ) y_train = train_data[ 'disease' ] model.fit(x_train, y_train)
x_test = test_data.drop( 'disease' , axis = 1 ) y_test = test_data[ 'disease'
] y_model = model.predict(x_test) return accuracy_score(y_test, y_model),
model def impute_all(data): return impute_categorical(impute_numeric(data))
def full_pipeline(data): data = impute_all(data) data = encode_dummy(data)
return scale(data) Random forest rf_train_data = impute_all(train_data)
rf_test_data = impute_all(test_data) acc, rf_model =
run_modeling(RandomForestClassifier(random_state = 0 ),
rf_train_data,rf_test_data) print ( f'accuracy: { acc } ' ) accuracy:
0.7527272727272727 Logistic regression lr_train_data =
full_pipeline(train_data) lr_test_data = full_pipeline(test_data) acc, model =
run_modeling(LogisticRegression(), lr_train_data, lr_test_data) print (
f'accuracy: { acc } ' ) accuracy: 0.7781818181818182 SVM There are many model
parameters and they are not easy to choose, so I use the GridSearchCV tool in
sklearn to help to complete the parameter selection. The main parameters
include kernel, C and gamma from sklearn import svm from
sklearn.model_selection import GridSearchCV parameters = { 'kernel' :(
'linear' , 'rbf' ), 'C' :[ 1 ], 'gamma' :[ 0.05 , 0.07 , 0.125 , 0.25 , 0.5 ]}
model = GridSearchCV(svm.SVC(), parameters, scoring = 'accuracy' )
svm_train_data = full_pipeline(train_data) svm_test_data =
full_pipeline(test_data) acc, model = run_modeling(model, svm_train_data,
svm_test_data) print ( f'accuracy: { acc } ' ) accuracy: 0.7781818181818182
Interpretation I use information from the random forest feature importance to
do interpretation. From the random forest feature importance information I
conclude that the most important variables for predicting heart disease are:
Chest pain type (cp) Maximum heart rate achieved (thalach) Oldpeak Age
Cholesterol Meanwhile such parameters as blood sugar and the results of the
electrocardiogram contributed the least to the heart disease prediction in the
random forest model. The most of the missing values flags were not important
for the model. Code (pd.Series(rf_model.feature_importances_, index =
rf_train_data.drop([ 'disease' ], axis = 1 ).columns).sort_values().plot(kind
= 'barh' , figsize = ( 10 , 10 )))

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://galax.dev/apps.html***

galax.dev - apps playground Logo Header Categories All (0) afterhoursbilly
GitHub Twitter [email protected] No matching items Back to top

***URL: https://galax.dev/index.html***

galax.dev My name is Szymon and, I am an aspiring Machine/Deep Learning
Engineer, currently in my third year of Computer Science. If you are already
here, you can check out my blog posts and demo apps in the playground below.
You can find most of the code for my projects on my GitHub. blog Click here to
check out the all blog posts. LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Nov 13, 2023 An Image-to-Image
Implementation Demonstation of an image-to-image implementation of the Stable
Diffusion model. Oct 3, 2023 No matching items playground Click here to play
more in the playground. No matching items Contact Information afterhoursbilly
GitHub Twitter [email protected]

***URL: https://galax.dev/posts/00_Image-to-Image.html***

galax.dev - An Image-to-Image Implementation Text-Guided: Image-to-Image
Implementation This Python code demonstrates the implementation of the Image-
to-Image technique, allowing you to generate new images from existing ones
with the help of textual prompts. Explore how this innovative approach
combines images and text to create visually compelling artworks. Dive into the
code to understand the mechanics behind this cutting-edge image generation
technique. Pip install necessary libraries (click to show/hide) ! pip install
\- Uq diffusers transformers fastcore fastdownload Importing utilities (click
to show/hide) from transformers import CLIPTextModel, CLIPTokenizer import
torch from diffusers import LMSDiscreteScheduler from PIL import Image from
tqdm.auto import tqdm from diffusers import AutoencoderKL,
UNet2DConditionModel import logging from fastdownload import FastDownload from
pathlib import Path from huggingface_hub import notebook_login import
matplotlib.pyplot as plt from torchvision import transforms if not
(Path.home() / '.cache/huggingface' / 'token' ).exists(): notebook_login()
logging.disable(logging.WARNING) We need to load in the required libraries and
set up the models. tokenizer = CLIPTokenizer.from_pretrained( "openai/clip-
vit-large-patch14" , torch_dtype = torch.float16) text_encoder =
CLIPTextModel.from_pretrained( "openai/clip-vit-large-patch14" , torch_dtype =
torch.float16).to( "cuda" ) # Here we use a different VAE to the original
release, which has been fine-tuned for more steps vae =
AutoencoderKL.from_pretrained( "stabilityai/sd-vae-ft-ema" , torch_dtype =
torch.float16).to( "cuda" ) unet = UNet2DConditionModel.from_pretrained(
"CompVis/stable-diffusion-v1-4" , subfolder = "unet" , torch_dtype =
torch.float16).to( "cuda" ) Define the parameters. height = 512 width = 512
num_inference_steps = 70 guidance_scale = 7.5 batch_size = 1
beta_start,beta_end = 0.00085 , 0.012 scheduler =
LMSDiscreteScheduler(beta_start = beta_start, beta_end = beta_end,
beta_schedule = "scaled_linear" , num_train_timesteps = 1000 )
plt.plot(scheduler.sigmas) plt.title( 'Noise Schedule' ) plt.xlabel( 'Sampling
step' ) plt.ylabel( 'sigma' ) plt.show() def prep_img(img_link : str ) ->
torch.Tensor: """ Preprocesses an image from a given link. Args: img_link
(str): The URL or path to the image file. Returns: torch.Tensor: A tensor
representing the preprocessed image. """ p = FastDownload().download(img_link)
init_image = Image. open (p).convert( "RGB" ).resize(( 512 , 512 )) return
transforms.ToTensor()(init_image) The image we will use as a starting point.
Downloading the image (click to show/hide) link = "https://cdn-
uploads.huggingface.co/production/uploads/1664665907257-noauth.png"
transformed_image = prep_img(link) # show image in notebook. p =
FastDownload().download(link) display(Image. open (p).convert( "RGB"
).resize(( 512 , 512 ))) def tokenization(prompt: list , max_len : int = None
) \- > torch.Tensor: """ Tokenizes a text prompt and returns the corresponding
encoded tensor. Args: prompt (list): The input text prompt to be tokenized.
max_len (int, optional): The maximum length of the tokenized sequence. If not
specified, it defaults to the maximum length allowed by the tokenizer.
Returns: torch.Tensor: A tensor containing the encoded representation of the
tokenized prompt. """ if max_len is None : max_len =
tokenizer.model_max_length tokenized_prompt = tokenizer(prompt, padding =
"max_length" , max_length = max_len, truncation = True , return_tensors = 'pt'
) return text_encoder(tokenized_prompt.input_ids.to( 'cuda' ))[ 0 ].half() def
make_image(latent: torch.Tensor): """ Converts a tensor representation of an
image into a PIL Image. Args: latent (torch.Tensor): A tensor representing an
image. Returns: PIL.Image.Image: A PIL Image representing the image. """ image
= (latent / 2 \+ 0.5 ).clamp( 0 , 1 ).detach().cpu().permute( 1 , 2 , 0
).numpy() return Image.fromarray((image * 255 ). round ().astype( "uint8" ))
Denoising loop To ensure the effectiveness of this solution, it is essential
to incorporate the “start_step” parameter. Essentially, we aim to prevent
excessive noise from being added to the input image, particularly avoiding the
most intense noise additions. After this initial step, we can proceed with the
looping process. In summary, the key to success here is to introduce the
“start_step” parameter, which helps us avoid excessive noise in the early
stages and then continue with the loop as intended. def create_sample(prompt:
list ,transformed_image: torch.Tensor ,guidance_scale: float = 7.5 , seed: int
= 5 , steps: int = 70 ,start_step: int = 10 ): ''' Generate a sample image
based on a text prompt, provided image and guidance parameters. Args: prompt
(list): A list of text prompts. transformed_image (torch.Tensor): A tensor
representing the transformed image. guidance_scale (float, optional): The
scale factor for guiding the generation process. seed (int, optional): Seed
for random number generation. Default is 5. steps (int, optional): The total
number of steps for the generation process. Default is 70. start_step (int,
optional): The step at which the generation process starts. Default is 10.
Returns: torch.Tensor: A tensor representing the generated sample. This
function generates an image based on the provided text prompts , transformed
image and parametrs.It uses a predefined VAE model to encode the image and
then applies noise and guidance to generate the sample.It iteratively refines
the image by adding noise and updating the latent representation. The guidance
scale controls the influence of the text prompts on the image. The generated
image is returned as a PyTorch tensor. Example: >>> prompt = ["Translate the
following English sentence to French: 'Hello, how are you?'"] >>>
transformed_image = prep_img(image_link) >>> generated_sample =
create_sample(prompt, transformed_image) ''' bs = 1 # Implementation for only
a single prompt. text = tokenization(prompt) uncond = tokenization([ "" ] *
bs, text.shape[ 1 ]) emb = torch.cat([uncond, text]) if seed:
torch.manual_seed(seed) # Encode image image_latent =
vae.encode((transformed_image.unsqueeze( 0 ).half().to( 'cuda'
))).latent_dist.sample() image_latent = vae.config.scaling_factor *
image_latent # Create noise scheduler.set_timesteps(steps) noise_latents =
torch.randn_like(image_latent) latents = scheduler.add_noise(image_latent,
noise_latents, timesteps = torch.tensor([scheduler.timesteps[start_step]]))
for i, ts in enumerate (tqdm(scheduler.timesteps)): if i >= start_step: # Skip
the batches of noise that don't affect the input image. inp =
scheduler.scale_model_input(torch.cat([latents] * 2 ), ts) with
torch.no_grad(): noise_pred_uncond, noise_pred_text = unet(inp, ts,
encoder_hidden_states = emb).sample.chunk( 2 ) pred = noise_pred_uncond \+
guidance_scale * (noise_pred_text \- noise_pred_uncond) latents =
scheduler.step(pred, ts, latents).prev_sample with torch.no_grad(): return
vae.decode( 1 / 0.18215 * latents).sample prompt = [ 'Wolf howling at the
moon, photorealistic 4K' ] #prompt = ['unicorn'] #prompt = ['a kids drawing of
bacteria, cartoon style'] #prompt = [' Horse looking at the morning sun,
photorealistic 4K'] image = create_sample(prompt,transformed_image,steps = 50
,seed = 1000 ) display(make_image(image[ 0 ])) Back to top

***URL: https://galax.dev/blog.html***

galax.dev - blog Categories All (2) Deep Learning (2) Implementation (2)
Stable Diffusion (1) afterhoursbilly GitHub Twitter [email protected] Order By
Default Date - Oldest Date - Newest LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Monday, 13 November 2023 2 min An Image-
to-Image Implementation Demonstation of an image-to-image implementation of
the Stable Diffusion model. Tuesday, 03 October 2023 1 min No matching items
Back to top

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch.html***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://jellis18.github.io/***

Justin A. Ellis Justin Ellis Principal Engineer, Infinia ML. Former
Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML Recent Posts A fun multi-lingual concurrency
experiment 6 minute read The other day I was writing some code for a CI
pipeline that needed to fetch release notes from several different git repos
and collate them into one large s... Production Dockerfile Tips and Tricks 6
minute read Container images are the backbone of a lot of modern workflows,
especially in the age of Kubernetes and friends. Building images that are
small, secure, and ... Some Advanced Typing Concepts in Python 14 minute read
To start off, let me admit that yes, Python is a dynamically (gradually?)
typed language. However, with modern Python (3.6+, and really with 3.10+) and
stati... Early and late binding closures in Python 3 minute read So it
happened again, late-binding closures bit me. I once again had to discover
that this was a thing so I decided to make a little post about it just in
ca... Access Modifiers in Python 9 minute read If you have stumbled across
this post then you are probably coming to Python from another object oriented
language that has real support for access modifiers...

***URL: https://jellis18.github.io/post/page3/***

Justin A. Ellis - Page 3 Justin Ellis Principal Engineer, Infinia ML. Former
Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML Recent Posts Personal Analytics Part 1: Gmail 18
minute read Most of us are sending or receiving tens to hundreds of emails
daily. I have been using email regularly since my undergrad days in 2008 so I
thought it would... A Practical Guide to MCMC Part 1: MCMC Basics 15 minute
read Markov Chain Monte-Carlo (MCMC) is an art, pure and simple. Throughout my
career I have learned several tricks and techniques from various
"artists&quot...; Continuous Integration of Hugo Website using Travis CI and
Github. 3 minute read In this post I will go over how to set up a clean
continuous integration system for your personal Github site using Hugo and
Travis CI. Automating Jupyter Slides with Travis CI and gh-pages. 3 minute
read From a Jupyter notebook to a beautiful web presentation in no time! I'm
Quite Ready for Another Adventure 7 minute read As part of my transition to
Data Science I figured I should get some first hand experience into the field
in which I'm transitioning. A few months ago I got ...

***URL: https://jellis18.github.io/post/page2/***

Justin A. Ellis - Page 2 Justin Ellis Principal Engineer, Infinia ML. Former
Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML Recent Posts Abstract Base Classes and Protocols:
What Are They? When To Use Them?? Lets Find Out! 13 minute read In Python
there are two similar, yet different, concepts for defining something akin to
an interface, or a contract describing what methods and attributes a ... A
Modern and Explicit approach to Python Exceptions 13 minute read Two of the
most popular "modern" programming languages are Rust and Go. While these
languages are quite different, both have a few things in common... Building a
fully typed LRU Cache in Python 10 minute read In this post we are going to
build a fully typed LRU (least recently used) cache (almost) from scratch
using Python. We will then create a function decorator... A Practical Guide to
MCMC Part 2: Further Resources 1 minute read If you are coming here Part 1
then I'm sorry to tell you that this is not the fully fleshed out post you
were hoping for. If you are coming here directly, go... How to transition from
Academia to Data Science 21 minute read Oh boy, another one of these blog
posts about transitioning to Data Science from Academia. Well, in this post
I'll try to add a slightly different take on th...

***URL: https://jellis18.github.io/post/2022-01-11-abc-vs-protocol/***

Abstract Base Classes and Protocols: What Are They? When To Use Them?? Lets
Find Out! - Justin A. Ellis Justin Ellis Principal Engineer, Infinia ML.
Former Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML In Python there are two similar, yet different,
concepts for defining something akin to an interface, or a contract describing
what methods and attributes a class will contain. These are Abstract Base
Classes (ABCs) and Protocols . Until the advent of type annotations , ABCs
were the way to go if you wanted to have any kind of validation on
class/instance methods or properties and isinstance checks. With type
annotations, ABCs became more relevant as a way to define an "interface" for a
given class and then use that as a type annotation. However, to use ABCs as an
interface we must rely on nominal subtyping and a strict class hierarchy (we
will explain this later but, in short, we will have to subclass the ABCs in
order to use it as an interface). With Protocols we can use structural
subtyping or "Duck typing" (i.e. the class only has to have the same methods
and attributes, no subclassing necessary). So when do we use ABCs and when do
we use Protocols? Before we dig into this, lets first get a basic
understanding of how each works. What are Abstract Base Classes Here I will
give a brief overview of ABCs, if you want a much more detailed explanation
see this great video by one of the creators of ABCs . In general there are two
use cases for ABCs, as a pure ABC that defines an "interface" and as a tool
for code re-use via the Framework Design Pattern or through Mixins. Pure ABCs
(ABC as Interface) The simplest way to use an ABC is as a pure ABC, for
example: from abc import ABC , abstractmethod class Animal ( ABC ): @
abstractmethod def walk ( self ) -> None : pass @ abstractmethod def speak (
self ) -> None : pass Here we have defined an ABC Animal with two methods:
walk and speak . Note that the way to do this is to subclass ABC and to
decorate the methods that must be implemented (i.e. part of the "interface")
with the @abstractmethod decorator. Now we can implement this "interface" to
create a Dog class Dog ( Animal ): def walk ( self ) -> None : print ( "This
is a dog walking" ) def speak ( self ) -> None : print ( "Woof!" ) This will
work fine but if we happened to forget to implement the speak method then we
would get this error on creation >>> dog = Dog () TypeError : Can 't
instantiate abstract class Dog with abstract method speak We can see that we
get an error because we haven't implemented the abstract method speak . This
ensures that all subclasses implement the correct "interface". ABCs as a tool
for code reuse Another, and probably more common, use case for ABCs is for
code reuse. Below is a slightly more realistic example of a base class for a
statistical or Machine Learning regression model from abc import ABC ,
abstractmethod from typing import List , TypeVar import numpy as np T =
TypeVar ( "T" , bound = "Model" ) class Model ( ABC ): def __init__ ( self ):
self . _is_fitted = False def fit ( self : T , data : np . ndarray , target :
np . ndarray ) -> T : fitted_model = self . _fit ( data , target ) self .
_is_fitted = True return fitted_model def predict ( self , data : np . ndarray
) -> List [ float ]: if not self . _is_fitted : raise ValueError ( f " { self
. __class__ . __name__ } must be fit before calling predict" ) return self .
_predict ( data ) @ property def is_fitted ( self ) -> bool : return self .
_is_fitted @ abstractmethod def _fit ( self : T , data : np . ndarray , target
: np . ndarray ) -> T : pass @ abstractmethod def _predict ( self , data : np
. ndarray ) -> List [ float ]: pass Ok, lets unpack this first. There are two
public methods, fit and predict . The fit method calls the private abstract
method _fit and then sets the private attribute _is_fitted . The predict
method checks if we have fit the model before trying to make predictions and
then calls the private abstract method _predict . Lastly the base class
defines a property is_fitted . We do this so that a user cannot set this value
explicitly and it will only get set when calling fit . A quick aside. Other
than the ABC there are a few other important things going on. The first is the
use of generic self in the definition of the fit method. Here we type self
with a generic variable T that is bound to the model class. We do this to
ensure that subclasses of Model will return the correct type when calling fit
. Secondly we use a private attributed _is_fitted internally and expose this
with the is_fitted property. This is not strictly necessary but it is good
practice to keep this variable readonly since we really only want this to be
True if we have actually successfully run fit . Ok, back to ABCs. In the
example above we have used this base class to implement some logic that will
be inherited by all of its children, namely the fit and the predict methods.
These methods delegate the actual work to the private abstract methods, _fit
and _predict , that the children must implement. Of course we could make this
a pure ABC and have all children implement fit and predict but it would be
quite tedious for all children to have to re-implement the is_fitted
validation in both of these methods. Furthermore, this is a simple example and
in real situation there could be much more complicated shared code in the base
class. Finally, we could have made the abstract methods public but in this
case and probably most cases like the above example we should keep them
private instead of polluting the class. A end consumer only needs to know that
the class has a fit and a predict method and a is_fitted (i.e. readonly)
property. For a good, real life example of this kind of pattern take a look at
Pytorch's Module . While this class does not actually use ABCs it uses the
same pattern where there is a lot of reusable code in the base Module class
and users only have to implement the forward method. Ok, now that we have some
understanding of how to use ABCs for code reuse. Lets implement a super simple
model class MeanRegressor ( Model ): def __init__ ( self ): super (). __init__
() self . _mean = None def _fit ( self , data : np . ndarray , target : np .
ndarray ) -> "MeanRegressor" : self . _mean = target . mean () return self def
_predict ( self , data : np . ndarray ) -> List [ float ]: return list ( np .
ones ( data . shape [ 0 ]) * self . _mean ) In this example we have
implemented a very simple regression model to return the mean of the target
for every sample. In this case the _fit method only sets a private state
variable which is the mean of the target. The _predict method returns a list
that is the length of the number of samples with the computed mean as the
value. Lets see how this works. >>> data = np . array ([ 1.0 , 2.0 , 3.0 , 4.0
]) >>> target = np . array ([ 2.0 , 3.0 , 5.0 , 10.0 ]) >>> mean_regressor =
MeanRegressor () # try to predict without fitting first >>> mean_regressor .
predict ( data ) ValueError : MeanRegressor must be fit before calling predict
# Check if fitted >>> mean_regressor . is_fitted False # Fit and predict >>>
preds = mean_regressor . fit ( data , target ). predict ( data ) [ 5.0 , 5.0 ,
5.0 , 5.0 ] # Check if fitted now >>> mean_regressor . is_fitted True As we
can see in the example above we are actually calling the methods and property
from the ABC which under the hood is calling our concrete implementations of
the private _fit and _predict methods. So we get the error checking and
automatic setting of the _is_fitted attributes for free. This way, users do
not need to worry about that when creating a new type of Model . Isn't that
fun? What are Protocols Protocols were introduced in PEP-544 as a way to
formally incorporate structural subtyping (or "duck" typing) into the python
type annotation system. There are two main, but related, use cases for
Protocols. First, they can be used as an interface for classes and functions
which can be used downstream in other classes or functions. Secondly, they can
be used to set bounds on generic types. Protocols as Interfaces Protocols
allow you to define an interface for a class or function that will be type
checked on usage rather than on creation. For example, we can make our Animal
ABC above a Protocol from typing import Protocol class Animal ( Protocol ):
def walk ( self ) -> None : ... def speak ( self ) -> None : ... Note that
this looks pretty similar to our ABC based Animal class above. We inherit from
typing.Protocol instead of abc.ABC and we don't need to add the
@abstractmethod decorators since Protocols are not meant to be "implemented"
but simply act as an interface in downstream tasks. Lastly, it is common
practice to use ... in the body of methods in a Protocol instead of pass as we
did in the ABC, although either will work in both places. Ok, lets now
implement a Dog class Dog : def walk ( self ) -> None : print ( "This is a dog
walking" ) def speak ( self ) -> None : print ( "Woof!" ) Note here that we
don't subclass animal, we simply have to implement the methods specified in
the Animal Protocol. We can then use this in a downstream task like: def
make_animal_speak ( animal : Animal ) -> None : animal . speak () >>> dog =
Dog () >>> make_animal_speak ( dog ) 'Woof!' Here static type checkers would
be happy because the dog instance does indeed implement the Animal Protocol
because it has the same structure but is not itself a child class of Animal .
Lets see how Protocols enforce the interface. Lets say we forget to implement
the speak method on our Dog class >>> dog = Dog () >>> make_animal_speak ( dog
) Argument of type "Dog" cannot be assigned to parameter "animal" of type
"Animal" in function "make_animal_speak" "Dog" is incompatible with protocol
"Animal" "speak" is not present In this case a static type checker (Pylance in
the example above) would raise an error and tell the user that Dog does obey
the Animal Protocol since it doesn't implement the speak method. Notice that
this is different than an ABC that will raise an error when creating the Dog
class, whereas Protocols will raise an error where they are used. Ok, lets get
a bit more tricky. What if we define the Dog class but change the signature of
speak a bit class Dog : def walk ( self ) -> None : print ( "This is a dog
walking" ) def speak ( self , name : str ) -> None : print ( f "Woof! My name
is { name } " ) When we try to use this in the function >>> dog = Dog () >>>
make_animal_speak ( dog ) Argument of type "Dog" cannot be assigned to
parameter "animal" of type "Animal" in function "make_animal_speak" "Dog" is
incompatible with protocol "Animal" "speak" is an incompatible type Type
"(name: str) -> None" cannot be assigned to type "() -> None" Keyword
parameter "name" is missing in destination So, once again we get an error but
this time it is because the speak method on Dog does not have the same
signature. Pretty cool huh? In the example function above, we don't actually
even need the walk method since it won't be used in the function. We can
narrow down the input type by defining a new Protocol class SupportsSpeak (
Protocol ): def speak ( self ) -> None : ... def make_animal_speak ( animal :
SupportsSpeak ) -> None : animal . speak () >>> dog = Dog () >>>
make_animal_speak ( dog ) 'Woof!' So we still have the same Dog class with a
walk and speak method but we define the new Protocol SupportsSpeak (the naming
is somewhat standard if you are defining an interface with one method) that
just defines the speak method. And everything still works and would indeed
work with any class that has the same speak method signature. This is a simple
example but this can be quite powerful in more complicated code. One last
thing to mention about using Protocols as interfaces is that it is possible to
make them useable at runtime via an isinstance check with the
runtime_checkable decorator. from typing import Protocol , runtime_checkable @
runtime_checkable class Animal ( Protocol ): def walk ( self ) -> None : ...
def speak ( self ) -> None : ... >>> dog = Dog () >>> isinstance ( dog ,
Animal ) True Protocols as Generic Type Bounds When defining a generic type
variable in python we can give it a bound which means that the generic type
must either be a child class of the bound if given a class bound or it must
implement the protocol if given a Protocol. If you were paying attention we
actually used a type bound in the Model ABC example above. In this case we
typed the self variable with the generic type T that was bound by Model
itself. This was done so that child classes of Model would return the correct
type for fit which indeed returns self . If that is not clear take a look at
the documentation . So that was an example of using a class bound. However, in
generic programming we usually want to make things as specific as possible.
The max function is a great example of this. The builtin max function can take
in several different types of input. We could use overload for every different
kind of input but that can be tedious. Instead we notice that a specific
implementation of max only requires that the input types define the __lt__
magic method (meaning we can do x < y ). We can type (a somewhat simplified)
max method as follows (for a much more detailed video, check out this ) from
typing import TypeVar , Protocol class SupportsLessThan ( Protocol ): def
__lt__ ( self , __other : Any ) -> bool : ... S = TypeVar ( "S" , bound =
SupportsLessThan ) def my_max ( x : S , y : S ) -> S : if x < y : return y
return x Lets break this down. First, we implement a Protocol that defines the
__lt__ method. We then create a generic type variable S that is bound by our
SupportsLessThan Protocol. This means that S can be any type as long as it
implements __lt__ . We then define the max function which takes in two
arguments x and y , which are both generic type S and returns the same type. A
lot of builtin types have a __lt__ method implemented so we can use this
function with integers or strings, for example >>> max_int = my_max ( 4 , 5 )
>>> max_str = my_max ( "hello" , "world!" ) In the example above the max_int
will return an int and max_str will return a string. If we pass in an object
that does not have __lt__ implemented then we will get an error. So ABC or
Protocol? Yes. You should use both as they are good at different things and
both have should their place in your toolbox. We have already seen above the
main use cases for ABCs and Protocols and how they work. Given those examples
here are some good overall suggestions and observations. Abstract Base Classes
Belong to their subclasses. An ABC is not usable by itself, it can only be
used by implementing a child class. So because of this, ABCs inherently belong
to their subclasses as part of a strict class hierarchy. ABCs are a good
mechanism for code reuse, especially for boilerplate code or logic that will
not change for any (or most) subclasses. The best strategy here is to have the
ABC (i.e. parent class) do most of the work and have the children implement
the specifics. Good for real time validation when creating an instance of a
child class. As we saw above, ABCs will raise an error on creation if the
child does not implement all abstract methods. Protocols Belong where they are
used. As we saw above, Protocols are not "implemented" but tell downstream
code (i.e. other functions or classes) what the structure of the input object
is expected to be. Also, we saw that we can define multiple protocols for the
same kind of object depending on what is needed. This means that Protocols
belong where they are used. Good for defining interfaces, especially for 3rd-
party libraries when we don't want to tightly couple our code to a specific
3rd party library. Good (really the only way) for specifying flexible generic
type bounds. This somewhat goes without saying but Protocols only are useful
if using type annotations and cannot be used in any other way (except for
runtime_checkable ). Ok, so we know that ABCs and Protocols are, we know what
they are good at. So when should we use them? The answer to that is somewhat
subjective and depends on your environment but here are some rules of thumb
Use ABCs if you want to reuse code. Inheritance is not always the best method
of code reuse but it can be quite useful. Use ABCs if you require strict class
hierarchy (as in you need to use method resolution order or you need to check
__subclasses__ ) in your application. Use ABCs if you will need several
implementations of a class with several methods. Use Protocols for strict type
annotations (i.e.only annotate the methods/attributes you need) Use Protocols
for generic bounds Use Protocols for abstract interfaces for 3rd party
libraries Well, thats it for this time. Now go forth into our bold almost
statically typed python future with confidence! Share on Twitter Facebook
Google+ LinkedIn Leave a Comment You May Also Enjoy A fun multi-lingual
concurrency experiment 4 minute read The other day I was writing some code for
a CI pipeline that needed to fetch release notes from several different git
repos and collate them into one large s... Production Dockerfile Tips and
Tricks 6 minute read Container images are the backbone of a lot of modern
workflows, especially in the age of Kubernetes and friends. Building images
that are small, secure, and ... Some Advanced Typing Concepts in Python 13
minute read To start off, let me admit that yes, Python is a dynamically
(gradually?) typed language. However, with modern Python (3.6+, and really
with 3.10+) and stati... Early and late binding closures in Python 3 minute
read So it happened again, late-binding closures bit me. I once again had to
discover that this was a thing so I decided to make a little post about it
just in ca... Please enable JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/2022-11-23-late-binding-python/***

Early and late binding closures in Python - Justin A. Ellis Justin Ellis
Principal Engineer, Infinia ML. Former Astrophysicist, former Data Scientist.
Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML So it happened
again, late-binding closures bit me. I once again had to discover that this
was a thing so I decided to make a little post about it just in case someone
happens to run into this and by some miracle lands here. The goal is to keep
this short but I want to cover 3 main things What are closures? How can you
shoot yourself in the foot with them? How not so shoot aforementioned foot.
What are closures (the short version) In many cases closures are taken to be
synonymous with anonymous (or lambda, no not the AWS kind) functions. However,
one can have an anonymous function that is not a closure and a closure that is
not an anonymous function. In simple terms a closure is a function that
captures at least one variable from the environment (i.e. outside of its
lexical scope). Make sense? Lets look at some examples. from typing import
Callable # closure with a nested function def f ( x : int ) -> Callable [[ int
], int ]: def g ( y : int ) -> int : return x \+ y return g # closure with a
lambda function def h ( x : int ) -> Callable [[ int ], int ]: return lambda y
: x \+ y CONSTANT : Final [ int ] = 4 # also a closure l = lambda x : x \+
CONSTANT # same... def k ( x : int ) -> int : return x \+ CONSTANT Both
functions g and h return a closure and l and k are closures themselves. They
are closures because they use variables outside of their lexical scope, g and
h from the outer functions f and h scope, respectively, and l and k use the
CONSTANT from the module scope. So if we call some of these functions it would
look like # these return functions that take in an integer and return an
integer a = f ( 2 ) b = h ( 2 ) print ( a ( 4 )) # returns 6 (2+4) print ( b (
2 )) # returns 4 (2+2) So we can first generate our closure by calling g and h
, respectively and then pass our second variable when calling that returned
function. This may seem strange to newer programmers or maybe those from pure
object oriented languages but using functions as first-class citizens in your
code can be quite powerful. The late-binding vs. early-binding problem Ok, now
that we have a decent understanding of what closures are, lets look at how we
can shoot ourselves in the foot. Say we are all excited, because closures are
awesome, and we want to make a list of functions to perform some action, say
add one to a given number. And since we love being "pythonic" we decide to be
clever: new_funcs = [ lambda : ii \+ 1 for ii in range ( 5 )] output = [ nf ()
for nf in new_funcs ] We have created our closure and then created 5
functions, the first will do 0+1 , the second 1+1 and if we evaluate these
functions in order we will get [5,5,5,5,5] . Wait that doesn't seem right,
what happened? Late-binding closures is what happened. This means that the
value used in the closure, ii in this case, is looked up when the function is
called and now when it was defined. Since the function is called after the
loop if finished we will end up always using the last value in the loop. How
to avoid foot shooting We can avoid the problem above in a few ways, one is to
use something like our functions f or h above to bind the argument to the
closure early by passing it as the argument of the outer function. from typing
import Callable def add_one ( x : int ) -> Callable [[], int ]: def g () ->
int : return x \+ 1 return g # could use lambda here as well def add_one ( x :
int ) -> Callable [[], int ]: return lambda : x \+ 1 new_funcs = [ add_one (
ii ) for ii in range ( 5 )] outputs = [ nf () for nf in new_funcs ] The output
will now be my luggage combination [1,2,3,4,5] . Another option, which is much
less verbose (although almost identical under the hood) is to use
functools.partial from functools import partial def add_one ( x : int ) -> int
: return x \+ 1 new_funcs = [ partial ( add_one , ii ) for ii in range ( 5 )]
outputs = [ nf () for nf in new_funcs ] This will return the same output. Both
of these approaches lead to early binding of the arguments and can be quite
useful for making generic callback functions, among other things. Well, thats
it for this time and watch out for those early-binding closures. Share on
Twitter Facebook Google+ LinkedIn Leave a Comment You May Also Enjoy A fun
multi-lingual concurrency experiment 4 minute read The other day I was writing
some code for a CI pipeline that needed to fetch release notes from several
different git repos and collate them into one large s... Production Dockerfile
Tips and Tricks 6 minute read Container images are the backbone of a lot of
modern workflows, especially in the age of Kubernetes and friends. Building
images that are small, secure, and ... Some Advanced Typing Concepts in Python
13 minute read To start off, let me admit that yes, Python is a dynamically
(gradually?) typed language. However, with modern Python (3.6+, and really
with 3.10+) and stati... Access Modifiers in Python 9 minute read If you have
stumbled across this post then you are probably coming to Python from another
object oriented language that has real support for access modifiers... Please
enable JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/2023-03-04-concurrency-in-languages/***

A fun multi-lingual concurrency experiment - Justin A. Ellis Justin Ellis
Principal Engineer, Infinia ML. Former Astrophysicist, former Data Scientist.
Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML The other day I was
writing some code for a CI pipeline that needed to fetch release notes from
several different git repos and collate them into one large set of release
notes for all the different services. My first instinct was to write a little
python helper function and use that in the CI pipeline. On the first iteration
I just looped over the URLs for the different services and did a GET request,
then I collected the results and did some formatting on them. Even though
there were only a few GET requests I noticed this took a few seconds to run.
Then I started thinking about concurrency and implemented something in Go for
fun because I've been obsessed with Go recently. Then I implemented the same
thing in Python and good ole Bash. This post will mostly just go over these
implementations, do a little explanation and mainly just point out that the
overall code is nearly identical for these three languages. Then at the end we
will see which one is the fastest. In this example I am using a placeholder
API to grab titles of TODO items. I add a 500ms delay to simulate a real
endpoint that is a bit slow. All the code here can be found on Github . The
overall structure is the same for all three. Create function to fetch a single
todo given its ID Concurrently fetch all TODOs using this function Process
TODOs and output a single list of TODOs in markdown format Go Version package
main import ( "encoding/json" "fmt" "log" "net/http" "os" "strconv" "sync"
"time" ) const ( todoServer = "https://jsonplaceholder.typicode.com/todos" )
func getTodo ( index int ) ( string , error ) { time . Sleep ( 500 * time .
Millisecond ) resp , err := http . Get ( fmt . Sprintf ( "%s/%d" , todoServer
, index )) if err != nil { return "" , err } if resp . StatusCode != http .
StatusOK { return "" , fmt . Errorf ( "Error getting todo %d: %s" , index ,
resp . Status ) } defer resp . Body . Close () var todo struct { Title string
`json:"title"` } if err := json . NewDecoder ( resp . Body ) . Decode ( & todo
); err != nil { return "" , err } return todo . Title , nil } func main () {
numTodos , err := strconv . Atoi ( os . Args [ 1 ]) if err != nil { log .
Fatalf ( "Error parsing number of todos: %v" , err ) } var wg sync . WaitGroup
todos := make ([] string , numTodos ) for i := 0 ; i < numTodos ; i ++ { wg .
Add ( 1 ) go func ( j int ) { defer wg . Done () todo , err := getTodo ( j \+
1 ) if err != nil { log . Printf ( "Error getting todo %d: %v" , j \+ 1 , err
) } todos [ j ] = todo }( i ) } wg . Wait () fmt . Println ( "# Todos" ) for _
, todo := range todos { if todo == "" { continue } fmt . Printf ( "* %s \n " ,
todo ) } } In the Go version I used waitgroups instead of channels since I
want the output to be ordered and it is just easier with waitgroups. In the
main function we loop over the TODO IDs and make an inline goroutine to get
the todo and add it to the TODOs array. Notice that we make the array
beforehand and are not appending to a slice, this would lead to race
conditions. We then wait for all goroutines to finish by using wg.Wait() ,
lastly we loop over the TODOs that we have pulled down and make a markdown
list with a title. Overall its very simple and concise. As usual, most of the
lines are error checking. Python Version import asyncio import logging import
sys from typing import Optional import httpx TODO_SERVER =
"https://jsonplaceholder.typicode.com/todos" logger = logging . getLogger (
__name__ ) async def get_todos ( client : httpx . AsyncClient , index : int )
-> Optional [ str ]: try : await asyncio . sleep ( 0.5 ) response = await
client . get ( f " { TODO_SERVER } / { index } " ) response . raise_for_status
() except httpx . HTTPStatusError as exc : logger . warning ( f "Failed to get
todo { index } : { exc } " ) return return response . json ()[ "title" ] async
def main ( num_todos : int ) -> None : async with httpx . AsyncClient () as
client : tasks = [ get_todos ( client , index \+ 1 ) for index in range (
num_todos )] results = await asyncio . gather ( * tasks ) print ( "# Todos" )
for todo in results : if todo is None : continue print ( f "* { todo } " ) if
__name__ == "__main__" : asyncio . run ( main ( int ( sys . argv [ 1 ]))) In
the Python version I used the asyncio library to make the concurrent requests
using asynico.gather and used the httpx library to make the asynchronous HTTP
requests. Like the Go version we have a separate get_todos function that takes
in a client and an index and returns the title of the TODO. In the main
function we create a client and then create a list of tasks that we will run
concurrently. We then use asyncio.gather to run all of the tasks and wait for
them to finish. They will be gathered in the order that the list comprehension
was created with. Lastly we then loop over the results and print out the TODOs
in markdown format. Overall this is slightly shorter than the Go version.
There are many ways to do nearly the same thing in Python, but this is
probably the most popular way nowadays since async/await was added to the
language. Bash Version #!/bin/bash TODO_SERVER =
"https://jsonplaceholder.typicode.com/todos" NUM_TODOS = $1 get_todo (){ sleep
0.5 curl -s \--fail $TODO_SERVER / $1 | jq -r '.title' > "/tmp/todo_ $1 .txt"
} for (( i = 1 ; i< = ${ NUM_TODOS } ; i++ )) ; do get_todo $i & done wait
echo "# Todos" for (( i = 1 ; i< = ${ NUM_TODOS } ; i++ )) ; do echo "* $( cat
/tmp/todo_ ${ i } .txt ) " && rm "/tmp/todo_ ${ i } .txt" done This one was
the most fun to write. Even though I use bash everyday in CI/CD tasks, I
usually don't write full scripts or try to do much concurrently. To mirror the
structure of the other implementations I created a function to get a single
TODO using curl and jq and then looped over the TODO IDs and called the
function in the background. The big difference here is that since by running
in the background we are using separate processes and it is hard to share data
between them. To get around this I used a temporary file to store the TODO
title with a filename that was the TODO ID. I then used wait to wait for all
the background processes to finish. Lastly I looped over the TODO IDs again
and printed out, read in the content of the temporary files and output the
TODOs in markdown format. This implementation is the shortest and most
concise, but, as we will see in the next section, it is also the slowest.
Benchmarking So I didn't do any fancy benchmarking, I ran each version with
200 TODOs and ran them 10 times and took the average. The results are below:
Language Average Time (s) Uncertainty (ms) Go 0.71 26 Python 1.42 121 Bash
3.84 540 So as we can see the results are probably what you would expect. Go
is fastest, Python is second and Bash is last; however, remember that all of
these would have taken ~100 seconds if they were done sequentially. So even
though Bash is the slowest, it is still 26x faster than the sequential
version. Bash is slowest because it us spawning a new process for each TODO
and doing extra IO with the temporary files. For Python vs Go, I'm sure there
is a full reason why it is about 2x slower than the Go version but I think
overall it is just because the Go version is compiled and the Python version
is interpreted. Furthermore Go was specifically build with concurrency in mind
and it is more of an add-on to Python. Conclusion So this was a strange one
but I had fun playing around with it and hopefully you will dig into it a bit
more. I'm planning on more posts using Go as I am really starting to like it
and I think this post shows some of its power and simplicity. I'm also
planning on doing some more posts on Python and Bash, so stay tuned. If you
have any questions or comments feel free to reach out to me on Twitter or
leave a comment below. Thanks for reading! (These last two sentences were
completely generated with Copilot but it sounds right, so there it is.) Share
on Twitter Facebook Google+ LinkedIn Leave a Comment You May Also Enjoy
Production Dockerfile Tips and Tricks 6 minute read Container images are the
backbone of a lot of modern workflows, especially in the age of Kubernetes and
friends. Building images that are small, secure, and ... Some Advanced Typing
Concepts in Python 14 minute read To start off, let me admit that yes, Python
is a dynamically (gradually?) typed language. However, with modern Python
(3.6+, and really with 3.10+) and stati... Early and late binding closures in
Python 3 minute read So it happened again, late-binding closures bit me. I
once again had to discover that this was a thing so I decided to make a little
post about it just in ca... Access Modifiers in Python 9 minute read If you
have stumbled across this post then you are probably coming to Python from
another object oriented language that has real support for access modifiers...
Please enable JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/2023-01-24-python-dockerfile/***

Production Dockerfile Tips and Tricks - Justin A. Ellis Justin Ellis Principal
Engineer, Infinia ML. Former Astrophysicist, former Data Scientist. Follow
Durham, NC Email Twitter LinkedIn GitHub Infinia ML Container images are the
backbone of a lot of modern workflows, especially in the age of Kubernetes and
friends. Building images that are small, secure, and efficient is crucial for
production level applications. In this post we will cover some quick tips and
tricks for writing better Dockerfiles (you don't actually have to use Docker
but the name Dockerfile is probably gonna stick around). The TLDR version of
this post is: Use the smallest base image possible Make builds deterministic
Run with a privileged user (never as root) Separate dependencies from source
code Keep things that change more frequently closer to the bottom of the
Dockerfile The smaller the final image, the better Instead starting with a bad
Dockerfile and building up a better one, lets just start with a production
level Dockerfile and use that as our example to explain some concepts. FROM
python:3.10.9-slim-bullseye ENV USER=rta WORKDIR /app RUN useradd -mrU $USER
&& \ chown -R $USER : $USER /app COPY src/requirements.txt . RUN pip install
\--no-cache-dir -r requirements.txt && \ rm -f requirements.txt COPY
\--chown=$USER:$USER src/main.py /app USER $USER ENTRYPOINT ["uvicorn",
"main:app", "--host", "0.0.0.0", "--port", "8080"] Small base image and
deterministic builds FROM python:3.10.9-slim-bullseye Ok, lets start at the
top, notice we use a "slim" python image and we also specify the exact python
version. Specifying the exact python version helps with making our build
deterministic. And choosing the slim image helps keep the final image smaller.
In general there are 3ish main flavors of python images: python 3.10-alpine
8a3a8409a638 Less than a second ago 50.1MB python 3.10-slim-bullseye
72fb1d206063 Less than a second ago 126MB python 3.10 109421b4b8cd Less than a
second ago 921MB Although the Alpine image is the smallest, it can be hard to
work with sometimes. I would recommend always starting with the slim image and
going from there. It is very unlikely that you will need a full debian or
ubuntu OS in your Docker image. So we have a small (mostly deterministic) base
image. If we really want this truly deterministic then we could use the full
image digest ( docker images --digests | grep python ) instead of the tag but
this may be overkill and can make your Dockerfile less readable. Privileged
non-root user ENV USER=rta WORKDIR /app RUN useradd -mrU $USER && \ chown -R
$USER : $USER /app Next we set a USER environment variable that we will use
throughout the Dockerfile. This next step is probably the one that is most
often missed in example Dockerfiles that you find on the interwebs. After
specifying the WORKDIR , which will create the directory, we create a
privileged user and group with the useradd command. Using the -m flag we are
creating a home director for this user. This is important since many python
packages may put data files in the user's home directory by default so we need
it to exist. In this same step (because we want to keep the number of layers
to a minimum) we also give this user ownership over the WORKDIR . This is
important so that we don't end up with any runtime errors if our code writes
and files to this directory. Note we are just creating the user and group
here, we are not setting the user yet. Its generally good to install packages
(especially system level packages) using the default root user and to set the
privileged user at the end of the Dockerfile. Separating dependencies from
source code COPY src/requirements.txt . RUN pip install \--no-cache-dir -r
requirements.txt && \ rm -f requirements.txt In a lot of samples online you
see some junk like COPY . . . Don't do this. First of all you will likely be
copying a bunch of stuff you won't need, but even worse, you will be
invalidating the cache any time you change any code. Since each layer is
cached and since generally dependencies change way less often than the source
code itself its best to separate dependencies from code as we do here. We just
copy the requirements.txt file (remember those deterministic builds, use this
requirements file as a real lock file using something like pip-tools , every
dependency version should be fully specified). Keeping these layers separated
like this will improve build times where you have a local cache and will also
improve pull times on systems like Kubernetes. In the next line we install our
dependencies. Note that we are using the \--no-cache-dir flag here, this will
reduce the final docker image size since we are not creating a pip cache. We
also clean up after ourselves by removing the requirements file since we don't
need it to be in the final image. Move things that change often close to
bottom of Dockerfile COPY \--chown=$USER:$USER src/main.py /app USER $USER
ENTRYPOINT ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"] Ok,
the dependencies are built, now we need to actually bring in the source code.
Note that we use the COPY --chown=user:group syntax. This is because by
default the copy command will copy the file with root ownership but we want
our user to own the source code. Lastly, we set the user that (that only owns
what is in this WORKDIR ) and run our ENTRYPOINT script. There are some
arguments as to whether you should use ENTRYPOINT or CMD for things like this
but its a little harder to accidentally run the wrong thing with ENTRYPOINT
than it is with CMD so I would recommend using ENTRYPOINT . Multi-stage builds
for smaller images In some cases, especially if your code requires compilers
or other system level libraries it is best to use a multi-stage build where
you install any system dependencies along with your python dependencies in the
first stage and copy just the needed python environment into the second stage.
Lets assume that there a something in your code that requires a c-compiler
when installing the python dependencies. Your Dockerfile may look something
like FROM python:3.10.9-slim-bullseye as build RUN apt-get update && apt-get
install -y \--no-install recommends \ build-essential gcc WORKDIR /app RUN
python -m venv /opt/venv ENV PATH="/opt/venv/bin:$PATH" COPY
src/requirements.txt . RUN pip install -r requirements.txt FROM
python:3.10.9-slim-bullseye WORKDIR /app RUN useradd -mrU $USER && \ chown -R
$USER : $USER /app COPY \--from=build /opt/venv /opt/venv COPY
\--chown=$USER:$USER src/main.py /app USER $USER ENV
PATH="/opt/venv/bin:$PATH" ENTRYPOINT ["uvicorn", "main:app", "--host",
"0.0.0.0", "--port", "8080"] In the first stage we install our system level
dependencies and our python dependencies. Note that we create a virtual
environment here so we can copy over just our python dependencies into the
second stage. The second stage creates our privileged user, copies over the
virtual environment and sets the PATH variable to point there. As a rule of
thumb, any time you need extra "stuff" to setup your app that you won't need
at runtime you should use a two stage build and copy over the bare minimum of
what will be needed. Conclusion In this short post we have seen some best
practices for building production level dockerfiles. There are several great
resources out there that go into a lot more detail than this post but
hopefully this will get you on your way to writing great Dockerfiles.
Aforementioned great resources Best practices from Snyk Best practices from
testdriven.io Best practices from docker Share on Twitter Facebook Google+
LinkedIn Leave a Comment You May Also Enjoy A fun multi-lingual concurrency
experiment 4 minute read The other day I was writing some code for a CI
pipeline that needed to fetch release notes from several different git repos
and collate them into one large s... Some Advanced Typing Concepts in Python
14 minute read To start off, let me admit that yes, Python is a dynamically
(gradually?) typed language. However, with modern Python (3.6+, and really
with 3.10+) and stati... Early and late binding closures in Python 3 minute
read So it happened again, late-binding closures bit me. I once again had to
discover that this was a thing so I decided to make a little post about it
just in ca... Access Modifiers in Python 9 minute read If you have stumbled
across this post then you are probably coming to Python from another object
oriented language that has real support for access modifiers... Please enable
JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/publication/***

Publications - Justin A. Ellis Justin Ellis Principal Engineer, Infinia ML.
Former Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML Here is a list of my peer-reviewed and submitted
papers.

***URL: https://jellis18.github.io/about/***

About Me - Justin A. Ellis Justin Ellis Principal Engineer, Infinia ML. Former
Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML I am a Principal Engineer at Infinia ML . There I
work at the intersection of DevOps, Web Development, and Machine Learning. On
a given day I may be working on a FastAPI/Django backend while dealing with
mountains of YAML for k8s deployments, writing terraform for infrastructure
deployment, helping ML Engineers use cloud resources effectively, and helping
architect awesome deployment pipelines for continuous Delivery. I love to
code. I love learning new languages/platforms/paradigms. Most of my career I
have been a Python programmer but I am always on the hunt for new exciting
things in the programming/cloud space.

***URL: https://jellis18.github.io/year-archive/***

Posts by Year - Justin A. Ellis Justin Ellis Principal Engineer, Infinia ML.
Former Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML Posts by Year 2023 A fun multi-lingual concurrency
experiment 6 minute read The other day I was writing some code for a CI
pipeline that needed to fetch release notes from several different git repos
and collate them into one large s... Production Dockerfile Tips and Tricks 6
minute read Container images are the backbone of a lot of modern workflows,
especially in the age of Kubernetes and friends. Building images that are
small, secure, and ... Some Advanced Typing Concepts in Python 14 minute read
To start off, let me admit that yes, Python is a dynamically (gradually?)
typed language. However, with modern Python (3.6+, and really with 3.10+) and
stati... 2022 Early and late binding closures in Python 3 minute read So it
happened again, late-binding closures bit me. I once again had to discover
that this was a thing so I decided to make a little post about it just in
ca... Access Modifiers in Python 9 minute read If you have stumbled across
this post then you are probably coming to Python from another object oriented
language that has real support for access modifiers... Abstract Base Classes
and Protocols: What Are They? When To Use Them?? Lets Find Out! 13 minute read
In Python there are two similar, yet different, concepts for defining
something akin to an interface, or a contract describing what methods and
attributes a ... 2021 A Modern and Explicit approach to Python Exceptions 13
minute read Two of the most popular "modern" programming languages are Rust
and Go. While these languages are quite different, both have a few things in
common... Building a fully typed LRU Cache in Python 10 minute read In this
post we are going to build a fully typed LRU (least recently used) cache
(almost) from scratch using Python. We will then create a function
decorator... A Practical Guide to MCMC Part 2: Further Resources 1 minute read
If you are coming here Part 1 then I'm sorry to tell you that this is not the
fully fleshed out post you were hoping for. If you are coming here directly,
go... How to transition from Academia to Data Science 21 minute read Oh boy,
another one of these blog posts about transitioning to Data Science from
Academia. Well, in this post I'll try to add a slightly different take on
th... 2018 Personal Analytics Part 1: Gmail 18 minute read Most of us are
sending or receiving tens to hundreds of emails daily. I have been using email
regularly since my undergrad days in 2008 so I thought it would... A Practical
Guide to MCMC Part 1: MCMC Basics 15 minute read Markov Chain Monte-Carlo
(MCMC) is an art, pure and simple. Throughout my career I have learned several
tricks and techniques from various "artists&quot...; 2017 Continuous
Integration of Hugo Website using Travis CI and Github. 3 minute read In this
post I will go over how to set up a clean continuous integration system for
your personal Github site using Hugo and Travis CI. Automating Jupyter Slides
with Travis CI and gh-pages. 3 minute read From a Jupyter notebook to a
beautiful web presentation in no time! 2016 I'm Quite Ready for Another
Adventure 7 minute read As part of my transition to Data Science I figured I
should get some first hand experience into the field in which I'm
transitioning. A few months ago I got ... Starting a Blog less than 1 minute
read As is seemingly more common in our field of Astrophysics, I am working on
making the transition to Data Science. As part of the learning process for me
and i...

***URL: https://jellis18.github.io/post/2021-12-13-python-exceptions-rust-go/***

A Modern and Explicit approach to Python Exceptions - Justin A. Ellis Justin
Ellis Principal Engineer, Infinia ML. Former Astrophysicist, former Data
Scientist. Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML Two of
the most popular "modern" programming languages are Rust and Go. While these
languages are quite different, both have a few things in common. First, they
are loved by their users , are growing fast, and both forego exceptions as a
way of handling errors, instead they treat errors like any other type/object.
Since Python is dynamically types or nowadays, "gradually typed" , handling
errors with exceptions makes sense. But in the modern Python era we can make
use of type annotations to make error handling a bit more explicit. After all,
"Explicit is better than implicit" . In this post we will be exploring some
alternative ways of handling errors in Python based on the ways in which Rust
and Go handle errors. But first, lets review how errors are handled in the
three languages. {: .notice--info} As I was writing this and working on the
code I came across a much more fleshed out version of a python Rust result
type so if you actually want to use this in your code I would recommend this
package as it is much more fleshed out. Traditional Error Handling in Python
Traditionally error handling in Python is handled through Exceptions . While
there are many ways to handle these exceptions with differing levels of
sophistication the basic structure looks like this: def divide ( x : float , y
: float ) -> float : if y == 0 : raise ZeroDivisionError ( "Cannot divide by
0" ) return x / y So here we check that the denominator is 0 (because we can't
divide by 0) and if so, we will raise a special exception ZeroDivisionError
with a custom message, otherwise we get on with our usual business of dividing
x by y . When we call this function we would handle the error like: try :
value = divide ( x , y ) except ZeroDivisionError : # handle here Or if choose
not to recover then we can just call the function and the Exception will cause
the program to crash. This is all fine and is indeed the standard way of
handling errors but it is a bit tricky to know that this function may raise
this error. Even if is provided in the documentation (which it is best
practice to document any exceptions that could be raised) there is nothing for
any linters or static type checkers to pick up on that will let a user know
that the divide function could raise an error. This is where Rust and Go do
something different as we will now see. Error Handling in Rust In Rust errors
are handles by using the Result type which is actually a rust enum containing
two variants, an Ok representing success and an Err representing an error. We
specify in the return type the values stored in Ok and Err , respectively. Our
divide function would look something like this in Rust: fn divide ( x : f64 ,
y : f64 ) -> Result < f64 , & 'static str > { if y == 0.0 { return Err (
"Cannot divide by 0" ); } Ok ( x / y ) } If we call this function we can
handle this in many different ways but a few common ways would be // handle
via pattern matching match divide ( x , y ) { Ok ( v ) => println! ( "The
result is {}" , v ), Err ( e ) => println! ( "Error in divide: {}" , e ), } //
handle via a method on Result that will return the value is successful or //
panic if there was an error let value = divide ( x , y ) .unwrap (); There are
many many other ways to handle this and don't worry about the syntax if you
are unfamiliar with Rust, the main point is that value is not itself the value
x/y but an enum of type Result with two variants, Ok(x/y) or Err("Cannot
divide by 0") . This forces the caller to handle any potential errors
explicitly . The second method by calling unwrap() is most similar to just
calling the function in python and letting it raise an error if it indeed
throws that error; however, calling unwrap is generally bad practice in Rust.
Nevertheless, a caller is still aware that the function can fail just by the
fact that the return type is a Result . Error Handling in Go In Go, errors are
handled very explicitly. Instead of returning a single type like in Rust, Go
returns both the value and an error. If there is no error then it will be set
to nil . While this is more verbose than Rust it does follow the same
principle of explicitly leaving error handling up to the caller by putting the
error in the return of the function. Our division function would be written
something like this: import ( "errors" ) func Divide ( x , y float64 ) (
float64 , error ) { if y == 0.0 { return 0.0 , errors . New ( "Cannot divide
by 0" ) } return x / y , nil } Just like in the above cases we check for the
error case of y=0 and then return a placeholder value and the error. If there
is no error then we return the real value and nil . Then when calling this
function we would do something like import ( "fmt" ) value , err := Divide ( x
, y ) // it is always best practice to do this check // the value should not
be trusted if this check is not done if err != nil { fmt . Println ( "An error
has occured:" , err ) return } fmt . Println ( "Dividend is:" , value ) Just
like with Rust, the user will explicitly know that this function can fail,
just by the fact that it returns a value an a possible error. This check of if
err != nil is ubiquitous in Go code and is the main way to check for errors.
One difference with Rust is that a user could still go ahead and use the value
even if there is an error but that wouldn't be very smart and is not
recommended. A Go-like error handling pattern in Python We have seen the
basics of how errors are handled in Rust and Go. Lets see how we can make our
Python error handling more Go-like. The simplest way to do this is to re-write
our function like: from typing import Optional , Tuple def divide ( x : float
, y : float ) -> Tuple [ float , Optional [ ZeroDivisionError ]]: if y == 0 :
return 0 , ZeroDivisionError ( "Cannot divide by 0" ) return x / y , None Here
we return two values just like in our Go code, the actual value or a
placeholder along with a ZeroDivisionError or None . In this case we still
make use of the builtin exceptions because hey, we are using Python! For the
typing, we have to specify the error as Optional since we can also return None
. When using this function we can follow this pattern value , err = divide ( x
, y ) if err is not None : raise err print ( f "Dividend is: { value } " )
Looks almost just like the Go code! Just as in our example above we can still
use the value without the check but at least we know that this can return an
error by the return signature and types. In this case since we are still using
Python exceptions we can raise the error if we don't have any way of avoiding
it. This is similar to the try-except statement from above but now we
explicitly use the error as a value and not just something that may happen
when calling the function. Just for fun, we can make this a bit cleaner using
python generics! T = TypeVar ( "T" ) E = TypeVar ( "E" , bound = Exception )
Result = Tuple [ T , Optional [ E ]] def divide ( x : float , y : float ) ->
Result [ float , ZeroDivisionError ]: if y == 0 : return 0 , ZeroDivisionError
( "Cannot divide by 0" ) return x / y , None In Rust-like fashion we now
return a Result type that is just a type alias for a tuple of any type and an
option error that is a subclass of Exception . While this does the same thing
it is less verbose and could represent an idiomatic way of doing things. If we
don't want to specify the exact exception type we could make another type
alias: from typing import Optional , Tuple , TypeVar T = TypeVar ( "T" )
ResultWithErr = Tuple [ T , Optional [ Exception ]] def divide ( x : float , y
: float ) -> ResultWithErr [ float ]: if y == 0 : return 0 , ZeroDivisionError
( "Cannot divide by 0" ) return x / y , None This just tells us that we are
returning a result and an error, where the error is an Exception of some kind.
These type aliases could be nice to cut down on boilerplate but possibly make
things a bit less explicit. In any case, Python offers a lot of options in
terms of typing and actually returning errors. Doing error handling the Go way
is pretty straightforward in Python and is definitely more explicit than the
try-except methods. A Rust-like error handling pattern in Python While
implementing errors Go-style was pretty straightforward, to get a Rust-like
system will be a bit more work. So, in Rust-land an enum can hold values and
have methods defined on it. In rust, the Result enum looks like: enum Result <
T , E > { Ok ( T ), Err ( E ), } where T and E are generic types. One could
then implement methods on this enum. In Python things don't quite work that
way, so to mimic the Result type we will instead define two generic classes
Ok[T,E] and Err[T, E] and use a type alias of Union to make a Result[T, E] .
In the end we will have a Rust-like error handling system with a small subset
of the Rust functionality. Lets start with the Ok class from typing import Any
, Callable , Generic , TypeVar T = TypeVar ( "T" ) E = TypeVar ( "E" , bound =
BaseException ) class Ok ( Generic [ T , E ]): _value : T __match_args__ = (
"_value" ,) def __init__ ( self , value : T ): self . _value = value def
__eq__ ( self , other : Any ) -> bool : if isinstance ( other , Ok ): return
self . _value == other . _value # type: ignore return False def unwrap ( self
) -> T : return self . _value def unwrap_or ( self , default : T ) -> T :
return self . unwrap () def unwrap_or_else ( self , op : Callable [[ E ], T ])
-> T : return self . unwrap () def __repr__ ( self ) -> str : return f "Ok( {
repr ( self . _value ) } )" Ok (get it!) lets unpack this a bit. First this
class is generic (since it inherits from Generic ) over a general type
variable T and a bound type variable E . In this case we place a bound on E so
that it must be a subclass of BaseException . This is different than Rust
where anything can be used as an error but Python already has exceptions so we
may as well use them. Next we see that this class is initialized with a value
of type T which can be any type. We define the magic method __eq__ to return
True if the contained values match in another instance of Ok . This will be
important later when performing pattern matching or any other equality tests.
We have also defined a nice repr on this class so that it is easily inspected.
Now, we have some methods on this class which are meant to mirror the
functionality of the Rust Result enum . Before we get into the methods, lets
define the Err class since it will have the same methods. class Err ( Generic
[ T , E ]): _err : E __match_args__ = ( "_err" ,) def __init__ ( self , err :
E ): self . _err = err def __eq__ ( self , other : Any ) -> bool : if
isinstance ( other , Err ): return self . _err == other . _err # type: ignore
return False def unwrap ( self ) -> NoReturn : raise self . _err def unwrap_or
( self , default : T ) -> T : return default def unwrap_or_else ( self , op :
Callable [[ E ], T ]) -> T : return op ( self . _err ) def __repr__ ( self )
-> str : return f "Err( { repr ( self . _err ) } )" Just as in the Ok case
this class is generic over T and E . It is also initialized with an err of
type E (must be a subclass of BaseException ) And it defined __eq__ in an
analogous way to Ok . Now for the methods. Well, first lets define our result
type: Result = Union [ Ok [ T , E ], Err [ T , E ]] This is just a type-alias
of Union . So if we do Result[float, ZeroDivisionError] this means that a
function/method will return either Ok(float) or Err(ZeroDivisionError) . Since
we can return either we must have exactly the same methods on both the Ok and
Err classes. This is as close as we can get to defining methods on a single
enum as we can do in Rust. Without further ado lets look at the methods and
how they would be used in our canonical example. def divide ( x : float , y :
float ) -> Result [ float , ZeroDivisionError ]: if y == 0 : return Err (
ZeroDivisionError ( "Cannot divide by 0" )) return Ok ( x / y ) where we
return Err if y=0 and wrap our output in Ok if not just like our Rust example
above. When we use this function we can handle the errors in multiple ways #
handle via pattern matching (very Rusty!, but only in python >= 3.10) match
divide ( x , y ): case Ok ( v ): print ( f "The value is { v } " ) case Err (
e ): print ( f "The error is { e } " ) # unwrap the result and let an error be
raised if the function failed value = divide ( x , y ). unwrap () The first
method here uses Python's new structural pattern matching which has a syntax
very similar to Rust. Note, the __match_args__ class attributes in Ok and Err
lets us match on the values without providing the keyword. The second method
here calls the unwrap method, which, as we can see from the code above, will
just return the value if Ok and will raise the error if the function errored.
This is nearly equivalent to just raising the error in the function itself
(the standard python way) but now at least we know the function can error by
the Result return type and the fact that we must call unwrap . However, there
are more things we could do. We can provide a default value to fall back on if
the function errors # this will return the Ok value if no error, otherwise it
will return the default (0.0) value = divide ( x , y ). unwrap_or ( 0.0 ) We
can also provide a callable to try to recover from the error x , y = 1.0 , 0.0
EPS = 1e-9 value = divide ( x , y ). unwrap_or_else ( lambda e : x / ( y \+
EPS )) In this case we supply a function that takes in the error (in this case
we don't actually use the error value but we could) and returns the division
again but with a small offset to the zero denominator. This may or not be a
good idea in this case but it illustrates the point. This method
unwrap_or_else will return the Ok value if there is no error, otherwise it
will call the supplied function. It is not shown directly here but all of this
is fully typed and will work with type checkers in and intellisense! So this
was a bit more work but we have partially replicated some of the Rust
functionality and we have made error handling explicit ! Summary Python
handles errors by raising exceptions which can then be caught in downstream
callers. However, since Python is becoming gradually typed it makes sense to
think of alternate ways to handle errors that are made explicit through the
type system. In this post we have looked at how two modern languages, Rust and
Go, handle errors and showed how we can do similar things in Python. In
summary: Go handles errors by returning the value alongside the error. We can
do this in Python by returning a tuple or the value and a possible error. We
then check for this error explicitly in our downstream code and we know about
it from the return type. Rust handles errors by returning an enum with two
variants Ok or Err . The caller then must pull out the value or error through
various different methods. We can replicate this in python by returning a
single value which is a type alias of Union . A downstream caller then will
"unwrap" this in ways similar to Rust. Lastly, I will mention that neither of
these approaches is likely to become mainstream simply because of the massive
code bases already written and the now idiomatic way of handling exceptions;
however it may still be useful to use these methods on stand-alone projects or
to just think about alternatives! Share on Twitter Facebook Google+ LinkedIn
Leave a Comment You May Also Enjoy A fun multi-lingual concurrency experiment
4 minute read The other day I was writing some code for a CI pipeline that
needed to fetch release notes from several different git repos and collate
them into one large s... Production Dockerfile Tips and Tricks 6 minute read
Container images are the backbone of a lot of modern workflows, especially in
the age of Kubernetes and friends. Building images that are small, secure, and
... Some Advanced Typing Concepts in Python 13 minute read To start off, let
me admit that yes, Python is a dynamically (gradually?) typed language.
However, with modern Python (3.6+, and really with 3.10+) and stati... Early
and late binding closures in Python 3 minute read So it happened again, late-
binding closures bit me. I once again had to discover that this was a thing so
I decided to make a little post about it just in ca... Please enable
JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/portfolio/***

Portfolio - Justin A. Ellis Portfolio Projects I’m working on, talks or
interviews I’ve given and other things about me. Machine Learning Exercises
Jupyter notebook solutions and notes to the Coursera Machine Learning course.
enterprise A well tested, well documented data analysis toolbox for pulsar
timing. WV Public Radio Interview An interview I did with WVU public
broadcasting about my academic career and my WV roots. Police Shooting
Analysis Presentation Slides from Research Methods in Data Science Meetup in
Glendale CA. Police Shooting Data A collection of police shooting data along
with various crime and census covariates. Parallel Tempering MCMC Sampler A
Python package that implements adaptive parallel tempering MCMC using MPI
Demystifying Deep Learning Slides from Research Triangle Analysts Meetup in
Cary NC

***URL: https://jellis18.github.io/portfolio/police-shooting-talk/***

Police Shooting Analysis Presentation - Justin A. Ellis Direct Link Share on
Twitter Facebook Google+ LinkedIn

***URL: https://jellis18.github.io/portfolio/police-shooting/***

Police Shooting Data - Justin A. Ellis Direct Link Share on Twitter Facebook
Google+ LinkedIn

***URL: https://jellis18.github.io/portfolio/ptmcmc/***

Parallel Tempering MCMC Sampler - Justin A. Ellis Direct Link Share on Twitter
Facebook Google+ LinkedIn

***URL: https://jellis18.github.io/portfolio/enterprise/***

enterprise - Justin A. Ellis Direct Link Share on Twitter Facebook Google+
LinkedIn

***URL: https://jellis18.github.io/post/2021-11-25-lru-cache/***

Building a fully typed LRU Cache in Python - Justin A. Ellis Justin Ellis
Principal Engineer, Infinia ML. Former Astrophysicist, former Data Scientist.
Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML In this post we are
going to build a fully typed LRU (least recently used) cache (almost) from
scratch using Python. We will then create a function decorator that mirrors
the builtin functools implementation . This exercise will cover several
advanced python concepts including generic types and other advanced typing
concepts, decorators (including second order decorators), and magic methods .
{: .notice--info} This exercise is for learning and demonstration purposes and
thus is not optimized for performance. All source code can be found here What
we will end up with What we will have at the end of this exercise is a clone
(at least in terms of the public interface) of the functools lru_cache
decorator. We should get these results: @ lru_cache ( maxsize = 100 ) def fib
( n : int ) -> int : if n < 2 : return n return fib ( n \- 1 ) \+ fib ( n \- 2
) >>> [ fib ( n ) for n in range ( 16 )] [ 0 , 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21
, 34 , 55 , 89 , 144 , 233 , 377 , 610 ] >>> fib . cache_info () CacheInfo (
hits = 28 , misses = 16 , maxsize = 100 , currsize = 16 ) In addition to this
we also want to retain all type information from the decorated function. In
older versions of functools.lru_cache this information is lost so static type
checkers and intellisense could not function correctly. In our version we will
retain this type information so that static type checkers will work correctly.
Building the cache class First, we will build a standalone LruCache class to
handle that actual heavy work. In most implementations of LRU cache, a hash
map (i.e. dictionary) and a doubly linked list are used . In this case, since
the main point of this article is how to use some of the more advanced python
features we will use one single built in data type, the OrderedDict The full
implementation is below: from collections import OrderedDict from typing
import Generic , Hashable , Optional , TypeVar T = TypeVar ( "T" ) class
LruCache ( Generic [ T ]): def __init__ ( self , capacity : int ): self .
capacity = capacity self . __cache : OrderedDict [ Hashable , T ] =
OrderedDict () def get ( self , key : Hashable ) -> Optional [ T ]: if key not
in self . __cache : return None self . __cache . move_to_end ( key ) return
self . __cache [ key ] def insert ( self , key : Hashable , value : T ) ->
None : if len ( self . __cache ) == self . capacity : self . __cache . popitem
( last = False ) self . __cache [ key ] = value self . __cache . move_to_end (
key ) def __len__ ( self ) -> int : return len ( self . __cache ) def clear (
self ) -> None : self . __cache . clear () Ok, there is a lot to unpack here.
First lets look at the basic functionality. # build a new cache with capacity
2 (i.e. we can store at most two values at a time) >>> cache = LruCache [ str
]( capacity = 2 ) # put two values in to the cache # cache keys will now be
[1, 2] >>> cache . insert ( 1 , "hello" ) >>> cache . insert ( 2 , "hello
again" ) # length is 2 since that is the number of values in the cache >>>
print ( len ( cache )) 2 # can retreive 1 since both 1 and 2 are in the cache
# cache keys are now [2, 1] since 2 was called more recently than 1 >>> print
( cache . get ( 2 )) hello again # this will evict key 1 with value "hello" #
cache keys are now [3, 2] >>> cache . insert ( 3 , "goodbye" ) # we now return
None since 1 is no longer in the cache >>> print ( cache . get ( 1 )) None #
but we can get the cached value for 3 # cache keys are still [3, 2] since 3
was called more recently than 2 >>> print ( cache . get ( 3 )) goodbye # cache
keys are now [2, 3] >>> print ( cache . get ( 2 )) hello again # this will
drop key 3 # cache keys are now [1, 2] >>> cache . insert ( 1 , "I'm back!" )
# get None since 3 was dropped >>> print ( cache . get ( 3 )) None # finally
we can clear the cache >>> cache . clear () >>> print ( len ( cache )) 0 We
can see that the capacity determines how large the cache is and every call to
cache.get or cache.insert puts that key at the top of the queue essentially
keeping track of the most recently used keys and when we insert a value when
the cache is full (i.e. the length is equal to the capacity) the least
recently used value is dropped. This is what we want. A few important things
to notice about the code, first is that LruCache itself is generic over a type
T which represents the value type of the cache. In the example above we
specify LruCache[str] to indicate the type of value in the insert method and
the return type in the get method is a string. We can pass in any type here
and our IDE and static type checkers are able to infer and check the input and
return types. Pretty awesome huh? The next thing to notice is the Hashable
type. Since we want this class to generic we need to give it the least amount
of information possible about what kinds of keys will be uses. At the very
least dictionary keys need to be hashable (i.e. implements the __hash__
method), and that is exactly what Hashable checks for. If we try to pass in a
key that is not hashable like a list then we will get an error from a static
type checker like pyright Argument of type "list[int]" cannot be assigned to
parameter "key" of type "Hashable" in function "get" "list[int]" is
incompatible with protocol "Hashable" "__hash__" is an incompatible type
Another thing to notice here is the use of the magic method __len__ . This is
a fairly common one but by implementing this method it allows us to call len
on an instance of LruCache . One of ther bit of fanciness here is the use of
the double leading underscore on __cache . We don't want to allow outside
users to be able to modify the cache, and while there are no truly private
variable in python by using the double underscore you will actually get an
AttributeError if you try to access it outside of the class. As for
implementation, as mentioned above, we make use of the OrderedDict provided by
the collections module. For the most part, OrderedDict is almost obsolete
since regular python dictionaries now retain insertion order; however, it is
very useful in this case because of the move_to_end method which lets us move
any key-value pair to the end of the dictionary which is how we keep track of
the lest/most recently used keys in our lru cache. Finally, the popitem method
allows for last=False which drops the key-value pair in a FIFO (first-in-
first-out) manner as oppsed to the default LIFO order (last-in-first-out)
order. This lets us easliy pop off the least recently used key. Building the
function wrapper class Now that we have a generic LruCache implementation we
move on to building a class that will wrap our function that we wish to cache.
What we want is something that has the exact same call signature as our
original function but also has a cache_info and cache_clear method as well as
a __wrapped__ attribute containing a refrerence to the wrapped function
itself. This all mirrors the functools.lru_cache API. from collections.abc
import Callable from typing import Final , Generic , NamedTuple , ParamSpec ,
TypeVar T = TypeVar ( "T" ) P = ParamSpec ( "P" ) class CacheInfo ( NamedTuple
): hits : int misses : int maxsize : int currsize : int class
_LruCacheFunctionWrapper ( Generic [ P , T ]): def __init__ ( self , func :
Callable [ P , T ], maxsize : int ): self . __wrapped__ = func self . __cache
= LruCache [ T ]( capacity = maxsize ) self . __hits = 0 self . __misses = 0
self . __maxsize : Final = maxsize def __call__ ( self , * args : P . args ,
** kwargs : P . kwargs ) -> T : call_args = args \+ tuple ( kwargs . items ())
ret = self . __cache . get ( call_args ) if ret is None : self . __misses += 1
ret = self . __wrapped__ ( * args , ** kwargs ) self . __cache . insert (
call_args , ret ) else : self . __hits += 1 return ret def cache_info ( self )
-> CacheInfo : return CacheInfo ( hits = self . __hits , misses = self .
__misses , currsize = len ( self . __cache ), maxsize = self . __maxsize , )
def cache_clear ( self ) -> None : self . __cache . clear () self . __hits = 0
self . __misses = 0 Ok, lets walk through this. First, we define the CacheInfo
named tuple to match the signature of the functools.lru_cache implementation.
NamedTuple s are good for things like this where you want an immutable but
easy to read/use return type of some function or method. If you need to add
methods or want to modify fields its probably better to use a dataclass . The
next thing to notice is that _LruCacheFunctionWrapper (leading underscore is
used here to indicate that this is a "private" class) is generic of types P
and T which as we see from the __init__ corresponds to the arguments and
return type of the wrapped function, respectively. For the P generic we make
use of the new (in python 3.10) ParamSpec which will make things much nicer as
we will see. From python 3.8 up you can import this from typing_extensions but
it is moved into the builtin typing module in python 3.10. We also use private
attributes (leading underscore) again here because we really don't want these
attributes to be used outside of this class. Note that __wrapped__ is still
accessible since it is a "dunder" attribute. We use the LruCache specifying
the type using the generic variable T as our cache. In this class we make use
of another quite popular magic method __call__ . The call magic method allows
us to call an instance of this class like a function. Furthermore, we have
used the power of generics and the new ParamSpec to actually indicate what the
argument names and types are via the P.args and P.kwargs methods as well as
the return type T . If we were to use this to wrap our function above then
static type checkers and intellisense will know the correct types and will
actually raise errors if the wrong type is passed in, just like it would on
the original function. >>> wrapped = _LruCacheFunctionWrapper ( fib , maxsize
= 4 ) >>> x = wrapped ( 3 ) # type signature (int) -> int This call function
itself is pretty simple since most of the work is handled in LruCache . We
turn the arguments and keyword arguments into a tuple (i.e. something
hashable) and use that as the key in our LRU cache. This does assume that the
input arguments and keyword arguments in our original function are hashable.
Unfortunately, as far as I know, there is no way to bound a ParamSpec so that
it would check if the arguments are hashable. Maybe one day... Nonetheless, we
create the call_args tuple and try to get the function return value from the
LRU cache. If we get a non None return value then we increment the hits (hit
the cache) counter and return the function value. If we don't have the value
in the cache we call our wrapped function and insert the value into the cache
and increment the misses counter (missed the cache). The cache_info method
simply returns a CacheInfo named tuple with the current state of the function
cache. Lastly the clear_cache method just clears the LRU cache and resets the
counters. One last thing to note here before moving onto the final step is
that this way of hashing the function arguments if far from optimal in that it
assumes that positional arguments are not called as keyword arguments and that
keyword arguments are called in the same order. It also does not take into
account default keyword arguments. These things are all fixable through the
use of something like a frozen set and using the inspect module. Maybe in a
follow on post... Building the decorator Ok, we are almost there. We have our
generic LruCache and a function wrapper that uses that cache. Now we just need
our function decorator. In this case, since our decorator takes an input
argument maxsize what we are really constructing is a decorator factory or
second order decorator. from collections.abc import Callable from typing
import ParamSpec , TypeVar T = TypeVar ( "T" ) P = ParamSpec ( "P" ) def
lru_cache ( maxsize : int ) -> Callable [[ Callable [ P , T ]],
_LruCacheFunctionWrapper [ P , T ]]: def decorator ( func : Callable [ P , T
]) -> _LruCacheFunctionWrapper [ P , T ]: return _LruCacheFunctionWrapper (
func , maxsize ) return decorator Without the type annotations this if just a
function that returns another function which wraps our original function.
However, through the use of the generic types, as we have done above we can
retain and pass forward all of the function argument and return types. The
outer decorator lru_cache returns a callable that takes in a generic callable
with arguments P and return type T and returns a generic instance of
_LruCacheFunctionWrapper that we have defined above. The inner decorator takes
in the actual function to be cached and returns an instance of
_LruCacheFunctionWrapper . This may look somewhat messy but the generic types
are quite magical and really will help make your code more safe and "correct".
But thats it, we now have created a decorator that will cache function calls
with a maxsize of maxsize ! Feel free to go back to the beginning and use this
on our fibonacci sequence function and play around in an IDE to see the full
power of types. Summary And there we have it, we have implemented a fully
typed version of lru_cache through the heavy use of generics and OrderedDict .
We have also made things pythonic by using magic methods like __init__ and
__call__ and followed some best practices by separating out the caching
implementation from the actual function wrapper. Share on Twitter Facebook
Google+ LinkedIn Leave a Comment You May Also Enjoy A fun multi-lingual
concurrency experiment 4 minute read The other day I was writing some code for
a CI pipeline that needed to fetch release notes from several different git
repos and collate them into one large s... Production Dockerfile Tips and
Tricks 6 minute read Container images are the backbone of a lot of modern
workflows, especially in the age of Kubernetes and friends. Building images
that are small, secure, and ... Some Advanced Typing Concepts in Python 13
minute read To start off, let me admit that yes, Python is a dynamically
(gradually?) typed language. However, with modern Python (3.6+, and really
with 3.10+) and stati... Early and late binding closures in Python 3 minute
read So it happened again, late-binding closures bit me. I once again had to
discover that this was a thing so I decided to make a little post about it
just in ca... Please enable JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/2016-10-31-tdwi-bootcamp-overview/***

I'm Quite Ready for Another Adventure - Justin A. Ellis Justin Ellis Principal
Engineer, Infinia ML. Former Astrophysicist, former Data Scientist. Follow
Durham, NC Email Twitter LinkedIn GitHub Infinia ML As part of my transition
to Data Science I figured I should get some first hand experience into the
field in which I'm transitioning. A few months ago I got an email about a Data
Science conference in San Diego so I looked in to it and saw that they were
offering a bootcamp. I figured it would be worth it since it was fairly
nearby. It was a bit scary going in since I've never been to a conference
where I literally did not know or had never even seen a single person there.
Nonetheless, here is an overview of the bootcamp (quite delayed at this
point). Am I dreaming? When I arrived in San Diego for the bootcamp I had just
finished reading "Player Piano" by Kurt Vonnegut, which is a book about a
future in which machines (a bit more analog than our current future but
still...) had taken over most jobs and the only real jobs left were for the
engineers and managers. Everyone else is relegated to meaningless public works
jobs or the military. Anyhow, the main character eventually gets caught up in
a revolution to stop the machines and give the jobs back to humans. Eventually
the revolution fails with revolutionaries building their own machines all over
again; meanwhile the leaders of the revolution give up understanding that
maybe it was enough just to try resistance for the record. Anyway, this is
relevant because my first look in to this new data science / business world
was a keynote talk entitled "Replacing Middle Class Factory Jobs with Machine
Learning". At first I thought that maybe it would be about training more
people about machine learning but no, it was actually about replacing people
with machine(s) (learning). However, despite the dubious setup in my mind the
talk was quite interesting and even a bit inspiring. For the most part the
talk described how combinations of neural networks and genetic algorithms
could create schedules for thousands of factory workers, how a support vector
machine (with some additional hardware) could predict the failure of certain
products, and how more neural networks could eliminate the need for humans in
quality assurance. However, for each of these innovative and technical
solutions came social problems. For instance, the new ML scheduler still
needed human intervention at the end but the woman who had previously been
making the schedules for 30 years quit because she felt that she had lost her
purpose. The fancy support vector machine was tricked by factory workers
gaming the system to get more products approved, and the replacement of half
of the work force with neural networks led to lower productivity because the
workers responsible for assembly lost their quality assurance work mates (they
would work in teams). Sounds terrible, I know, but this talk was partially a
cautionary tale about ignoring the human consequences of further automation
(which is inevitable). In fact in many of the instances above, workarounds
were found to increase communication between the workers and the new ML
systems and their "engineers". One case which I liked was the one in which the
factory workers missed having someone to talk to while working. The solution
to this was to simply rotate their stations so that they could talk to the
other workers assembling the materials. Furthermore, they added an element of
competition by adding leader boards and giving out bonuses based on this. In
this case productivity increased (over pre-ML levels) and worker satisfaction
was higher. So in the end I got a handle on the sociological and machine
learning aspects of factory automation with a lesson that people are not
machines and that we will always be better off when we work together. And that
was just the first hour! Data Science Bootcamp Day 1 Day 1 consisted of two
sections: "An Overview of Data Science" and "Data Sourcing and Preparation".
For the most part these two sessions covered similar material; however the
first session was much more interesting. The speaker for the first session has
been doing data science (or data mining or predictive analytics) for 20 years
and he had many examples working with the New York Times to improve
subscription rates, working with the Canadian Tax Agency to spot tax evaders
and other tax fraud, working with other companies to predict customer behavior
(think Amazon). In the second session, the speaker had more limited
experience, mainly climate data and some image tagging work. The introduction
section began by describing the differences between statistics, business
intelligence (BI), predictive analytics, and data science. In brief,
statisticians are very worried about parametric modeling and how that model
fits the data. To the statistician, the model is the king. BI analysts focus
on data but are most interested in key findings (i.e., summary of sales, or
turnover rates in factories). Both predictive analytics and data science deal
with using data to make predictions about future behavior. Data scientists are
more code-centric, mostly programming in R or Python making use of SQL
databases and scalable platforms like Hadoop, whereas predictive analytics
deals more with proprietary GUI-like interfaces and less cutting-edge
algorithms. After this introduction, the two sessions largely dealt with
CRISP-DM (CRoss-Industry Standard Process Model for Data Mining), which
describes the process cycle that many data scientists go through. Most of the
sessions were focused on the Data Understanding and Data Preparation steps as
in many cases 90% of the work lies there. A quick summary of the steps is as
follows Business Understanding This is the step where data scientists talk
with people that are familiar with the problem that you are trying to solve.
These people are also called domain experts. There will be iteration with this
group as you explore the data (i.e., why are there missing values, are
outliers understood, etc) and define objectives. Data Understanding This is
probably the biggest task (in combination with data prep) in the cycle. Here
data scientists explore distributions of the features, look for missing
values, look for outliers, explore the mix of continuous, categorical, and/or
cardinal variables. Essentially this involves making lots of plots and summary
statistics, which depending on you, can either be fun or incredibly tedious.
Data Preparation This is the step where we decide what data we are going to
use, remove redundant variables, replace missing values (mean, median,
distribution, etc), transform features (log, binning, encoding of categorical
features), and create new features (ratios, differences, etc.). Modeling What
kind of algorithm? Supervised vs Unsupervised? Categorical target vs
continuous target? Do we want to understand how the features impact the
prediction or do we just care that the prediction is correct (i.e. decision
trees vs neural networks)? Evaluation What are we trying to optimize? This is
where cross-validation and testing of the model happens. The fitness of the
model is very dependent upon the objective. Deployment This is where
everything is put together. Where will the model be deployed, how will it be
maintained, how often will the model be re-trained? Does it need to be
optimized in a different language or will it work in the original modeling
language? Overall, these sessions were quite helpful in understanding how
professionals approach data science problems. I was pleasantly surprised that
I actually followed most of these steps instinctively when playing around with
my own small data science problems! Data Science Bootcamp Day 2 Honestly, day
two of the bootcamp was more useful personally than it was practically. The
morning session was just a review of basic supervised and unsupervised
learning techniques like linear regression, decision trees, and k-means
clustering; all of which I have already seen quite a bit before. However, over
lunch I managed to start a few conversations with a pretty wide range of
people who work in the tech industry, manufacturing, and public utilities. I
learned that BI means Business Intelligence and I further learned that
business intelligence basically means that you create data dashboards and
other visualizations that summarize data that has been collected by the
company. I further learned that in many data science groups one can earn quite
a large bonus if they recommend a person that is then hired; so I may be less
bashful in asking for help in the future. The last session focused mostly on
how data science teams actually work. I was happy with this session because in
the few examples shown they used Jupyter Notebooks, which are a personal
favorite of mine. I also liked that they focused on time-series analysis a bit
more than the other presenters, mentioning that time-series analysis is often
an overlooked skill that can add real value to data science groups. Good news
for me! This session also introduced me to Agile Software Development which
seems extremely useful and made me a bit envious that we are not organized
enough to use this in academia. Lastly, the group who ran this session has
Github page with a lot of awesome tutorials and tools. So as I left the
bootcamp I literally thought to myself "I'm quite ready for another
adventure!". Share on Twitter Facebook Google+ LinkedIn Leave a Comment You
May Also Enjoy A fun multi-lingual concurrency experiment 4 minute read The
other day I was writing some code for a CI pipeline that needed to fetch
release notes from several different git repos and collate them into one large
s... Production Dockerfile Tips and Tricks 6 minute read Container images are
the backbone of a lot of modern workflows, especially in the age of Kubernetes
and friends. Building images that are small, secure, and ... Some Advanced
Typing Concepts in Python 13 minute read To start off, let me admit that yes,
Python is a dynamically (gradually?) typed language. However, with modern
Python (3.6+, and really with 3.10+) and stati... Early and late binding
closures in Python 3 minute read So it happened again, late-binding closures
bit me. I once again had to discover that this was a thing so I decided to
make a little post about it just in ca... Please enable JavaScript to view the
comments powered by Disqus.

***URL: https://jellis18.github.io/post/2017-11-20-automating-jupyter-slides/***

Automating Jupyter Slides with Travis CI and gh-pages. - Justin A. Ellis
Justin Ellis Principal Engineer, Infinia ML. Former Astrophysicist, former
Data Scientist. Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML
First off, if you use Python and you don't use Jupyter Notebooks then you need
to start right now. For that matter I would say the same thing about R users
(although I'm not that familiar with R-markdown so maybe that's ok). Jupyter
notebooks are changing the way we work with code, instead of writing obscure
"scripts" or re-writing the same code blocks over and over in Ipython or
Rstudio, we now are writing computational essays in which the code, comments,
notes, and figures all live in the same place ensuring more robust and
reproducible research. Ok, now that my evangelizing is done, lets get down to
business. Lets say you have been working on a beautiful Jupyter notebook and
now you have to present your results at a conference or business meeting. The
standard way of doing this would be to gather up all of the figures, some
text, and maybe some code from the notebook and put it into a
PowerPoint/Keynote/Beamer presentation. If you then want to share this
presentation more widely you may host them on Speaker Deck or just throw them
on a Github repository somewhere. Actually, if you have already done this,
then you are ahead of the game in terms of open-source science. But you did so
much work on that notebook, wouldn't it be better if you didn't then have to
spend all that extra valuable time making slides too? Well, we can do that
with Jupyter notebooks and with a few quick hacks you can go from notebook to
web-published slides. The Notebook I will assume a familiarity with Jupyter
notebooks in general but not with the slideshow features. Lets say you have a
notebook called nb.ipynb with some content. To go into slideshow mode click
View -> Cell Toolbar -> Slideshow , this will now add a toolbar to each cell
in the notebook from which you can choose what kind of slide you want each
cell to be. You can choose Slide for a standard slide, Sub-Slide for a
continuation slide, Fragment for revealing bullet points or other elements
within a single slide, Notes to add slide notes, Skip to not include that cell
in the slideshow. Here is an example notebook in case you don't already have
one ready. Lastly, if you include any extra images, place them in an img/
folder and commit that to your Github repo. You can view your notebook in the
web browser with jupyter nbconvert nb.ipynb --to slides --post serve . Now
that we have the notebook, lets do this! Setup Much of this setup is inspired
by Dan Foreman-Mackey's awesome article on continuous integration of academic
papers and the first few steps are nearly identical to what is done there.
First, create a Github repo and commit your notebook. To get started with
Travis, create a .travis.yml file and log in to Travis with your Github
account and enable builds for your repository. You will also need to create a
personal access token . Copy this token and go to the settings page of your
repo on travis and add set GITHUB_API_KEY : set to the personal access token
you just created. GITHUB_USER : set to your Github username. OK, now that we
have Travis set up lets edit the .travis.yml file: language : python cache :
pip python : \- " 2.7" # install jupyter and get reveal.js as we will need it
to build the website # from Travis install : \- pip install jupyter \- wget
https://github.com/hakimel/reveal.js/archive/master.zip \- unzip master.zip \-
mv reveal.js-master reveal.js script : \- jupyter nbconvert nb.ipynb --to
slides --reveal-prefix=reveal.js after_success : | if [ -n "$GITHUB_API_KEY"
]; then git checkout --orphan gh-pages git rm -rf --cached . mv nb.slides.html
index.html git add -f --ignore-errors index.html img reveal.js git -c
user.name='travis' -c user.email='travis' commit -m init git push -f -q
https://$GITHUB_USER:$GITHUB_API_KEY@github.com/$TRAVIS_REPO_SLUG gh-pages fi
Push this to your Github repository and if all goes well then your web-
presentation should be available at https://USERNAME.github.io/REPONAME . If
this worked, any further changes that you make to your Jupyter notebook will
then be automatically built and displayed in the web-presentation. Here is my
example Github repo and presentation . Final Thoughts Now you can make nice
automated Jupyter notebook slides; however, I only showed the very basics of
Jupyter Notebook slideshow options and themes. Things are pretty customizable
if you aren't opposed to some hacking. Lastly, I don't think Jupyter notebooks
are good for all presentations but it is great for code demos and talks that
have code attached to them. Share on Twitter Facebook Google+ LinkedIn Leave a
Comment You May Also Enjoy A fun multi-lingual concurrency experiment 4 minute
read The other day I was writing some code for a CI pipeline that needed to
fetch release notes from several different git repos and collate them into one
large s... Production Dockerfile Tips and Tricks 6 minute read Container
images are the backbone of a lot of modern workflows, especially in the age of
Kubernetes and friends. Building images that are small, secure, and ... Some
Advanced Typing Concepts in Python 13 minute read To start off, let me admit
that yes, Python is a dynamically (gradually?) typed language. However, with
modern Python (3.6+, and really with 3.10+) and stati... Early and late
binding closures in Python 3 minute read So it happened again, late-binding
closures bit me. I once again had to discover that this was a thing so I
decided to make a little post about it just in ca... Please enable JavaScript
to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/2023-01-15-advanced-python-types/***

Some Advanced Typing Concepts in Python - Justin A. Ellis Justin Ellis
Principal Engineer, Infinia ML. Former Astrophysicist, former Data Scientist.
Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML To start off, let
me admit that yes, Python is a dynamically ( gradually? ) typed language.
However, with modern Python (3.6+, and really with 3.10+) and static tooling
(i.e. mypy , pyright/pylance ) it is a pretty robust statically typed
language. Most third party libraries either are typed or have types stubs
distributed separately. From a developer perspective, this version of the
language making use of full strict static typing is essentially a different
language the Python of yesteryear. I won't fully make the argument here but
treating Python like a statically typed language greatly increases readability
and expressiveness of the code, greatly reduces bugs and even the amount of
tests you need to write, and makes it much easier to move on to a "real"
statically typed language like Rust (do it, its great!) or Go (do it, its
pretty good!) or even oldies like C/C++/Java. While there is good material on
more advanced typing concepts in Python (the mypy docs are actually quite
good), most tutorials you find online are "hello world" level examples just
introducing types to people who have never used Python or are from the dark
days of Python pre-3.6. This post assumes that you have a decent grasp of
Python and already know the basics of type annotations, or that you are coming
from another statically typed language and just want to see if Python is up to
snuff. Ok, rant over, lets get into some semi-advanced typing goodness. All of
the examples in this post are available here if you want to play around with
them. I would suggest using vscode and the "strict" Pylance type checking
mode. Type annotations and the interface segregation principle Remember the
interface segregation principle from Uncle Bob's SOLID principles . Well if
you don't, it basically means that users of your code should not be forced to
rely on interfaces/types/classes that they don't need. We will see how Python
type annotations can help with this. Some general good practices First lets
look at how we can make our code more robust by defining loose types for input
parameters and strict types for outputs. Lets say we are writing a function to
filter out a certain word or phrase from some strings. You could write your
function like this from typing import List def filter_things ( things : List [
str ], filter_string : str ) -> List [ str ]: return [ t for t in things if
filter_string not in t ] This is fine and would work like this things = [
"badger" , "snake" , "mushroom" ] filtered_things = filter_things ( things ,
"mushroom" ) # returns ["badger", "snake"] But do we really need the inputs
things to be a list, all we are doing is iterating over it. Lots of python
types do that. We could guess and do something like from typing import Any ,
Dict , List , Set , Tuple def filter_things ( things : List [ str ] | Tuple [
str ,...] | Set [ str ] | Dict [ str , Any ], filter_string : str ) -> List [
str ]: return [ t for t in things if filter_string not in t ] but that would
be terrible and you have not just confused your user even more. Instead we can
use the type that specifies the bare minimum constraint for what we are doing.
Since we are just iterating over things we can do from typing import Iterable
, List def filter_things ( things : Iterable [ str ], filter_string : str ) ->
List [ str ]: return [ t for t in things if filter_string not in t ] Using the
Iterable type just states that things is a collection of strings that can be
iterated over. Now our function can work with lots of different input types
like tuples, sets, dicts, even generators since all of those types are
iterable. Note that we do return a concrete type of List[str] here. It is
always a good idea to hav the return type be as narrow as possible so the
behavior is deterministic no matter what kind of input you give the function.
To hammer this home, lets look at one more example. from typing import List
def get_fifth_element ( things : List [ str ]) -> str : if len ( things ) < 5
: raise ValueError ( f "Length of input must be >= 5, found len= { len (
things ) } " ) return things [ 4 ] Again, we don't really need a list here but
we aren't iterating. Instead we are checking the length and grabbing the value
at a given index. So this means that we must be able to get the size of the
input collection and we must be able to get the item in the collection at a
given index. If we look at the docs we can see that the correct type to use
here is Sequence since it implements the __len__ and __getitem__ methods which
allow for us to call len on it and to index into it. So we have from typing
import Sequence def get_fifth_element ( things : Sequence [ str ]) -> str : if
len ( things ) < 5 : raise ValueError ( f "Length of input must be >= 5, found
len= { len ( things ) } " ) return things [ 4 ] # good with a tuple elements =
( "earth" , "wind" , "water" , "fire" , "multipass" ) fifth_element =
get_fifth_element ( elements ) # or a list elements = [ "earth" , "wind" ,
"water" , "fire" , "multipass" ] fifth_element = get_fifth_element ( elements
) As we will see later, even this type is a bit too narrow still, but without
defining our own type this will usually be fine. Protocols I had a previous
post about Protocols but here we will go over them in the context of the
interface segregation principle and just give a bit more detail as to why they
are useful. So lets start with the age old example of defining an Animal
import abc class Animal ( abc . ABC ): @ abc . abstractmethod def make_sound (
self ) -> None : pass @ abc . abstractmethod def act ( self ) -> None : pass @
property @ abc . abstractmethod def num_legs ( self ) -> int : pass Note, I
used an ABC here. I could have used a protocol for this but I'm trying to fake
a 3rd party library that may already have a lot of different implementation of
a class using standard class hierarchy. Now lets implement some animals class
Dog ( Animal ): def make_sound ( self ) -> None : print ( "woof" ) def act (
self ) -> None : print ( "dog is walking" ) @ property def num_legs ( self )
-> int : return 4 class Fish ( Animal ): def make_sound ( self ) -> None :
print ( "blub" ) def act ( self ) -> None : print ( "fish is swimming" ) @
property def num_legs ( self ) -> int : return 0 and lets make a function so
we can hear some animal sounds from typing import Iterable def
get_animal_sounds ( animals : Iterable [ Animal ]) -> None : for animal in
animals : animal . make_sound () animals = [ Dog (), Fish ()]
get_animal_sounds ( animals ) # woof # blub So this function takes in an
Iterable (see we learned from the first part!) of Animal s. If we run this
function with these inputs then we will get woof and blub . Cool!, now we want
to know what a fox says so we implement a fox class Fox : def make_sound (
self ) -> None : print ( "redacted" ) Sweet, lets plug this in animals = [ Dog
(), Fish (), Fox ()] get_animal_sounds ( animals ) Uh oh we get an error from
our type checker like this Argument of type "list[Dog | Fish | Fox]" cannot be
assigned to parameter "animals" of type "Iterable[Animal]" in function
"get_animal_sounds" "list[Dog | Fish | Fox]" is incompatible with
"Iterable[Animal]" TypeVar "_T_co@Iterable" is covariant Type "Dog | Fish |
Fox" cannot be assigned to type "Animal" "Fox" is incompatible with "Animal"
PylancereportGeneralTypeIssues Ah, yes because we just implemented make_sound
for Fox and not the whole animal interface. Well thats dumb, I just want to
know what the fox says, why do I need to implement the whole "interface".
Protocols to the rescue from typing import Iterable , Protocol class
CanMakeSound ( Protocol ): def make_sound ( self ) -> None : ... def
get_animal_sounds ( animals : Iterable [ CanMakeSound ]) -> None : for animal
in animals : animal . make_sound () Now, we can find out what the fox says
since all three animals now implement the CanMakeSound Protocol. So of course
this is a silly example but there are many real life situation where our
functions, classes, etc only need to know about a few methods of the object
and don't need to know about all of them. If we use Protocols to only specify
what we need this means that a user can implement their own objects to just
satisfy this protocol. If you really are digging this, here is how we could
make our get_fifth_element function even more flexible by using a custom
protocol that only requires the bare minimum. from typing import Any ,
Protocol , TypeVar T = TypeVar ( "T" , covariant = True ) class MySequence (
Protocol [ T ]): def __getitem__ ( self , __idx : Any ) -> T : ... def __len__
( self ) -> int : ... def get_fifth_element ( things : MySequence [ str ]) ->
str : if len ( things ) < 5 : raise ValueError ( f "Length of input must be >=
5, found len= { len ( things ) } " ) return things [ 4 ] Generics with
Protocol Constraints In the examples above we used Protocols to make our
functions very general (you could even say generic!); however, we only used
them in the arguments and not in the return type. In general, it doesn't
really make sense to return a Protocol. Sometimes you need to use generics
with custom constraints. We can illustrate this by implementing the simple max
function that returns the maximum of two inputs. First lets just do it for int
s def max ( a : int , b : int ) -> int : if a < b : return b return a Ok,
simple but the python builtin max function works on lots of other types, like
float , str , and even lists and sequences. If we want a generic max function,
look at the above function and figure out what the minimum requirement is. If
we look, the only requirement is that a and b can be compared using the "less
than" operator. In Python, that means that they both must implement the __lt__
magic method. So lets make a bounded generic and re-write our function from
typing import Protocol , TypeVar from typing_extensions import Self class
HasLessThan ( Protocol ): def __lt__ ( self , __other : Self ) -> bool : ... T
= TypeVar ( "T" , bound = HasLessThan ) def max ( a : T , b : T ) -> T : if a
< b : return b return a Now if we use this function we will get the right
return types and it will allow for many different comparisons m = max ( 3 , 4
) # m is of type: int m = max ( "hello" , "world" ) # m is of type: string m =
max ([ 4 , 5 , 6 ], [ 1 , 2 ]) # m is of type: List[int] As always, we could
even define our own type. Lets define a line class where we want max to return
the longer line object from dataclasses import dataclass from
typing_extensions import Self @ dataclass class Line : x_min : float x_max :
float def __lt__ ( self , other : Self ) -> bool : """Compare based on line
length""" return ( self . x_max \- self . x_min ) < ( other . x_max \- other .
x_min ) m = max ( Line ( 0 , 3 ), Line ( 0 , 5 )) # returns Line(0, 5) This is
a very powerful method for defining generic functions/classes that only
specify the bare minimum type constraints. If you are coming from Rust this is
similar to generics with trait bounds . Parameter Specification Variables As
Python is a gradually typed language, it cannot or could not always have
static typing in all situations. Until Parameter Specification Variables or
just "ParamSpec" we could not get good static typing on functions that are
decorated. Once a function was decorated we lost the type information about
the input and output parameters of that function. Lets see how we can fix that
now with ParamSpec import time from typing import Callable , ParamSpec ,
TypeVar T = TypeVar ( "T" ) P = ParamSpec ( "P" ) def timer ( func : Callable
[ P , T ]) -> Callable [ P , T ]: def inner ( * args : P . args , ** kwargs :
P . kwargs ) -> T : tstart = time . perf_counter () out = func ( * args , **
kwargs ) print ( f "function: { func . __name__ } ran in { ( time .
perf_counter () \- tstart ) } seconds" ) return out return inner @ timer def
add ( a : float , b : float ) -> float : """ Add two numbers """ return a \+ b
out = add ( 3.4 , 4.5 ) # function: add ran in 6.902000677655451e-06 seconds
Here we have created a timer decorator that simply prints the time it took to
call the decorated function. When we add the decorator to the simple add
function it will print out the runtime. This is a simple example but lets look
at what is going on. Notice that the outer function of the decorator takes in
the decorated function func and we use a type annotation of Callable[P, T] . T
is just a generic variable stating that the function could return any type.
The P variable is the ParamSpec . This is what allows to keep that type
information in the decorated function. In the inner function we use *args and
**kwargs as you always would for unknown inputs but now we can annotate them
with P.args and P.kwargs , respectively. And this is the magic that respects
the type information of the decorated function. Your code editor (I use vscode
and it is amazing for this and really all things...) should show you the
function definition when you hover over it and the type checker should respect
the input and output types. I used this feature quite heavily in a previous
post about implementing a fully typed lru-cache . Overloads Overloads let you
specify different combinations of inputs and/or outputs for a single function.
This can lead to some really bloated functions that do way too much but, like
most things, everything in moderation. Broadly speaking there are two cases
where you would want to consider using overloads Optional Second order
decorators (i.e. decorators that may or may not take arguments) Functions
where there are several different combinations of input parameters which may
or may not lead to different output types. Note, overloads should only be used
here where the job can't be done with generics. First lets look at the second-
order decorator case by modifying our example above. What if we want the
ability to format our message that prints the runtime, or what if we want to
use a logger instead of just printing, but we still want to have the default
behavior without passing in any arguments. We can modify it as follows import
time from typing import Callable , Optional , ParamSpec , TypeVar , overload T
= TypeVar ( "T" ) P = ParamSpec ( "P" ) def _default_display_fn (
function_name : str , call_time : float ) -> None : print ( f "function: {
function_name } ran in { call_time } seconds" ) @ overload def timer ( __func
: Callable [ P , T ]) -> Callable [ P , T ]: ... @ overload def timer ( * ,
display_fn : Callable [[ str , float ], None ]) -> Callable [[ Callable [ P ,
T ]], Callable [ P , T ]]: ... def timer ( __func : Optional [ Callable [ P ,
T ]] = None , * , display_fn : Optional [ Callable [[ str , float ], None ]] =
None , ) -> Callable [[ Callable [ P , T ]], Callable [ P , T ]] | Callable [
P , T ]: display_fn = display_fn or _default_display_fn def decorator ( func :
Callable [ P , T ]) -> Callable [ P , T ]: def inner ( * args : P . args , **
kwargs : P . kwargs ) -> T : tstart = time . perf_counter () out = func ( *
args , ** kwargs ) display_fn ( func . __name__ , time . perf_counter () \-
tstart ) return out return inner if __func is not None : return decorator (
__func ) return decorator There is a lot of detail here but the main point is
the two overload decorators. These specify the two signatures of the timer
decorator. The first is a simple decorator just like we had before where you
can use it with no parentheses. The second case is our second-order decorator
which we can pass in a function to display the run time. We could use it as
follows import logging def logging_display_fn ( function_name : str ,
call_time : float ) -> None : logging . info ( f "function: { function_name }
ran in { call_time } seconds" ) @ timer ( display_fn = logging_display_fn )
def add ( a : float , b : float ) -> float : """ Add two numbers """ return a
\+ b Now instead of using the default print we will use a logger. In both
cases the type signature of the original add function are preserved. Just by
looking at the complexity of the actual timer implementation we can start to
see why overload can be dangerous since the function can get very complicated
and it may be better to split into separate functions. Nonetheless using
overload as a way to have both first- and second-order decorators is generally
a decent idea. To demonstrate the second case for overloads lets extend the
functionality of our max function so that a user can pass in types that don't
natively have support for < , but we allow them to pass in a key function to
determine how to compare values of a and b : from typing import Callable ,
Protocol , TypeVar , overload from typing_extensions import Self class
HasLessThan ( Protocol ): def __lt__ ( self , __other : Self ) -> bool : ... T
= TypeVar ( "T" , bound = HasLessThan ) S = TypeVar ( "S" ) def _default_key (
__val ): return __val @ overload def max ( a : T , b : T ) -> T : ... @
overload def max ( a : S , b : S , * , key : Callable [[ S ], T ]) -> S : ...
def max ( a , b , * , key = None ): key_func = key or _default_key if key_func
( a ) < key_func ( b ): return b return a In the example above we have two
overloads. The first overload is exactly what we had before where the inputs
must support the < operator. The second overload now allows a user to pass in
any type but they are required to pass in a key function that returns the
value that will be used to compare the two inputs. For example, lets return to
our custom line type but this time, we won't implement the __lt__ method but
instead use a key function in max from dataclasses import dataclass @
dataclass class Line : x_min : float x_max : float m = max ( Line ( 0 , 3 ),
Line ( 0 , 5 )) # this will fail the type check m = max ( Line ( 0 , 3 ), Line
( 0 , 5 ), key = lambda line : ( line . x_max \- line . x_min )) # returns
Line(0, 5) So here our overloads let us be more flexible in our types. Note,
that in this case I did not include types in the actual implementation. It is
up to you whether or not you do this since the actual implementation signature
will not be exposed to the end user, only the overloads. The max function in
python actually has several more overloads if you want to check it out.
Wrapping Up In this post we have covered some more advanced use cases for type
annotations. The TLDR version is Use general input types only specifying the
bare minimum of what is needed Use strict concrete output types Use custom
Protocols define the minimum interface needed to functions/methods/classes Use
custom Protocols as bounds on generic types to make code more general Use
Parameter Specification (i.e. ParamSpec ) when making decorators Use overloads
sparingly when generics cannot do the job. Prefer overloads over usage of
Union types, especially in return types As I mentioned in the introduction,
none of this is required for valid Python but making use of the type
annotation system will make your code more readable and will make it much
nicer to work with. Share on Twitter Facebook Google+ LinkedIn Leave a Comment
You May Also Enjoy A fun multi-lingual concurrency experiment 4 minute read
The other day I was writing some code for a CI pipeline that needed to fetch
release notes from several different git repos and collate them into one large
s... Production Dockerfile Tips and Tricks 6 minute read Container images are
the backbone of a lot of modern workflows, especially in the age of Kubernetes
and friends. Building images that are small, secure, and ... Early and late
binding closures in Python 3 minute read So it happened again, late-binding
closures bit me. I once again had to discover that this was a thing so I
decided to make a little post about it just in ca... Access Modifiers in
Python 9 minute read If you have stumbled across this post then you are
probably coming to Python from another object oriented language that has real
support for access modifiers... Please enable JavaScript to view the comments
powered by Disqus.

***URL: https://jellis18.github.io/post/2021-11-25-lru-cache***

Building a fully typed LRU Cache in Python - Justin A. Ellis Justin Ellis
Principal Engineer, Infinia ML. Former Astrophysicist, former Data Scientist.
Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML In this post we are
going to build a fully typed LRU (least recently used) cache (almost) from
scratch using Python. We will then create a function decorator that mirrors
the builtin functools implementation . This exercise will cover several
advanced python concepts including generic types and other advanced typing
concepts, decorators (including second order decorators), and magic methods .
{: .notice--info} This exercise is for learning and demonstration purposes and
thus is not optimized for performance. All source code can be found here What
we will end up with What we will have at the end of this exercise is a clone
(at least in terms of the public interface) of the functools lru_cache
decorator. We should get these results: @ lru_cache ( maxsize = 100 ) def fib
( n : int ) -> int : if n < 2 : return n return fib ( n \- 1 ) \+ fib ( n \- 2
) >>> [ fib ( n ) for n in range ( 16 )] [ 0 , 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21
, 34 , 55 , 89 , 144 , 233 , 377 , 610 ] >>> fib . cache_info () CacheInfo (
hits = 28 , misses = 16 , maxsize = 100 , currsize = 16 ) In addition to this
we also want to retain all type information from the decorated function. In
older versions of functools.lru_cache this information is lost so static type
checkers and intellisense could not function correctly. In our version we will
retain this type information so that static type checkers will work correctly.
Building the cache class First, we will build a standalone LruCache class to
handle that actual heavy work. In most implementations of LRU cache, a hash
map (i.e. dictionary) and a doubly linked list are used . In this case, since
the main point of this article is how to use some of the more advanced python
features we will use one single built in data type, the OrderedDict The full
implementation is below: from collections import OrderedDict from typing
import Generic , Hashable , Optional , TypeVar T = TypeVar ( "T" ) class
LruCache ( Generic [ T ]): def __init__ ( self , capacity : int ): self .
capacity = capacity self . __cache : OrderedDict [ Hashable , T ] =
OrderedDict () def get ( self , key : Hashable ) -> Optional [ T ]: if key not
in self . __cache : return None self . __cache . move_to_end ( key ) return
self . __cache [ key ] def insert ( self , key : Hashable , value : T ) ->
None : if len ( self . __cache ) == self . capacity : self . __cache . popitem
( last = False ) self . __cache [ key ] = value self . __cache . move_to_end (
key ) def __len__ ( self ) -> int : return len ( self . __cache ) def clear (
self ) -> None : self . __cache . clear () Ok, there is a lot to unpack here.
First lets look at the basic functionality. # build a new cache with capacity
2 (i.e. we can store at most two values at a time) >>> cache = LruCache [ str
]( capacity = 2 ) # put two values in to the cache # cache keys will now be
[1, 2] >>> cache . insert ( 1 , "hello" ) >>> cache . insert ( 2 , "hello
again" ) # length is 2 since that is the number of values in the cache >>>
print ( len ( cache )) 2 # can retreive 1 since both 1 and 2 are in the cache
# cache keys are now [2, 1] since 2 was called more recently than 1 >>> print
( cache . get ( 2 )) hello again # this will evict key 1 with value "hello" #
cache keys are now [3, 2] >>> cache . insert ( 3 , "goodbye" ) # we now return
None since 1 is no longer in the cache >>> print ( cache . get ( 1 )) None #
but we can get the cached value for 3 # cache keys are still [3, 2] since 3
was called more recently than 2 >>> print ( cache . get ( 3 )) goodbye # cache
keys are now [2, 3] >>> print ( cache . get ( 2 )) hello again # this will
drop key 3 # cache keys are now [1, 2] >>> cache . insert ( 1 , "I'm back!" )
# get None since 3 was dropped >>> print ( cache . get ( 3 )) None # finally
we can clear the cache >>> cache . clear () >>> print ( len ( cache )) 0 We
can see that the capacity determines how large the cache is and every call to
cache.get or cache.insert puts that key at the top of the queue essentially
keeping track of the most recently used keys and when we insert a value when
the cache is full (i.e. the length is equal to the capacity) the least
recently used value is dropped. This is what we want. A few important things
to notice about the code, first is that LruCache itself is generic over a type
T which represents the value type of the cache. In the example above we
specify LruCache[str] to indicate the type of value in the insert method and
the return type in the get method is a string. We can pass in any type here
and our IDE and static type checkers are able to infer and check the input and
return types. Pretty awesome huh? The next thing to notice is the Hashable
type. Since we want this class to generic we need to give it the least amount
of information possible about what kinds of keys will be uses. At the very
least dictionary keys need to be hashable (i.e. implements the __hash__
method), and that is exactly what Hashable checks for. If we try to pass in a
key that is not hashable like a list then we will get an error from a static
type checker like pyright Argument of type "list[int]" cannot be assigned to
parameter "key" of type "Hashable" in function "get" "list[int]" is
incompatible with protocol "Hashable" "__hash__" is an incompatible type
Another thing to notice here is the use of the magic method __len__ . This is
a fairly common one but by implementing this method it allows us to call len
on an instance of LruCache . One of ther bit of fanciness here is the use of
the double leading underscore on __cache . We don't want to allow outside
users to be able to modify the cache, and while there are no truly private
variable in python by using the double underscore you will actually get an
AttributeError if you try to access it outside of the class. As for
implementation, as mentioned above, we make use of the OrderedDict provided by
the collections module. For the most part, OrderedDict is almost obsolete
since regular python dictionaries now retain insertion order; however, it is
very useful in this case because of the move_to_end method which lets us move
any key-value pair to the end of the dictionary which is how we keep track of
the lest/most recently used keys in our lru cache. Finally, the popitem method
allows for last=False which drops the key-value pair in a FIFO (first-in-
first-out) manner as oppsed to the default LIFO order (last-in-first-out)
order. This lets us easliy pop off the least recently used key. Building the
function wrapper class Now that we have a generic LruCache implementation we
move on to building a class that will wrap our function that we wish to cache.
What we want is something that has the exact same call signature as our
original function but also has a cache_info and cache_clear method as well as
a __wrapped__ attribute containing a refrerence to the wrapped function
itself. This all mirrors the functools.lru_cache API. from collections.abc
import Callable from typing import Final , Generic , NamedTuple , ParamSpec ,
TypeVar T = TypeVar ( "T" ) P = ParamSpec ( "P" ) class CacheInfo ( NamedTuple
): hits : int misses : int maxsize : int currsize : int class
_LruCacheFunctionWrapper ( Generic [ P , T ]): def __init__ ( self , func :
Callable [ P , T ], maxsize : int ): self . __wrapped__ = func self . __cache
= LruCache [ T ]( capacity = maxsize ) self . __hits = 0 self . __misses = 0
self . __maxsize : Final = maxsize def __call__ ( self , * args : P . args ,
** kwargs : P . kwargs ) -> T : call_args = args \+ tuple ( kwargs . items ())
ret = self . __cache . get ( call_args ) if ret is None : self . __misses += 1
ret = self . __wrapped__ ( * args , ** kwargs ) self . __cache . insert (
call_args , ret ) else : self . __hits += 1 return ret def cache_info ( self )
-> CacheInfo : return CacheInfo ( hits = self . __hits , misses = self .
__misses , currsize = len ( self . __cache ), maxsize = self . __maxsize , )
def cache_clear ( self ) -> None : self . __cache . clear () self . __hits = 0
self . __misses = 0 Ok, lets walk through this. First, we define the CacheInfo
named tuple to match the signature of the functools.lru_cache implementation.
NamedTuple s are good for things like this where you want an immutable but
easy to read/use return type of some function or method. If you need to add
methods or want to modify fields its probably better to use a dataclass . The
next thing to notice is that _LruCacheFunctionWrapper (leading underscore is
used here to indicate that this is a "private" class) is generic of types P
and T which as we see from the __init__ corresponds to the arguments and
return type of the wrapped function, respectively. For the P generic we make
use of the new (in python 3.10) ParamSpec which will make things much nicer as
we will see. From python 3.8 up you can import this from typing_extensions but
it is moved into the builtin typing module in python 3.10. We also use private
attributes (leading underscore) again here because we really don't want these
attributes to be used outside of this class. Note that __wrapped__ is still
accessible since it is a "dunder" attribute. We use the LruCache specifying
the type using the generic variable T as our cache. In this class we make use
of another quite popular magic method __call__ . The call magic method allows
us to call an instance of this class like a function. Furthermore, we have
used the power of generics and the new ParamSpec to actually indicate what the
argument names and types are via the P.args and P.kwargs methods as well as
the return type T . If we were to use this to wrap our function above then
static type checkers and intellisense will know the correct types and will
actually raise errors if the wrong type is passed in, just like it would on
the original function. >>> wrapped = _LruCacheFunctionWrapper ( fib , maxsize
= 4 ) >>> x = wrapped ( 3 ) # type signature (int) -> int This call function
itself is pretty simple since most of the work is handled in LruCache . We
turn the arguments and keyword arguments into a tuple (i.e. something
hashable) and use that as the key in our LRU cache. This does assume that the
input arguments and keyword arguments in our original function are hashable.
Unfortunately, as far as I know, there is no way to bound a ParamSpec so that
it would check if the arguments are hashable. Maybe one day... Nonetheless, we
create the call_args tuple and try to get the function return value from the
LRU cache. If we get a non None return value then we increment the hits (hit
the cache) counter and return the function value. If we don't have the value
in the cache we call our wrapped function and insert the value into the cache
and increment the misses counter (missed the cache). The cache_info method
simply returns a CacheInfo named tuple with the current state of the function
cache. Lastly the clear_cache method just clears the LRU cache and resets the
counters. One last thing to note here before moving onto the final step is
that this way of hashing the function arguments if far from optimal in that it
assumes that positional arguments are not called as keyword arguments and that
keyword arguments are called in the same order. It also does not take into
account default keyword arguments. These things are all fixable through the
use of something like a frozen set and using the inspect module. Maybe in a
follow on post... Building the decorator Ok, we are almost there. We have our
generic LruCache and a function wrapper that uses that cache. Now we just need
our function decorator. In this case, since our decorator takes an input
argument maxsize what we are really constructing is a decorator factory or
second order decorator. from collections.abc import Callable from typing
import ParamSpec , TypeVar T = TypeVar ( "T" ) P = ParamSpec ( "P" ) def
lru_cache ( maxsize : int ) -> Callable [[ Callable [ P , T ]],
_LruCacheFunctionWrapper [ P , T ]]: def decorator ( func : Callable [ P , T
]) -> _LruCacheFunctionWrapper [ P , T ]: return _LruCacheFunctionWrapper (
func , maxsize ) return decorator Without the type annotations this if just a
function that returns another function which wraps our original function.
However, through the use of the generic types, as we have done above we can
retain and pass forward all of the function argument and return types. The
outer decorator lru_cache returns a callable that takes in a generic callable
with arguments P and return type T and returns a generic instance of
_LruCacheFunctionWrapper that we have defined above. The inner decorator takes
in the actual function to be cached and returns an instance of
_LruCacheFunctionWrapper . This may look somewhat messy but the generic types
are quite magical and really will help make your code more safe and "correct".
But thats it, we now have created a decorator that will cache function calls
with a maxsize of maxsize ! Feel free to go back to the beginning and use this
on our fibonacci sequence function and play around in an IDE to see the full
power of types. Summary And there we have it, we have implemented a fully
typed version of lru_cache through the heavy use of generics and OrderedDict .
We have also made things pythonic by using magic methods like __init__ and
__call__ and followed some best practices by separating out the caching
implementation from the actual function wrapper. Share on Twitter Facebook
Google+ LinkedIn Leave a Comment You May Also Enjoy A fun multi-lingual
concurrency experiment 4 minute read The other day I was writing some code for
a CI pipeline that needed to fetch release notes from several different git
repos and collate them into one large s... Production Dockerfile Tips and
Tricks 6 minute read Container images are the backbone of a lot of modern
workflows, especially in the age of Kubernetes and friends. Building images
that are small, secure, and ... Some Advanced Typing Concepts in Python 13
minute read To start off, let me admit that yes, Python is a dynamically
(gradually?) typed language. However, with modern Python (3.6+, and really
with 3.10+) and stati... Early and late binding closures in Python 3 minute
read So it happened again, late-binding closures bit me. I once again had to
discover that this was a thing so I decided to make a little post about it
just in ca... Please enable JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/2022-01-15-access-modifiers-python/***

Access Modifiers in Python - Justin A. Ellis Justin Ellis Principal Engineer,
Infinia ML. Former Astrophysicist, former Data Scientist. Follow Durham, NC
Email Twitter LinkedIn GitHub Infinia ML If you have stumbled across this post
then you are probably coming to Python from another object oriented language
that has real support for access modifiers. Python has almost no support for
access modifiers at runtime but it does now have pretty good support for
access modifiers at "compile time" through linters and type checkers. In this
short post we will cover the main ways that one can mimic access modifiers in
Python. We will cover private and protected attributes and methods, readonly
attributes, and final methods and variables. For this post, the code is
checked with the great Pylance based language server in vscode. So other IDEs
or static checkers/linters may not give the same warnings/errors. I also make
use of the phrase "compile time", which just means that at the time that
linters or type checkers are run. Private vs Protected I have been programming
in Python for almost 15 years now and I only recently discovered that the
"underscore" (i.e. naming a variable or method _name instead of name )
convention actually represents a protected value and not a private value.
Furthermore, I discovered that Python actually does have runtime support for
private variables through a double underscore (i.e. __name ). It will actually
raise an error! As a refresher, private means that the attribute/method can
only be used inside the class where it is defined. Protected means that the
attribute/method can only be used in the class where it is defined or its
subclasses. Ok, so how do we use this? What does it actually do? And why would
we do it? Lets look at a convoluted, but illuminating example: class Thing :
def __init__ ( self , public : str , * , protected : str = "protected" ,
private : str = "private" ): self . public = public self . _protected =
protected self . __private = private def info ( self ) -> None : print ( ( f
"This class has public attribute: { self . public } , " f "protected
attribute: { self . _protected } , " f "and private attribute: { self .
__private } " ) ) So here we have a simple Thing class with three attributes.
One is public, one is protected, and one is private. We can see that we denote
"protected" with a single underscore before the variable name and "private"
with a double underscore before the variable name. The first thing to note is
that if you define a private attribute you should use it in the class since
that is the only place it can be used. If you do not you will probably get a
warning such as "__private is not accessed". Ok, lets use this thing! >>>
thing = Thing ( "public" ) # this is fine because it is assessing the
variables internally in the info method >>> thing . info () 'This class has
public attribute: public, protected attribute: protected, and private
attribute: private' # this is also fine because the public attribute is indeed
public >>> print ( thing . public ) 'public' # this will run but will give an
error when checked with pylance >>> print ( thing . _protected ) '"_protected"
is protected and used outside of the class in which it is declared' # this
will not actually run and will raise an AttributeError but it will also give
an error when checked >>> print ( thing . __private ) '"__private" is private
and used outside of the class in which it is declared' So we see that we can
use both the __private and _protected attributes inside the info method and we
can use the public attribute anywhere. Furthermore, we cannot use the
_protected or __private attributes outside of the class. So far in this
example, there is really no difference between protected and private since we
only have the one class. Lets make a subclass class SomeThing ( Thing ): def
more_info ( self ) -> None : print ( f "This class has public attribute: {
self . public } , protected attribute: { self . _protected } " ) >>>
some_thing = SomeThing ( "public" ) # still can use the info method which uses
the private attribute internally >>> some_thing . info () 'This class has
public attribute: public, protected attribute: protected, and private
attribute: private' # can use the new more_info method that uses the public
and protected attribute >>> some_thing . more_info () 'This class has public
attribute: public, protected attribute: protected' We have made a subclass of
Thing and we have a simple method that can indeed use the _protected
attribute. However, if we try to add a new method that uses the private
attribute then bad things will happen class SomeThing ( Thing ): def more_info
( self ) -> None : print ( f "This class has public attribute: { self . public
} , protected attribute: { self . _protected } " ) def use_private ( self ) ->
None : print ( f "Private attribute is { self . __private } " ) >>> some_thing
= SomeThing ( "public" ) # this will raise an AttributeError and will also
give an error when checked >>> some_thing . use_private () '"__private" is
private and used outside of the class in which it is declared' So we cannot
use the __private attribute in the subclass, as expected. Ok, we have shown a
basic example of how to use protected and private attributes. The same rules
work for protected and private methods. But when should you use them? There
aren't any hard and fast rules for this but there are a few rules of thumb and
things to be aware of protected and private variables are part of a concept
known as information hiding which deals with hiding implementation details
from downstream users. private attributes/methods should be used in cases
where you don't want downstream users or developers to have access to that
attribute or method. This is good for hiding implementation details which may
be prone to change but will not affect downstream users. protected
attributes/methods should be used where developers can have access (through
subclassing) but not outside users. This is useful for defining methods to be
implemented by subclasses (through ABCs) which are then used in the parent
class through a code re-use mechanism. Check out my previous post for more
info on this use case. If you use protected attributes/methods and allow for
subclassing then these attributes/methods essentially become part of the
public API since other developers can have access to them in their subclasses.
This means that a change to the protected implementation in the parent class
will affect all subclasses. We will see later how we can protect against
unwanted subclassing with the @final decorator. Readonly attributes In the
example above the public attribute was read/write. What if we want a readonly
attribute? There are two ways to do this, one is enforced at runtime and the
other is enforced through type checking at "compile time". Lets modify our
example above to include a readonly attribute class Thing : def __init__ (
self , readonly : str ): self . __readonly = readonly @ property def readonly
( self ) -> str : return self . __readonly >>> thing = Thing ( "readonly" )
>>> print ( thing . readonly ) 'readonly' # this will raise an AttributeError
but will also raise an error when checking >>> thing . readonly = "Hello!"
'Cannot assign member "readonly" for type "Thing" Property "readonly" has no
defined setter' We have made use a @property which allows us to make readonly
a readonly attribute. This will raise both a runtime error and a "compile
time" error when trying to set this attribute. However, it does require the
use of a private attribute (could also use protected, making sure to pay
attention the caveats mentioned above) and the use of a property. There is a
shorter way by using the Final type from typing import Final class Thing : def
__init__ ( self , readonly : str = "readonly" ): self . readonly : Final =
readonly >>> thing = Thing ( "readonly" ) >>> print ( thing . readonly )
'readonly' # this will not raise a runtime error but will raise an error when
checking >>> thing . readonly = "Hello!" 'Cannot assign member "readonly" for
type "Thing" "readonly" is declared as Final and cannot be reassigned' In this
case we only have to annotate the readonly attribute with the Final type to
get the same behavior at "compile time", but we will not get the runtime error
(i.e. we can set this variable). So depending on how you use type checking
this method could more or less useful to you. Lastly, the Final type does not
only apply to attributes but can be used anywhere. It is very useful for
defining module level constants that should not be modified. Final classes and
methods In the last section, we saw how to use the Final type annotations to
make variables readonly, at least in a static type checking sense. In this
last section we will see how to make classes and methods more restricted in
terms of what can be subclasses or overridden. Lets go back to our Thing class
from typing import final @ final class Thing : def __init__ ( self , public :
str , * , protected : str = "protected" , private : str = "private" ): self .
public = public self . _protected = protected self . __private = private def
info ( self ) -> None : print ( ( f "This class has public attribute: { self .
public } , " f "protected attribute: { self . _protected } , " f "private
attribute: { self . __private } , " ) ) This is identical to our definition
above but we have added the @final decorator. If we not try to subclass this
class class SomeThing ( Thing ): pass then we will get an error (through
static type checking, not at runtime) "Base class "Thing" is marked final and
cannot be subclassed". We should also note that if we do mark a class a final
then there is no need to distinguish between protected and private variables
since both technically have the same meaning now since the class cannot be
subclassed. You can also use the @final decorator to control which methods are
allowed to be overridden: class Thing : def __init__ ( self , public : str , *
, protected : str = "protected" , private : str = "private" ): self . public =
public self . _protected = protected self . __private = private @ final def
info ( self ) -> None : print ( ( f "This class has public attribute: { self .
public } , " f "protected attribute: { self . _protected } , " f "private
attribute: { self . __private } , " ) ) When we sublcass and try to override
the info method class SomeThing ( Thing ): def info ( self ) -> None : print (
"Overriding info method" ) We would get the following error: "Method "info"
cannot override final method defined in class "Thing"". Both of these methods
can be useful when you really want to control what downstream users/developers
can do with your classes and methods. Conclusion In this post we have seen how
to make attributes/methods both private and protected and the caveats
associated with that. We have also seen how to use a @property decorator and
Final annotation to make an attribute or variable readonly. Finally (get it?!)
we have seen how to use the @final decorator to mark a class or a method as
final, thus barring it from being subclassed or overridden. Lastly, I'll offer
some advice. In the world of open source software we need think differently
about what is actually private, public, and protected. We don't want to
restrict access too much so that it makes developer's lives harder when trying
to extend your code, but we also don't want to create a large API surface with
a lot of unneeded public methods and attributes that have to be kept backwards
compatible. So now that you know how to use these features in Python, think
carefully about how you use them and remember "With great power comes great
responsibility". Share on Twitter Facebook Google+ LinkedIn Leave a Comment
You May Also Enjoy A fun multi-lingual concurrency experiment 4 minute read
The other day I was writing some code for a CI pipeline that needed to fetch
release notes from several different git repos and collate them into one large
s... Production Dockerfile Tips and Tricks 6 minute read Container images are
the backbone of a lot of modern workflows, especially in the age of Kubernetes
and friends. Building images that are small, secure, and ... Some Advanced
Typing Concepts in Python 13 minute read To start off, let me admit that yes,
Python is a dynamically (gradually?) typed language. However, with modern
Python (3.6+, and really with 3.10+) and stati... Early and late binding
closures in Python 3 minute read So it happened again, late-binding closures
bit me. I once again had to discover that this was a thing so I decided to
make a little post about it just in ca... Please enable JavaScript to view the
comments powered by Disqus.

***URL: https://jellis18.github.io/post/2022-01-11-abc-vs-protocol***

Abstract Base Classes and Protocols: What Are They? When To Use Them?? Lets
Find Out! - Justin A. Ellis Justin Ellis Principal Engineer, Infinia ML.
Former Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML In Python there are two similar, yet different,
concepts for defining something akin to an interface, or a contract describing
what methods and attributes a class will contain. These are Abstract Base
Classes (ABCs) and Protocols . Until the advent of type annotations , ABCs
were the way to go if you wanted to have any kind of validation on
class/instance methods or properties and isinstance checks. With type
annotations, ABCs became more relevant as a way to define an "interface" for a
given class and then use that as a type annotation. However, to use ABCs as an
interface we must rely on nominal subtyping and a strict class hierarchy (we
will explain this later but, in short, we will have to subclass the ABCs in
order to use it as an interface). With Protocols we can use structural
subtyping or "Duck typing" (i.e. the class only has to have the same methods
and attributes, no subclassing necessary). So when do we use ABCs and when do
we use Protocols? Before we dig into this, lets first get a basic
understanding of how each works. What are Abstract Base Classes Here I will
give a brief overview of ABCs, if you want a much more detailed explanation
see this great video by one of the creators of ABCs . In general there are two
use cases for ABCs, as a pure ABC that defines an "interface" and as a tool
for code re-use via the Framework Design Pattern or through Mixins. Pure ABCs
(ABC as Interface) The simplest way to use an ABC is as a pure ABC, for
example: from abc import ABC , abstractmethod class Animal ( ABC ): @
abstractmethod def walk ( self ) -> None : pass @ abstractmethod def speak (
self ) -> None : pass Here we have defined an ABC Animal with two methods:
walk and speak . Note that the way to do this is to subclass ABC and to
decorate the methods that must be implemented (i.e. part of the "interface")
with the @abstractmethod decorator. Now we can implement this "interface" to
create a Dog class Dog ( Animal ): def walk ( self ) -> None : print ( "This
is a dog walking" ) def speak ( self ) -> None : print ( "Woof!" ) This will
work fine but if we happened to forget to implement the speak method then we
would get this error on creation >>> dog = Dog () TypeError : Can 't
instantiate abstract class Dog with abstract method speak We can see that we
get an error because we haven't implemented the abstract method speak . This
ensures that all subclasses implement the correct "interface". ABCs as a tool
for code reuse Another, and probably more common, use case for ABCs is for
code reuse. Below is a slightly more realistic example of a base class for a
statistical or Machine Learning regression model from abc import ABC ,
abstractmethod from typing import List , TypeVar import numpy as np T =
TypeVar ( "T" , bound = "Model" ) class Model ( ABC ): def __init__ ( self ):
self . _is_fitted = False def fit ( self : T , data : np . ndarray , target :
np . ndarray ) -> T : fitted_model = self . _fit ( data , target ) self .
_is_fitted = True return fitted_model def predict ( self , data : np . ndarray
) -> List [ float ]: if not self . _is_fitted : raise ValueError ( f " { self
. __class__ . __name__ } must be fit before calling predict" ) return self .
_predict ( data ) @ property def is_fitted ( self ) -> bool : return self .
_is_fitted @ abstractmethod def _fit ( self : T , data : np . ndarray , target
: np . ndarray ) -> T : pass @ abstractmethod def _predict ( self , data : np
. ndarray ) -> List [ float ]: pass Ok, lets unpack this first. There are two
public methods, fit and predict . The fit method calls the private abstract
method _fit and then sets the private attribute _is_fitted . The predict
method checks if we have fit the model before trying to make predictions and
then calls the private abstract method _predict . Lastly the base class
defines a property is_fitted . We do this so that a user cannot set this value
explicitly and it will only get set when calling fit . A quick aside. Other
than the ABC there are a few other important things going on. The first is the
use of generic self in the definition of the fit method. Here we type self
with a generic variable T that is bound to the model class. We do this to
ensure that subclasses of Model will return the correct type when calling fit
. Secondly we use a private attributed _is_fitted internally and expose this
with the is_fitted property. This is not strictly necessary but it is good
practice to keep this variable readonly since we really only want this to be
True if we have actually successfully run fit . Ok, back to ABCs. In the
example above we have used this base class to implement some logic that will
be inherited by all of its children, namely the fit and the predict methods.
These methods delegate the actual work to the private abstract methods, _fit
and _predict , that the children must implement. Of course we could make this
a pure ABC and have all children implement fit and predict but it would be
quite tedious for all children to have to re-implement the is_fitted
validation in both of these methods. Furthermore, this is a simple example and
in real situation there could be much more complicated shared code in the base
class. Finally, we could have made the abstract methods public but in this
case and probably most cases like the above example we should keep them
private instead of polluting the class. A end consumer only needs to know that
the class has a fit and a predict method and a is_fitted (i.e. readonly)
property. For a good, real life example of this kind of pattern take a look at
Pytorch's Module . While this class does not actually use ABCs it uses the
same pattern where there is a lot of reusable code in the base Module class
and users only have to implement the forward method. Ok, now that we have some
understanding of how to use ABCs for code reuse. Lets implement a super simple
model class MeanRegressor ( Model ): def __init__ ( self ): super (). __init__
() self . _mean = None def _fit ( self , data : np . ndarray , target : np .
ndarray ) -> "MeanRegressor" : self . _mean = target . mean () return self def
_predict ( self , data : np . ndarray ) -> List [ float ]: return list ( np .
ones ( data . shape [ 0 ]) * self . _mean ) In this example we have
implemented a very simple regression model to return the mean of the target
for every sample. In this case the _fit method only sets a private state
variable which is the mean of the target. The _predict method returns a list
that is the length of the number of samples with the computed mean as the
value. Lets see how this works. >>> data = np . array ([ 1.0 , 2.0 , 3.0 , 4.0
]) >>> target = np . array ([ 2.0 , 3.0 , 5.0 , 10.0 ]) >>> mean_regressor =
MeanRegressor () # try to predict without fitting first >>> mean_regressor .
predict ( data ) ValueError : MeanRegressor must be fit before calling predict
# Check if fitted >>> mean_regressor . is_fitted False # Fit and predict >>>
preds = mean_regressor . fit ( data , target ). predict ( data ) [ 5.0 , 5.0 ,
5.0 , 5.0 ] # Check if fitted now >>> mean_regressor . is_fitted True As we
can see in the example above we are actually calling the methods and property
from the ABC which under the hood is calling our concrete implementations of
the private _fit and _predict methods. So we get the error checking and
automatic setting of the _is_fitted attributes for free. This way, users do
not need to worry about that when creating a new type of Model . Isn't that
fun? What are Protocols Protocols were introduced in PEP-544 as a way to
formally incorporate structural subtyping (or "duck" typing) into the python
type annotation system. There are two main, but related, use cases for
Protocols. First, they can be used as an interface for classes and functions
which can be used downstream in other classes or functions. Secondly, they can
be used to set bounds on generic types. Protocols as Interfaces Protocols
allow you to define an interface for a class or function that will be type
checked on usage rather than on creation. For example, we can make our Animal
ABC above a Protocol from typing import Protocol class Animal ( Protocol ):
def walk ( self ) -> None : ... def speak ( self ) -> None : ... Note that
this looks pretty similar to our ABC based Animal class above. We inherit from
typing.Protocol instead of abc.ABC and we don't need to add the
@abstractmethod decorators since Protocols are not meant to be "implemented"
but simply act as an interface in downstream tasks. Lastly, it is common
practice to use ... in the body of methods in a Protocol instead of pass as we
did in the ABC, although either will work in both places. Ok, lets now
implement a Dog class Dog : def walk ( self ) -> None : print ( "This is a dog
walking" ) def speak ( self ) -> None : print ( "Woof!" ) Note here that we
don't subclass animal, we simply have to implement the methods specified in
the Animal Protocol. We can then use this in a downstream task like: def
make_animal_speak ( animal : Animal ) -> None : animal . speak () >>> dog =
Dog () >>> make_animal_speak ( dog ) 'Woof!' Here static type checkers would
be happy because the dog instance does indeed implement the Animal Protocol
because it has the same structure but is not itself a child class of Animal .
Lets see how Protocols enforce the interface. Lets say we forget to implement
the speak method on our Dog class >>> dog = Dog () >>> make_animal_speak ( dog
) Argument of type "Dog" cannot be assigned to parameter "animal" of type
"Animal" in function "make_animal_speak" "Dog" is incompatible with protocol
"Animal" "speak" is not present In this case a static type checker (Pylance in
the example above) would raise an error and tell the user that Dog does obey
the Animal Protocol since it doesn't implement the speak method. Notice that
this is different than an ABC that will raise an error when creating the Dog
class, whereas Protocols will raise an error where they are used. Ok, lets get
a bit more tricky. What if we define the Dog class but change the signature of
speak a bit class Dog : def walk ( self ) -> None : print ( "This is a dog
walking" ) def speak ( self , name : str ) -> None : print ( f "Woof! My name
is { name } " ) When we try to use this in the function >>> dog = Dog () >>>
make_animal_speak ( dog ) Argument of type "Dog" cannot be assigned to
parameter "animal" of type "Animal" in function "make_animal_speak" "Dog" is
incompatible with protocol "Animal" "speak" is an incompatible type Type
"(name: str) -> None" cannot be assigned to type "() -> None" Keyword
parameter "name" is missing in destination So, once again we get an error but
this time it is because the speak method on Dog does not have the same
signature. Pretty cool huh? In the example function above, we don't actually
even need the walk method since it won't be used in the function. We can
narrow down the input type by defining a new Protocol class SupportsSpeak (
Protocol ): def speak ( self ) -> None : ... def make_animal_speak ( animal :
SupportsSpeak ) -> None : animal . speak () >>> dog = Dog () >>>
make_animal_speak ( dog ) 'Woof!' So we still have the same Dog class with a
walk and speak method but we define the new Protocol SupportsSpeak (the naming
is somewhat standard if you are defining an interface with one method) that
just defines the speak method. And everything still works and would indeed
work with any class that has the same speak method signature. This is a simple
example but this can be quite powerful in more complicated code. One last
thing to mention about using Protocols as interfaces is that it is possible to
make them useable at runtime via an isinstance check with the
runtime_checkable decorator. from typing import Protocol , runtime_checkable @
runtime_checkable class Animal ( Protocol ): def walk ( self ) -> None : ...
def speak ( self ) -> None : ... >>> dog = Dog () >>> isinstance ( dog ,
Animal ) True Protocols as Generic Type Bounds When defining a generic type
variable in python we can give it a bound which means that the generic type
must either be a child class of the bound if given a class bound or it must
implement the protocol if given a Protocol. If you were paying attention we
actually used a type bound in the Model ABC example above. In this case we
typed the self variable with the generic type T that was bound by Model
itself. This was done so that child classes of Model would return the correct
type for fit which indeed returns self . If that is not clear take a look at
the documentation . So that was an example of using a class bound. However, in
generic programming we usually want to make things as specific as possible.
The max function is a great example of this. The builtin max function can take
in several different types of input. We could use overload for every different
kind of input but that can be tedious. Instead we notice that a specific
implementation of max only requires that the input types define the __lt__
magic method (meaning we can do x < y ). We can type (a somewhat simplified)
max method as follows (for a much more detailed video, check out this ) from
typing import TypeVar , Protocol class SupportsLessThan ( Protocol ): def
__lt__ ( self , __other : Any ) -> bool : ... S = TypeVar ( "S" , bound =
SupportsLessThan ) def my_max ( x : S , y : S ) -> S : if x < y : return y
return x Lets break this down. First, we implement a Protocol that defines the
__lt__ method. We then create a generic type variable S that is bound by our
SupportsLessThan Protocol. This means that S can be any type as long as it
implements __lt__ . We then define the max function which takes in two
arguments x and y , which are both generic type S and returns the same type. A
lot of builtin types have a __lt__ method implemented so we can use this
function with integers or strings, for example >>> max_int = my_max ( 4 , 5 )
>>> max_str = my_max ( "hello" , "world!" ) In the example above the max_int
will return an int and max_str will return a string. If we pass in an object
that does not have __lt__ implemented then we will get an error. So ABC or
Protocol? Yes. You should use both as they are good at different things and
both have should their place in your toolbox. We have already seen above the
main use cases for ABCs and Protocols and how they work. Given those examples
here are some good overall suggestions and observations. Abstract Base Classes
Belong to their subclasses. An ABC is not usable by itself, it can only be
used by implementing a child class. So because of this, ABCs inherently belong
to their subclasses as part of a strict class hierarchy. ABCs are a good
mechanism for code reuse, especially for boilerplate code or logic that will
not change for any (or most) subclasses. The best strategy here is to have the
ABC (i.e. parent class) do most of the work and have the children implement
the specifics. Good for real time validation when creating an instance of a
child class. As we saw above, ABCs will raise an error on creation if the
child does not implement all abstract methods. Protocols Belong where they are
used. As we saw above, Protocols are not "implemented" but tell downstream
code (i.e. other functions or classes) what the structure of the input object
is expected to be. Also, we saw that we can define multiple protocols for the
same kind of object depending on what is needed. This means that Protocols
belong where they are used. Good for defining interfaces, especially for 3rd-
party libraries when we don't want to tightly couple our code to a specific
3rd party library. Good (really the only way) for specifying flexible generic
type bounds. This somewhat goes without saying but Protocols only are useful
if using type annotations and cannot be used in any other way (except for
runtime_checkable ). Ok, so we know that ABCs and Protocols are, we know what
they are good at. So when should we use them? The answer to that is somewhat
subjective and depends on your environment but here are some rules of thumb
Use ABCs if you want to reuse code. Inheritance is not always the best method
of code reuse but it can be quite useful. Use ABCs if you require strict class
hierarchy (as in you need to use method resolution order or you need to check
__subclasses__ ) in your application. Use ABCs if you will need several
implementations of a class with several methods. Use Protocols for strict type
annotations (i.e.only annotate the methods/attributes you need) Use Protocols
for generic bounds Use Protocols for abstract interfaces for 3rd party
libraries Well, thats it for this time. Now go forth into our bold almost
statically typed python future with confidence! Share on Twitter Facebook
Google+ LinkedIn Leave a Comment You May Also Enjoy A fun multi-lingual
concurrency experiment 4 minute read The other day I was writing some code for
a CI pipeline that needed to fetch release notes from several different git
repos and collate them into one large s... Production Dockerfile Tips and
Tricks 6 minute read Container images are the backbone of a lot of modern
workflows, especially in the age of Kubernetes and friends. Building images
that are small, secure, and ... Some Advanced Typing Concepts in Python 13
minute read To start off, let me admit that yes, Python is a dynamically
(gradually?) typed language. However, with modern Python (3.6+, and really
with 3.10+) and stati... Early and late binding closures in Python 3 minute
read So it happened again, late-binding closures bit me. I once again had to
discover that this was a thing so I decided to make a little post about it
just in ca... Please enable JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/2021-01-16-mcmc-part2/***

A Practical Guide to MCMC Part 2: Further Resources - Justin A. Ellis Justin
Ellis Principal Engineer, Infinia ML. Former Astrophysicist, former Data
Scientist. Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML If you
are coming here Part 1 then I'm sorry to tell you that this is not the fully
fleshed out post you were hoping for. If you are coming here directly, good
for you going straight to the "Part 2", but alas you may be disappointed. Part
1 was written when I was finishing up my postdoc career and was still fresh in
my MCMC techniques. Since then I got a new job in Data Science and haven't had
a lot of time to get back to blogging and have not done any MCMC since my
postdoc days. However, I have gotten a fair number of requests for more
information about advanced MCMC techniques and this post is my weak attempt to
disseminate some further resources for you, though they may be a bit more
academic than the last post. MCMC Resources Slides on some more advanced MCMC
techniques : This talk and the code in the next point are part of a summer-
school and also has some exercises if you want to try them out. Simple
Differential Evolution and Parallel Tempering Implementation : This contains
some basic classes (in outdated python 2) to do differential evolution and
parallel tempering. Can be useful to see how these work in practice instead of
just through equations. Brief yet technical intro to adaptive metropolis,
custom jump proposals, and parallel tempering : This is from an academic paper
which you are probably not interested in, but the appendix starting on page 17
has some useful information and more references if you are really interested.
Well thats all folks, hopefully this is some help. I will mention that there
are some great packages out there like emcee and pymc3 that may do a lot of
this for you although I haven't tried them out. Share on Twitter Facebook
Google+ LinkedIn Leave a Comment You May Also Enjoy A fun multi-lingual
concurrency experiment 4 minute read The other day I was writing some code for
a CI pipeline that needed to fetch release notes from several different git
repos and collate them into one large s... Production Dockerfile Tips and
Tricks 6 minute read Container images are the backbone of a lot of modern
workflows, especially in the age of Kubernetes and friends. Building images
that are small, secure, and ... Some Advanced Typing Concepts in Python 13
minute read To start off, let me admit that yes, Python is a dynamically
(gradually?) typed language. However, with modern Python (3.6+, and really
with 3.10+) and stati... Early and late binding closures in Python 3 minute
read So it happened again, late-binding closures bit me. I once again had to
discover that this was a thing so I decided to make a little post about it
just in ca... Please enable JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/2018-01-02-mcmc-part1/***

A Practical Guide to MCMC Part 1: MCMC Basics - Justin A. Ellis Justin Ellis
Principal Engineer, Infinia ML. Former Astrophysicist, former Data Scientist.
Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML Markov Chain Monte-
Carlo (MCMC) is an art, pure and simple. Throughout my career I have learned
several tricks and techniques from various "artists" of MCMC. In this guide I
hope to impart some of that knowledge to newcomers to MCMC while at the same
time learning/teaching about proper and pythonic code design. I also hope that
this will truly be a practical (i.e. little theoretical statistics knowledge
needed) guide to MCMC. By that, I mean that we will treat MCMC as a tool not
an area of statistical theory to study. To that end we will forego most of the
theoretical underpinnings of MCMCs and skip straight to implementation and
optimization. Overview In this post you will: Get a brief introduction to MCMC
techniques Understand and visualize the Metropolis-Hastings algorithm
Implement a Metropolis-Hastings MCMC sampler from scratch Learn about basic
MCMC diagnostics Run your MCMC and push its limits on various examples {:
.notice--info} Note : All material in this post is generated from a Jupyter
notebook that can be downloaded here . Introduction MCMCs are used to map out
and sample probability distribution or probability density functions. The
problem of performing probabilistic inference and fitting models to data are
ubiquitous in many areas of science, statistics, economics, and business. In
short, if you have a probabilistic model with unknown parameters, you will
need MCMC (or similar techniques) to obtain probability distributions of those
unknown parameters. Now, there is a huge ecosystem of MCMC variants:
Metropolis-Hastings sampling, Gibbs Sampling, ensemble sampling, parallel
tempering, adaptive MCMC, Hamiltonian Monte-Carlo, Reversible Jump MCMC, etc,
but don't panic yet; we will start with a basic Metropolis-Hastings sampler
and gradually build (in other posts) to more sophisticated techniques. All of
the techniques listed above can be quite useful in certain situations (and
sometimes extremely tricky to implement: I'm looking at you Reversible Jump!)
but I have found that simple Metropolis-Hastings with a bit of adaptation can
handle a large array of high dimensional complex problems. In this first post
we will build a simple Metropolis Hastings sampler. Before we get to the
specifics of this algorithm we need to lay down a few ground rules for MCMCs:
MCMCs stochastically explore the parameter space in such a way that the
histogram of their samples produces the target distribution. Markovian:
Evolution of the chain (i.e., collections of samples from one iteration to the
other) only depends on the current position and some transition probability
distribution (i.e., how we move from one point in parameter space to another).
This means that the chain has no memory and past samples cannot be used to
determine new positions in parameter space. The chain will converge to the
target distribution if the transition probability is: irreducible : From any
point in parameter space, we must be able to reach any other point in the
space in a finite number of steps. positive recurrent : For any point in
parameter space, the expected number of steps for the chain to return to that
point is finite. This means that the chain must be able to re-visit previously
explored areas of parameter space. aperiodic : The number of steps to return
to a given point must not be a multiple of some value $$k$$. This means that
the chain cannot get caught in cycles. Fear not, this is the most technical
thing that we will cover and all MCMC methods will have these rules built in!
Metropolis Hastings MCMC Suppose we have a target posterior distribution
$$\pi(x)$$, where $$x$$ here can be any collection of parameters (not a single
parameter). In order to move around this parameter space we must formulate
some proposal distribution $$q(x_{i+1}\mid x_i)$$, that specifies the
probability of moving to a point in parameter space, $$x_{i+1}$$ given that we
are currently at $$x_i$$. The Metropolis Hastings algorithm accepts a "jump"
to $$x_{i+1}$$ with the following probability $$ \kappa(x_{i+1}\mid x_i) =
\mathrm{min}\left(1, \frac{\pi(x_{i+1})q(x_i\mid
x_{i+1})}{\pi(x_{i})q(x_{i+1}\mid x_{i})}\right) = \mathrm{min}(1, H), $$
where the fraction above is called the Hastings ratio, $$H$$. What the above
expression means is that the probability of transitioning from point
$$x_{i+1}$$ given the current position $$x_i$$ is a function of the ratio of
the value of the posterior at the new point to the old point (i.e.,
$$\pi(x_{i+1})/\pi(x_i)$$) and the ratio of the transition probabilities at
the new point to the old point (i.e. $$q(x_i\mid x_{i+1})/q(x_{i+1}\mid
x_i)$$). Firstly, it is clear that if this ratio is $$>$$ 1 then the jump will
be accepted (i.e. the chain advances to $$x_{i+1}$$). Secondly, the ratio of
the target posteriors ensures that the chain will gradually move to high
probability regions. Lastly, the ratio of the transition probabilities ensures
that the chain is not influenced by "favored" locations in the proposal
distribution function. If this last point is confusing, never worry, we will
give an example of this later. For now, rejoice in the fact that many proposal
distributions are symmetric (i.e., $$q(x_{i+1}\mid x_i) = q(x_i\mid
x_{i+1})$$). The metropolis hastings algorithm is then: Initialize parameters
$$x_0$$. for $$i=1$$ to $$i=N$$: Generate proposed parameters: $$x_* \sim
q(x_*\mid x_i)$$ Sample from uniform distribution: $$u\sim U(0, 1)$$ Compute
Hastings ratio: $$H=\frac{\pi(x_{ })q(x_i\mid x_{ })}{\pi(x_{i})q(x_{*}\mid
x_{i})}$$ if $$u < \mathrm{min}(1,H)$$ then $$x_{i+1}=x_*$$ else $$x_{i+1} =
x_i$$ For each step in this loop, we draw a proposed parameters $$x_ $$. We
then compute the Hastings ratio using this proposed point. By drawing the
number $$u\sim U(0,1)$$ we allow for a "jump" to $$x_ $$ with probability
$$\mathrm{min}(1, H)$$. So if $$u < \mathrm{min}(1,H)$$ then the jump is
accepted then we advance the chain $$x_{i+1}=x_*$$, if it is not then we stay
at the current position $$x_i$$. If the proposal distribution $$q$$ obeys the
three rules above (irreducible, positive recurrent, and aperiodic) then this
algorithm is guaranteed to work. However, choosing smart proposal
distributions is what makes MCMC an art! Before we get into that, lets write
some code for a very simple but illustrative example. By the way, if you are
new to this then write this code yourself, don't just read it. Writing your
own simple MCMC code is the best way to learn and understand. Simple
Metropolis Hastings sampler The best way to learn is to do. So lets make a
simple metropolis hastings sampler. The code below implements the MH-sampler
through a function mh_sampler . This function is simple, but generic enough to
work with different kinds of posterior distributions and different kinds of
jump proposals def mh_sampler ( x0 , lnprob_fn , prop_fn , prop_fn_kwargs =
{}, iterations = 100000 ): """Simple metropolis hastings sampler. :param x0:
Initial array of parameters. :param lnprob_fn: Function to compute log-
posterior. :param prop_fn: Function to perform jumps. :param prop_fn_kwargs:
Keyword arguments for proposal function :param iterations: Number of
iterations to run sampler. Default=100000 :returns: (chain, acceptance,
lnprob) tuple of parameter chain , acceptance rate and log-posterior chain.
""" # number of dimensions ndim = len ( x0 ) # initialize chain, acceptance
rate and lnprob chain = np . zeros (( iterations , ndim )) lnprob = np . zeros
( iterations ) accept_rate = np . zeros ( iterations ) # first samples chain [
0 ] = x0 lnprob0 = lnprob_fn ( x0 ) lnprob [ 0 ] = lnprob0 # start loop
naccept = 0 for ii in range ( 1 , iterations ): # propose x_star , factor =
prop_fn ( x0 , ** prop_fn_kwargs ) # draw random uniform number u = np .
random . uniform ( 0 , 1 ) # compute hastings ratio lnprob_star = lnprob_fn (
x_star ) H = np . exp ( lnprob_star \- lnprob0 ) * factor # accept/reject step
(update acceptance counter) if u < H : x0 = x_star lnprob0 = lnprob_star
naccept += 1 # update chain chain [ ii ] = x0 lnprob [ ii ] = lnprob0
accept_rate [ ii ] = naccept / ii return chain , accept_rate , lnprob The
function follows the MH-algorithm exactly as written above. It will return the
chain samples (i.e. the columns are the parameters and the rows are the
sampler iterations), acceptance rate per iteration and log-posterior values
per iteration. Generally, this is all the information you need to produce
inferences and check convergence and efficiency. So, now we have a MH-sampler
function but we don't yet have a proposal distribution. The simplest proposal
distribution is just a draw from the prior distribution; however, this is
hugely inefficient, and since this is a practical guide lets do something
useful. Next to a prior draw, a Gaussian proposal with fixed standard
deviation is the simplest proposal. In fact, nearly all other sophisticated
proposal distributions are still a Gaussian distribution, albeit without a
constant standard deviation / covariance. We will cover these in the next post
but for now lets make a simple distribution. def gaussian_proposal ( x , sigma
= 0.1 ): """ Gaussian proposal distribution. Draw new parameters from Gaussian
distribution with mean at current position and standard deviation sigma. Since
the mean is the current position and the standard deviation is fixed. This
proposal is symmetric so the ratio of proposal densities is 1. :param x:
Parameter array :param sigma: Standard deviation of Gaussian distribution. Can
be scalar or vector of length(x) :returns: (new parameters, ratio of proposal
densities) """ # Draw x_star x_star = x \+ np . random . randn ( len ( x )) *
sigma # proposal ratio factor is 1 since jump is symmetric qxx = 1 return (
x_star , qxx ) This proposal is nearly as simple as it gets, mathematically it
is: $$ q(x_*\mid x_i) = \textrm{Normal}(x_i, \sigma^2), $$ that is, a Gaussian
centered on the current position $$x_i$$ with variance given by a standard
deviation parameter $$\sigma$$. Ok now we have our sampler and our proposal
distribution, lets test this out on a very simple 1-D Gaussian likelihood
function with an unknown mean and unit variance. We will use a uniform prior
on the mean $$\mu \sim U(-10, 10)$$. Of course, we don't actually need MCMC to
sampler this distribution but it works as a good example that is easy to
visualize. def simple_gaussian_lnpost ( x ): """ 1-D Gaussian distribution
with mean 0 std 1. Prior on mean is U(-10, 10) :param x: Array of parameters
:returns: log-posterior """ mu = 0 std = 1 if x < 10 and x > \- 10 : return \-
0.5 * ( x \- mu ) ** 2 / std ** 2 else : return \- 1e6 Visualizing the MH-step
Before we run a long chain, lets try to visualize what it going on at each
iteration. The plot above shows a schematic of a successful jump. The blue
shows the 1-D Gaussian posterior distribution, the orange is the Gaussian
proposal distribution, $$q(x_ \mid x_0)$$, ($$\sigma=0.5$$), and the green and
red points are the initial and proposed parameters, respectively. For
illustration purposes, the gray curve shows $$q(x_0\mid x_ )$$ to show the
symmetry of the proposal distribution. In this case we see that the proposed
point returns a Hastings ratio of 2.2 (i.e. transition probability of 1) and
therefore the jump will be accepted. In this plot, we show what happens when a
jump is proposed to a lower probability region. The transition probability is
equal to the Hastings ratio here (remember transition probability is
$$\mathrm{min}(1, H)$$) which is 0.32, which means that we will move to this
new point with 32% probability. This ability to move back down the posterior
distribution is what allows MCMC to sample the full probability distribution
instead of just finding the global maximum. As I promised above, lets give a
quick demonstration of what happens with a non-symmetric proposal
distribution. Lets suppose we have the same posterior but now we use a
proposal from a fixed Gaussian distribution (i.e. the mean does not change
with current position). In the above plot we show that the proposal
distribution is a fixed Gaussian $$q(x_*\mid x_0)\sim \textrm{Normal(-1,
1)}$$. Here we show that even though the proposed point is at a lower
posterior probability than the initial point, the Hastings ratio is still $$>
1$$ (will accept jump with 100% probability). Qualitatively this makes sense
because we need to weight that proposed point higher to take into account for
the fact that the proposed point is "hard" to get to even though it still has
a relatively high posterior probability value. In this last example, we show
the opposite effect. In this case, even though the posterior probabilities are
the same for the current and proposed points, the Hastings ratio is only 0.09
(i.e. 9% chance of accepting jump). Again, with some thought this makes sense.
The proposed point must be weighted down because it is near the peak of the
proposal distribution (i.e. lots of points will be proposed around this
position) and therefore is "easy" to get to even though the posterior
probability is no different than at the initial point. In many cases we do not
need to worry about the proposal densities as many proposal distributions are
symmetric but you always need to keep it in mind when constructing your own
jump proposals as it can lead to biased results if not correctly taken into
account. I will show how to easily check for the correct ratios later in this
series. Diagnosing efficiency and convergence Now that we understand the
Metropolis Hastings step. Lets actually run our sampler on the simple 1-D
gaussian likelihood. Since this is only a 1-D problem it is quite easy to
determine the optimal jump size but lets explore different values for
illustration purposes. The panels are (from top left to bottom right):
Histogram of samples, sample vs iteration (i.e. trace plot), log-posterior vs
iteration, and proposal acceptance rate vs iteration. In an optimally
performing MCMC, the histogram of samples should converge to the posterior
distribution, the trace of the chain should sample around the maximum of the
posterior such that the samples are close to i.i.d (independent, identical
distribution). The log-posterior chain should be smoothly varying around the
maximum. Lastly, the acceptance rate depends on the problem but typically for
1-d problems, the acceptance rate should be around 44% (around 23% for more
than 5 parameters). There are more sophisticated ways of diagnosing
convergence (it gets harder with many parameters) which we will get into in
further posts, but plots like this get you a long way to diagnosing problems
with your sampler. In this example we have used a jump standard deviation of
0.01 (remember the standard deviation of the posterior) is 1.0. So, it is
clear that the histogram of samples is very poorly reproducing the posterior,
the chain trace has large timescale (iteration scale) correlations (i.e. it
very slowly varies around the mean). We can also see that around the middle of
the run, the chain drifted off from the high probability region (bottom left
plot) and it took quite a large number of samples to get back because of the
small jump size. Lastly, we can see that the acceptance rate is 99%. Overall,
if you see something like this, the first step is to increase the jump
proposal size. Ok, so $$\sigma=0.01$$ was too small, what about
$$\sigma=500$$? Here we see the opposite of the first example. The jump size
here is way too big. While we did do a better job at recovering the posterior,
we can see from the choppiness of the trace plots and low acceptance rate that
this run is very inefficient as it spends a lot of time at fixed locations in
parameter space and rarely moves. If you see something like this, the first
thing to do is to decrease the jump proposal size. Finally, we have a good
jump proposal size. Actually this is the optimal step size for a gaussian
distribution $$\sigma_{\rm jump} = 2.38 \sigma_{\rm posterior} / n_{\rm
dim}$$. In this case we can see that the samples are near perfect draws from
the posterior distribution, the chain trace is nearly i.i.d and the acceptance
rate is 44% The above illustrates the point but it is so easy that it may be
hard to see why MCMCs are so important but difficult to implement efficiently.
Before we wrap up for this week, lets look at two more, slightly difficult
examples. A (slightly) more complicated problem We have tacked a 1-D Gaussian
posterior. Lets now look at a 10-D problem. This posterior distribution has
the following properties $$ \sigma^2 \sim \textrm{LogNormal}(0,1.5)\ \mu \sim
\textrm{Normal}(0, 10) $$ That is it is a gaussian with variance drawn from a
Log-Normal distribution and the means are drawn from a normal distribution
with 0 mean and variance 10. This way, we can have multiple scales present in
the posterior. class multi_gaussian_lnpost : """ N-Dimensional Gaussian
distribution with mu ~ Normal(0, 10) var ~ LogNormal(0, 1.5) Prior on mean is
U(-500, 500) :param ndim: Numver of dimensions to gaussian (default=10) :param
seed: Random number seed for reproducible results. """ def __init__ ( self ,
ndim = 10 , seed = 12345 ): np . random . seed ( seed ) self . var = 10 ** (
np . random . randn ( ndim ) * 1.5 ) self . mu = scipy . stats . norm ( loc =
0 , scale = 10 ). rvs ( ndim ) def __call__ ( self , x ): """ Call
multivariate normal posterior. :param x: Array of parameters :returns: log-
posterior """ if np . all ( x < 500 ) and np . all ( x > \- 500 ): return
scipy . stats . multivariate_normal ( mean = self . mu , cov = self . var ).
logpdf ( x ) else : return \- 1e6 These plots are the same as those for the
1-D problem but now we show the histogram and trace for all 10 mean
parameters. As before, the bottom plot shows the log-posterior vs iteration
and the acceptance rate vs iteration. We see that even if we have multiple
scales present, we can still do fairly well with our very simple jump proposal
with a single step size. However as we get to larger parameter spaces this
begins to pose a problem. However, if you see these kinds of results, it would
be best to try different jump sizes for different parameters. For example, I
will increase the jump size for parameters $$x_4$$, $$x_5$$, and $$x_9$$. OK,
not perfect, but better. As you can imagine, this can get quite tedious when
you have larger parameter spaces and more complicated posterior distributions.
In later posts we will discuss how to automate this tuning process via
adaptive jump proposals. To wrap out this week though lets move to a slightly
more realistic example: non-trivial covariance matrix and larger parameter
space. Covariant parameters in high dimensions This scenario is more
realistic. In many cases, parameters have some covariance with one another and
fancy techniques and tricks are really needed in high dimensions. Here we will
sample from a 50-dimensional gaussian distribution with non-trivial covariance
matrix. class corr_gaussian_lnpost : """ N-D Gaussian distribution with means
drawn from mu ~ Normal(0, 1) and non-trivial dense covariance matrix. Prior on
mean is U(-50, 50) :param ndim: Dimension of multivariate gaussian
(default=10) :param seed: Random number seed for reproducible distribution """
def __init__ ( self , ndim = 50 , seed = 12343 ): np . random . seed ( seed )
self . mu = np . random . rand ( ndim ) M = np . random . randn ( ndim , ndim
) self . cov = np . dot ( M , M . T ) def __call__ ( self , x ): """Return
multivariate-normal log posterior :param x: Array of parameters :returns: log-
posterior """ if np . all ( x < 50 ) and np . all ( x > \- 50 ): return scipy
. stats . multivariate_normal ( mean = self . mu , cov = self . cov ). logpdf
( x ) else : return \- 1e6 Above, we only plot the first 10 parameters but it
is obvious that, even after 1 million samples, our gaussian jump proposal is
not working very well at all as the chains have not converged. We will pick up
with this problem next week when we introduce adaptive jump proposals and jump
schedules. {: .notice--info} Update January 16, 2021: Unfortunately, a fully-
fledged Part 2 to this post never came about but you can check out some
further resources here . Share on Twitter Facebook Google+ LinkedIn Leave a
Comment You May Also Enjoy A fun multi-lingual concurrency experiment 4 minute
read The other day I was writing some code for a CI pipeline that needed to
fetch release notes from several different git repos and collate them into one
large s... Production Dockerfile Tips and Tricks 6 minute read Container
images are the backbone of a lot of modern workflows, especially in the age of
Kubernetes and friends. Building images that are small, secure, and ... Some
Advanced Typing Concepts in Python 13 minute read To start off, let me admit
that yes, Python is a dynamically (gradually?) typed language. However, with
modern Python (3.6+, and really with 3.10+) and stati... Early and late
binding closures in Python 3 minute read So it happened again, late-binding
closures bit me. I once again had to discover that this was a thing so I
decided to make a little post about it just in ca... Please enable JavaScript
to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/2018-01-17-mail-analysis/***

Personal Analytics Part 1: Gmail - Justin A. Ellis Justin Ellis Principal
Engineer, Infinia ML. Former Astrophysicist, former Data Scientist. Follow
Durham, NC Email Twitter LinkedIn GitHub Infinia ML Most of us are sending or
receiving tens to hundreds of emails daily. I have been using email regularly
since my undergrad days in 2008 so I thought it would be a neat project to
download my gmail data and analyze it to find out any interesting trends. This
post will detail the exploration of my Gmail account going back to 2018 but
the code should be applicable (with some minor modifications) to your emails
as well! Read in Data and explore Get mbox data here . Note that we only deal
with Gmail data here. If you can get your email into mbox format then this
should probably work as well but the tags may be different. # set raw data
mbox path mboxfile = 'gmail/Mail/All mail Including Spam and Trash.mbox' Use
mailbox.mbox to read in the mailbox data. This will essentially return a list
of dictionaries for each email. We will see what the dictionary contains in
the next step. # read in mbox mbox = mailbox . mbox ( mboxfile ) Lets loop
over the keys to see what Google keeps track of. If you are doing this on your
mailbox you can take a look at all the values as well, I just don't want to
accidentally post secure values on my blog!! for key in mbox [ 0 ]. keys ():
print ( key ) X-GM-THRID X-Gmail-Labels Delivered-To Received X-Google-Smtp-
Source X-Received ARC-Seal ARC-Message-Signature ARC-Authentication-Results
Return-Path Received Received-SPF Authentication-Results Date DKIM-Signature
From To Message-ID Subject Mime-Version Content-Type Content-Transfer-Encoding
X-Auto-Response-Suppress For the purposes of this analysis we really only want
to keep Subject , To , From , Date , and X-Gmail-Labels . The first four are
self explanatory and the last will tell us if the message was in "Sent" mail
or in my inbox. We will also keep X-GM-THIRD which is a variable that keeps
track of email threads. Convert to csv So to simplify our lives, lets make a
csv containing only those value for all of the messages. After this, we can
use pandas to read the csv into a dataframe. with open ( "mbox.csv" , "w" ) as
outfile : writer = csv . writer ( outfile ) for message in mbox : writer .
writerow ([ message [ 'subject' ], message [ 'from' ], message [ 'date' ],
message [ 'to' ], message [ 'X-Gmail-Labels' ], message [ 'X-GM-THRID' ]])
Read in the csv using read_csv and give the headers manually since mbox.csv
doesn't have any headers. We also convert the date to a datetime object. df =
pd . read_csv ( 'mbox.csv' , names = [ 'subject' , 'from' , 'date' , 'to' ,
'label' , 'thread' ]) df [ 'date' ] = df [ 'date' ]. apply ( lambda x : pd .
to_datetime ( x , errors = 'coerce' , utc = True )) df = df [ df [ 'date' ].
notna ()] Now lets print out some basic info. It looks like I have 72,728
emails from my gmail account (I'm actually disappointed I haven't collected
more over the 10 years that I've been using gmail.). We can also see that
there are some null values for each of the variables but we will deal with
those in turn. df . info ()  Int64Index: 72554 entries, 0 to 72727 Data
columns (total 6 columns): subject 72238 non-null object from 72554 non-null
object date 72554 non-null datetime64[ns, UTC] to 72337 non-null object label
63443 non-null object thread 72554 non-null int64 dtypes: datetime64[ns,
UTC](1), int64(1), object(4) memory usage: 3.9+ MB Now that we know the basic
metadata, lets look at the first 5 rows of our dataframe. df . head ()

***URL: https://jellis18.github.io/post/2021-01-03-from-academia-to-data-science/***

How to transition from Academia to Data Science - Justin A. Ellis Justin Ellis
Principal Engineer, Infinia ML. Former Astrophysicist, former Data Scientist.
Follow Durham, NC Email Twitter LinkedIn GitHub Infinia ML Oh boy, another one
of these blog posts about transitioning to Data Science from Academia. Well,
in this post I'll try to add a slightly different take on the usual advice
along with some more traditional advice. This is not a step-by-step guide
because I don't think such a thing exists in this case. Instead, this post
will be my opinionated views on what you can do to make a smooth transition
from Academia to Data Science. Also, this post is not about "how to get a Data
Science job". I won't talk about your resume, or prepping for interviews or
any of that stuff. This post is about how to become a Data Scientist. First of
all, I should note that while a lot of the things I cover in this post could
be generic and apply in many different situations, I will lay down a few
assumptions first. You are in a technical field already (Mathematics,
Statistics, Physics, Chemistry, Biology, etc). You have some programming
knowledge (any language is fine). Your intended Data Science career is applied
and not research oriented. All this will probably be more applicable if you
are in a PhD program but could still be relevant for a B.S. And with that out
of the way, let's get into it. What is Data Science anyway? If you have
stumbled across the blog, you probably have some idea of what Data Science is
or maybe you think you know what it is. Data science could mean that you
manage data bases and write SQL code all day. It could mean that you work with
"big data" and do things with Hadoop (is that still a thing, probably...),
Spark and other big data tools. 1 God forbid, it could mean that you do most
of your work in Excel making linear regression models. But in my experience
when most people say Data Science, what they really mean is "Machine Learning
Engineer" 2 , i.e. someone who develops and applies ML models to real data and
deploys them for use in real-world scenarios. Of course, there is more to it
than that, but that is the gist and this the kind of Data Scientist that I
will have in mind for the rest of this post. How has Academia already prepared
me for this role? I know when I was a graduate student I thought that I was
only prepared to do one thing with my life -- be a (astro)physics
professor/researcher (after an extended post-doc career). In my time
(2009-2014) in physics grad school, there was almost never any mention of any
other career trajectory. It was assumed that this was the path that everyone
would take, which basically meant there was one mode of operation: Write
Papers . 3 However, there are still a lot of things that you will learn in
your academic career (both undergraduate and graduate) that will be very
useful for your Data Science (ahem... Machine Learning Engineer) career.
Learning the struggle There are some skills that can not be taught. They can
only be learned through struggle. If you have done any kind of original
research in your academic field then you know the struggle. You have to learn
to figure things out for yourself because there is no "right answer" that you
can look up in the back of the book. If you have ever written any significant
code base then you also know the struggle. You have to figure out how to put
everything together and debug the inevitable bugs that will appear.
"https://jellis18.github.io/images/struggle.png" alt="Struggle"> Learning the
struggle. Sometimes completing a ML project seems like a Sisyphean task. In
Data Science, most of your existence is struggle and you need to be able to
feel comfortable not knowing what to do in a given moment. You need to be able
to experiment and possibly fail. In many cases a stakeholder (fancy language
for the person/entity that wants some ML product/functionality) does not know
much about ML, all they know is what they want an end product to look like.
Actually, in some cases they don't even know what they want the end product to
look like. It is more common that they have a lot of data (perfectly clean of
course) and they want “insights” from that data. You, as the Machine Learning
Engineer need to be prepared to struggle through finding a solution to their
problem. Even though the struggles you will go through in a ML based problem
may be different than those that you have had in academia, being comfortable
with being uncertain will prepare you well for a career as a machine learning
engineer. In the next section, I will offer some advice to get more familiar
with the ML-based struggle. Learning how to learn Very related to "the
struggle" is learning how to learn. Everyone learns in different ways and
throughout your academic career it is a good bet that you have discovered your
own method. In many cases, definitely in mine, you realize that you don't
really learn much from your classes. Sure you may do well on tests, but you
forget it all in a week. You learn by doing. All the theory is approximately
worthless without the practice. Would you trust a doctor that has spent years
reading up on the latest surgical techniques but never actually performed a
surgical procedure? Would you recruit a "guitarist" who knows all the musical
theory but has never actually played a guitar? Later I will offer some advice
in this area, but knowing how to learn on your own is one of the most
important skills in Data Science, or life for that matter. "Soft Skills"
Overall, soft skills are a combination of people skills and communication
skills. This one strongly depends on whether or now you have had to present
your work to both technical and non-technical audiences. It also depends if
you have been part of a large collaboration. With that being said, being able
to present technical information to non-technical audiences in a clear and
concise way is a skill that will help you land a good Data Science job. As I
mentioned above, I am focusing on an applied Data Science job, not a research
data scientist position. This means that you will need to be able to
communicate with the stakeholders as well as your teammates. If you have been
lucky enough to be part of a large collaboration with many different groups
that need to work together, then you will have gained invaluable skills that
will really help you in Data Science. Being able to translate between
different groups is a very useful skill. For example, when I was a grad
student I was part of a data analysis group and we used mostly Bayesian
techniques. There was another group that used mostly frequentist techniques.
On the surface these things may be very different, but there are some useful
translations between the two and I was able to speak both languages, which
made things move much more smoothly. Again, in the next section I will give
some advice on how to practice your communication skills. Practical (and very
opinionated) advice As the title says, this advice is very opinionated, but
these methods have worked well for me. Before I get into the actual advice let
me explain what I look for in a Data Scientist candidate: Someone who is easy
going and has good personal/communication skills. This might not seem too
important for a technical role, but to me it is critical. Most Data Scientists
work in teams, so being able to get along and communicate is of paramount
importance. Someone who has shown initiative in their past and who is fairly
self motivated. This does not need to be ML/Data Science related, it could
apply to your academic field of study but it could also apply to your
preparation for transitioning to data science. Someone who has experience with
"real" data. By real data, I mean messy data. I want to see that you have
learned the struggle and have accepted it. Again, this can apply to your
academic past or your data science preparation. Someone who can converse about
many different ML techniques. This does not mean that you need to know any in
very deep detail, but you should have an idea of what techniques exist and the
applications that they are used for. With that, lets get to the advice 1\.
Learn Python Python is the language of Machine Learning. It is very easy to
use and it mostly wraps libraries that are written in more performant
languages like C. "https://jellis18.github.io/images/python-ml.jpg"
alt="Python"> This is slightly outdated but Python is growing very fast and
has overtake other languages. It is growing across all areas, not just ML but
for this post I will focus on ML. Python is very powerful in ML because of
several purpose-build ML libraries such as pandas , tensorflow , pytorch ,
scikit-learn , spacy and transformers just to name a few. If you already know
Python, that is great. If you don't, there are a plethora of resources out
there to learn. I would not recommend using a book to learn Python when there
are so many great resources out there already. I learned Python after
switching from Matlab and have honed my skills over the years by looking at
well-designed code bases like those mentioned above, and by watching various
YouTube tutorials and other online sources. In fact I have never read a book
on Python programming (of machine learning or deep learning or statistics for
that matter, but I'll get into that a bit more later). 2\. Learn Software
Design Principles Ok, so you've learned Python. Good to go, right? Well not
really, I knew how to use Python for a pretty long time before I actually
learned how to design software. I always use the analogy of building a house.
You could be really good at using all of the tools, but if you don't have a
blueprint, know the various codes and standards, or know where and how to get
all of the materials, then you are out of luck. While knowing the Python
language is very important, actually knowing software design principles
(mostly language agnostic) will really let you stand out. This section in
itself could serve as several blog posts but I'll just mention the main
highlights here: Object Oriented Programming (OOP) : While there is nothing
wrong with functional programming and you will indeed use a mix of OOP and
functional programming in your day-to-day existence. The ML ecosystem (i.e.
the packages mentioned above, especially PyTorch and scikit-learn) is mostly
in the OOP camp. Being able to make sense of OOP based packages and being able
to write your own OOP-based code is super important. Unit testing and
continuous integration (CI) : These two usually go hand-in-hand. Unit testing
refers to writing specific tests of your individual code "units.” This serves
a check that your code is right but also serves as a fail-safe against
breaking your code when you make changes. CI basically involves automating
this test process and can also involve other types of tests other than unit
tests. Having a grasp on this will give you a huge advantage. Design Patterns
: These are re-usable patterns that solve specific problems. These patterns
are mostly language agnostic. Now, you don't need to know all of the different
patterns and you don't need to always use them; however, having a basic
understanding of what they do, (and that they exist) will put you in a great
position. Documentation : In your academic career you probably wrote code that
was just for you. You knew how to use it and it worked. As part of a data
science team, other people will need to be able to use and understand your
code. Code documentation is of paramount importance and being aware of the
best practices is super important. The list above only scratches the surface
in terms of details. I provided a basic tutorial link for each, but there are
a lot of other really good resources to learn more about proper coding
practices and tools. Here are a few: Making Pythonic code : Great talk on code
re-factoring and making beautiful code. Deep learning library from scratch :
Good introduction to package design and learning how to think about building a
package. calmcode.io : Lots of useful videos on various tools and techniques
Traversy Media : Good introduction to the web-developnent side of Python. I'll
end this section by noting that, in my experience, most data scientists
(especially of the machine learning engineer variety) only have limited
knowledge of these things. So, this is not something that you will be lacking
in terms of your competition. However, if you do know these things and take
the time to learn them, then it will put you in a position to stand out above
your competition. More importantly, it will make you a better data scientist.
3\. Do some projects (and share them) Ok, here is where the standard advice
comes in, but it is standard for a reason. If you are coming from an academic
background and not a data science background, you have to do some projects to
fill in for your lack of on-the-job experience. In some ways this is a bonus,
especially if you are competing against people who have gone to school for
data science or a similar field. Remember what I said about being self-
motivated. For those in a data science or analytics program, most of their
experience is through classes of which they "had" to do. For you, the self-
taught data scientist, you show that you are self-motivated right away by
presenting data science projects you have done. Now there are lots of
different kinds of projects you can do. I'll list three here in rough order of
importance: Data storytelling project : In my opinion, this kind of project is
the most useful that you can do. Basically, choose an area that you are
interested in, find data, analyze it and tell a story. You don't really even
need to do any ML. These projects show off your skills with raw unfiltered
data, (try not to use curated data sets) and can also give you some experience
in the data science struggle. In my case I looked at police shootings of
civilians and pulled in data from a few databases, census data, FBI crime
statistics and local police dept data. In the process, I struggled big time to
pull all of these sources together and learned a ton about pandas and various
python mapping packages. Full end-to-end ML application : This kind of project
would be very impressive and will show that you have experience with actual ML
deployment, which is missing in a lot of candidates and actual data
scientists, (myself included). Kaggle : You can learn a lot about ML just from
doing some Kaggle competitions and looking at the public notebooks. These are
on the list because they are important for you go get some experience applying
ML to data, but they are last because this kind of work is now ubiquitous so
it won't set you apart too much. Once you have done some projects and gained
some ML and data wrangling skills with a large dose of struggle, now you can
sharpen up on your communication skills. First off, definitely put whatever
you do on GitHub which will make your work public and teach you about version
control if you didn't already know about it. Make sure you document your
project with a project README. This is just the lowest level of sharing your
work. Next, you may want to share this work on a personal blog or on Medium .
This is another great way to hone your communication skills and to ensure that
you actually understood your project. The best way to test your knowledge is
to try to explain your project to someone else. However, writing a blog post
is still somewhat passive and these skills are not the main communication
skills you will need to be a successful Data Scientist. Lastly, you can
present your work by giving a talk. There are probably a lot of forums for
this kind of thing. In my case I looked on Meetup for any local Data Science
related groups. It’s probably a good idea to attend a few meetups first and
introduce yourself to the organizers. They are usually on the lookout for
anyone who wants to give a talk. If you get a chance, offer to give a
presentation on one of your projects. So, this sounds like a lot and your
future company may not even look at all of this work that you have done, but
even if they don't look at it, the fact that you have done all of this and
developed and honed all of these skills will help you immensely in your
overall career as a Data Scientist. 4\. Learn the lingo (fake it till you make
it) Back when I was an undergraduate, a physics professor of mine gave me some
great advice. He told us that one of the most important things in terms of
your career is that you "learn the lingo,” that is, you learn the language of
the field that you are in or want to be in. For example, if someone says to
you that they "used a transformer model for sentiment analysis,” what do you
need to take away from that statement? You definitely don't need to know in
any detail what a transformer model is or how to implement it, but you should
know that sentiment analysis refers to measuring how "positive" or "negative"
a segment of text is and that a "transformer" model is a kind of deep learning
model that is very popular in natural language processing nowadays.
"https://jellis18.github.io/images/fake-until-make.jpeg" alt=""> Knowing the
lingo of your field can make or break your career advancement. In other words,
it is far better to have a wide, but shallow understanding of many concepts
instead of a very deep understanding of a few concepts. This is almost the
opposite of academic research in many cases. In academia, one pretty much
needs to have a very deep understanding of their area at the expense of not
knowing much outside of that area. 4 As a data scientist the exact opposite is
true, it is far better to have a base understanding of many concepts and their
uses rather than knowing the exact algorithmic details. This gets back to my
earlier point about learning how to learn. If you have a shallow understanding
of many things then that will allow you to assess a given problem quickly and
come up with a potential solution; however, to actually implement that
solution you will probably need to learn more that you know at the moment.
Therefore, you need to be able to learn on your feet. As an academic, it may
seem like you always need to come up with something new, but in data science
no one really cares if you come up with something new, they only care if you
solve the problem. This means that there is no shame is scouring the internet
for solutions to a problem similar to yours and adapting it to solve your
problem. All of this may seem wrong, but it is some of the best advice that I
ever received and I think it has helped me immensely. Now, this all sounds
nice, but how do you do it? Well, this is the hard part. I think the best way
to develop this skill is to allow yourself to explore many areas, but also
control yourself from digging too deep, at least in the beginning when you are
still learning. Take introductory online course, follow tutorials, do some
simple projects, but don't think that you need to know all of the details. 5\.
Learn online Ok, now on to perhaps another controversial piece of advice.
Don't read books to learn your Data Science/ML skills. At least, don't use
books as a main source of information. The only thing I would recommend books
for is to learn basic programming language fundamentals, (although those are
probably better online as well). For some complete anecdotal evidence of this,
I have never read a book on programming, machine learning, deep learning,
statistics or communication, yet in my academic career I published several
papers and led large working groups. Outside of academia, I have advanced
fairly quickly and have led projects in several ML areas and I achieved all of
that strictly from learning online. Now, learning online does not mean that
you passively sit back and watch some YouTube videos. It means that you watch
some YouTube videos or online courses or read blog posts or arXiv papers and
then try the things out yourself. You struggle. You watch and read some more.
Struggle some more. Apply the techniques in your work or in your projects.
Struggle some more. By the end of this struggle session, you will realize that
you have managed to learn a whole lot, not by some ground up foundational
approach but by a more stochastic process of trial and error which eventually
leads to a much deeper understanding of the material. This may all sound crazy
and it may not work for you, but it has worked for me and it has worked for a
lot of other great Data Scientists and Machine Learning Engineers. I have a
very shotgun approach to learning in which I dabble in many different things,
but I can offer a few good resources here in addition to the resources that I
have already sprinkled throughout this post. Coursera : There are a lot of
great courses here that can give you a good introduction to many topics
especially machine learning and deep learning . Don't expect to be an expert
on anything after taking a Coursera course, but they can give you a good base
to start with. fast.ai : I would say this is the best resource for learning
cutting edge Deep Learning techniques. Unfortunately, it’s backed by a
software library that, in my opinion, combines all of the worst coding
practices and puts them all in one place. Even so, there are two great Deep
Learning courses and a more unknown Machine Learning ( github here ) and
Linear Algebra course. YouTube: You can pretty much find everything you want
about Data Science/ML/Programming on YouTube. The problem is that the quality
varies a lot. Instead of recommending specific channels or videos, (some of
which I've already mentioned above) I'll just say that, for me, the most
useful videos in many cases are real-time walkthroughs where they are coding
in real time (this is one of the reasons fast.ai is great). However, sometimes
you are just interested in a concept, so the coding doesn't matter as much.
Either way, YouTube is an amazing resource for everyone to use. Blog posts:
towards data science can have some really good posts, but the mileage varies.
This is mainly to say that looking at random blog posts to learn things may
not be a bad idea. Even if the post itself is not very good, usually there are
other references that can lead you to what you want. Code Documentation and
Source Code: Lots of large code bases have really extensive documentation and
tutorials. Going through these tutorials can be a great learning experience.
In some other cases looking at the source code itself can show you some good
programming practices, (and in some cases can surprise you at how bad the
source code for various projects is). For well-organized and excellently
documented code, I would recommend PyTorch and scikit-learn for starters.
Twitter: Can be a dumpster fire, but can also be a great tool for learning new
things and making new connections. I actually found my current job through a
twitter friend. There are too many to list here, but search for data science
or machine learning and follow some more popular accounts. Eventually, you
will grow this network and use it as a kind of filter for information. Wrap up
If you made it this far, congratulations! This post turned out to be much
longer than I originally had planned. I have covered a lot of things here and
it may seem daunting. When I first started on this journey, I kept thinking
that there was no way I could learn all of this stuff and maybe it would just
be better to stay in academia. If you are passionate about your academic field
then by all means don't give up, but if you are just staying in because you
think there is no other option then that is a completely wrong assumption. All
of the things I have mentioned in this post are things I have learned over the
previous 3.5 years as a Data Scientist. I did not do all of these things in
the very beginning and have learned a lot since then. Lastly, if you are in
this transition and you are feeling stressed, remember once you gain these
skills you will be a hot commodity and will be able to get a fulfilling job
almost anywhere. If you are reading this and you think "This guy has no idea
what he is talking about,” you would be right. I know almost nothing about big
data, which is the point I make in the next paragraph. Hang in there... ↩
While this term is gaining steam, I think it should be much more prevalent as
it is a fairly precise statement of the job role. ↩ I should note that this is
getting better now with students being encouraged to think about alternative
career roles and even having guest speakers from outside of academia. ↩ That
is not to say that this is the desired state, but more that the incentives in
academia lead to this outcome. The point my professor was making was that we
should not do this, which is relatively easy in a field like data science, but
much harder in academic physics. ↩ Share on Twitter Facebook Google+ LinkedIn
Leave a Comment You May Also Enjoy A fun multi-lingual concurrency experiment
4 minute read The other day I was writing some code for a CI pipeline that
needed to fetch release notes from several different git repos and collate
them into one large s... Production Dockerfile Tips and Tricks 6 minute read
Container images are the backbone of a lot of modern workflows, especially in
the age of Kubernetes and friends. Building images that are small, secure, and
... Some Advanced Typing Concepts in Python 13 minute read To start off, let
me admit that yes, Python is a dynamically (gradually?) typed language.
However, with modern Python (3.6+, and really with 3.10+) and stati... Early
and late binding closures in Python 3 minute read So it happened again, late-
binding closures bit me. I once again had to discover that this was a thing so
I decided to make a little post about it just in ca... Please enable
JavaScript to view the comments powered by Disqus.

***URL: https://jellis18.github.io/post/page4/***

Justin A. Ellis - Page 4 Justin Ellis Principal Engineer, Infinia ML. Former
Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML Recent Posts Starting a Blog less than 1 minute
read As is seemingly more common in our field of Astrophysics, I am working on
making the transition to Data Science. As part of the learning process for me
and i...

***URL: https://jellis18.github.io/post/2016-10-31-starting-a-blog/***

Starting a Blog - Justin A. Ellis Justin Ellis Principal Engineer, Infinia ML.
Former Astrophysicist, former Data Scientist. Follow Durham, NC Email Twitter
LinkedIn GitHub Infinia ML As is seemingly more common in our field of
Astrophysics, I am working on making the transition to Data Science. As part
of the learning process for me and information for others I have decided to
start a blog detailing my experiences, projects, and thoughts. Coming soon
will be a post on my thoughts on the TDWI Data Science Bootcamp , writing an
MCMC in R, and a few posts on some analysis that I've done on crime and police
shooting data. {: .notice--info} Update October 8, 2017: Ok that didn't work
out so well. It turns out that searching for jobs and moving across the
country is quite a lot of work. However, I'm back now, nearly a year after
starting this Blog, to try again. Expect some content from me soon... {:
.notice--info} Update January 3, 2021: LOL, well here I am again. It turns out
that I mainly worked on this website in order to get a job. Well, I have a
good job now and figured I should get back at it. Hopefully there will be more
posts from me soon... Share on Twitter Facebook Google+ LinkedIn Leave a
Comment You May Also Enjoy A fun multi-lingual concurrency experiment 4 minute
read The other day I was writing some code for a CI pipeline that needed to
fetch release notes from several different git repos and collate them into one
large s... Production Dockerfile Tips and Tricks 6 minute read Container
images are the backbone of a lot of modern workflows, especially in the age of
Kubernetes and friends. Building images that are small, secure, and ... Some
Advanced Typing Concepts in Python 13 minute read To start off, let me admit
that yes, Python is a dynamically (gradually?) typed language. However, with
modern Python (3.6+, and really with 3.10+) and stati... Early and late
binding closures in Python 3 minute read So it happened again, late-binding
closures bit me. I once again had to discover that this was a thing so I
decided to make a little post about it just in ca... Please enable JavaScript
to view the comments powered by Disqus.

***URL: https://iver56.github.io***

Site not found · GitHub Pages 404 There isn't a GitHub Pages site here. If
you're trying to publish one, read the full documentation to learn how to set
up GitHub Pages for your repository, organization, or user account. GitHub
Status — @githubstatus

***URL: https://iver56.github.io/***

Site not found · GitHub Pages 404 There isn't a GitHub Pages site here. If
you're trying to publish one, read the full documentation to learn how to set
up GitHub Pages for your repository, organization, or user account. GitHub
Status — @githubstatus

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://galax.dev/blog.html***

galax.dev - blog Categories All (2) Deep Learning (2) Implementation (2)
Stable Diffusion (1) afterhoursbilly GitHub Twitter [email protected] Order By
Default Date - Oldest Date - Newest LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Monday, 13 November 2023 2 min An Image-
to-Image Implementation Demonstation of an image-to-image implementation of
the Stable Diffusion model. Tuesday, 03 October 2023 1 min No matching items
Back to top

***URL: https://galax.dev/posts/00_Image-to-Image.html***

galax.dev - An Image-to-Image Implementation Text-Guided: Image-to-Image
Implementation This Python code demonstrates the implementation of the Image-
to-Image technique, allowing you to generate new images from existing ones
with the help of textual prompts. Explore how this innovative approach
combines images and text to create visually compelling artworks. Dive into the
code to understand the mechanics behind this cutting-edge image generation
technique. Pip install necessary libraries (click to show/hide) ! pip install
\- Uq diffusers transformers fastcore fastdownload Importing utilities (click
to show/hide) from transformers import CLIPTextModel, CLIPTokenizer import
torch from diffusers import LMSDiscreteScheduler from PIL import Image from
tqdm.auto import tqdm from diffusers import AutoencoderKL,
UNet2DConditionModel import logging from fastdownload import FastDownload from
pathlib import Path from huggingface_hub import notebook_login import
matplotlib.pyplot as plt from torchvision import transforms if not
(Path.home() / '.cache/huggingface' / 'token' ).exists(): notebook_login()
logging.disable(logging.WARNING) We need to load in the required libraries and
set up the models. tokenizer = CLIPTokenizer.from_pretrained( "openai/clip-
vit-large-patch14" , torch_dtype = torch.float16) text_encoder =
CLIPTextModel.from_pretrained( "openai/clip-vit-large-patch14" , torch_dtype =
torch.float16).to( "cuda" ) # Here we use a different VAE to the original
release, which has been fine-tuned for more steps vae =
AutoencoderKL.from_pretrained( "stabilityai/sd-vae-ft-ema" , torch_dtype =
torch.float16).to( "cuda" ) unet = UNet2DConditionModel.from_pretrained(
"CompVis/stable-diffusion-v1-4" , subfolder = "unet" , torch_dtype =
torch.float16).to( "cuda" ) Define the parameters. height = 512 width = 512
num_inference_steps = 70 guidance_scale = 7.5 batch_size = 1
beta_start,beta_end = 0.00085 , 0.012 scheduler =
LMSDiscreteScheduler(beta_start = beta_start, beta_end = beta_end,
beta_schedule = "scaled_linear" , num_train_timesteps = 1000 )
plt.plot(scheduler.sigmas) plt.title( 'Noise Schedule' ) plt.xlabel( 'Sampling
step' ) plt.ylabel( 'sigma' ) plt.show() def prep_img(img_link : str ) ->
torch.Tensor: """ Preprocesses an image from a given link. Args: img_link
(str): The URL or path to the image file. Returns: torch.Tensor: A tensor
representing the preprocessed image. """ p = FastDownload().download(img_link)
init_image = Image. open (p).convert( "RGB" ).resize(( 512 , 512 )) return
transforms.ToTensor()(init_image) The image we will use as a starting point.
Downloading the image (click to show/hide) link = "https://cdn-
uploads.huggingface.co/production/uploads/1664665907257-noauth.png"
transformed_image = prep_img(link) # show image in notebook. p =
FastDownload().download(link) display(Image. open (p).convert( "RGB"
).resize(( 512 , 512 ))) def tokenization(prompt: list , max_len : int = None
) \- > torch.Tensor: """ Tokenizes a text prompt and returns the corresponding
encoded tensor. Args: prompt (list): The input text prompt to be tokenized.
max_len (int, optional): The maximum length of the tokenized sequence. If not
specified, it defaults to the maximum length allowed by the tokenizer.
Returns: torch.Tensor: A tensor containing the encoded representation of the
tokenized prompt. """ if max_len is None : max_len =
tokenizer.model_max_length tokenized_prompt = tokenizer(prompt, padding =
"max_length" , max_length = max_len, truncation = True , return_tensors = 'pt'
) return text_encoder(tokenized_prompt.input_ids.to( 'cuda' ))[ 0 ].half() def
make_image(latent: torch.Tensor): """ Converts a tensor representation of an
image into a PIL Image. Args: latent (torch.Tensor): A tensor representing an
image. Returns: PIL.Image.Image: A PIL Image representing the image. """ image
= (latent / 2 \+ 0.5 ).clamp( 0 , 1 ).detach().cpu().permute( 1 , 2 , 0
).numpy() return Image.fromarray((image * 255 ). round ().astype( "uint8" ))
Denoising loop To ensure the effectiveness of this solution, it is essential
to incorporate the “start_step” parameter. Essentially, we aim to prevent
excessive noise from being added to the input image, particularly avoiding the
most intense noise additions. After this initial step, we can proceed with the
looping process. In summary, the key to success here is to introduce the
“start_step” parameter, which helps us avoid excessive noise in the early
stages and then continue with the loop as intended. def create_sample(prompt:
list ,transformed_image: torch.Tensor ,guidance_scale: float = 7.5 , seed: int
= 5 , steps: int = 70 ,start_step: int = 10 ): ''' Generate a sample image
based on a text prompt, provided image and guidance parameters. Args: prompt
(list): A list of text prompts. transformed_image (torch.Tensor): A tensor
representing the transformed image. guidance_scale (float, optional): The
scale factor for guiding the generation process. seed (int, optional): Seed
for random number generation. Default is 5. steps (int, optional): The total
number of steps for the generation process. Default is 70. start_step (int,
optional): The step at which the generation process starts. Default is 10.
Returns: torch.Tensor: A tensor representing the generated sample. This
function generates an image based on the provided text prompts , transformed
image and parametrs.It uses a predefined VAE model to encode the image and
then applies noise and guidance to generate the sample.It iteratively refines
the image by adding noise and updating the latent representation. The guidance
scale controls the influence of the text prompts on the image. The generated
image is returned as a PyTorch tensor. Example: >>> prompt = ["Translate the
following English sentence to French: 'Hello, how are you?'"] >>>
transformed_image = prep_img(image_link) >>> generated_sample =
create_sample(prompt, transformed_image) ''' bs = 1 # Implementation for only
a single prompt. text = tokenization(prompt) uncond = tokenization([ "" ] *
bs, text.shape[ 1 ]) emb = torch.cat([uncond, text]) if seed:
torch.manual_seed(seed) # Encode image image_latent =
vae.encode((transformed_image.unsqueeze( 0 ).half().to( 'cuda'
))).latent_dist.sample() image_latent = vae.config.scaling_factor *
image_latent # Create noise scheduler.set_timesteps(steps) noise_latents =
torch.randn_like(image_latent) latents = scheduler.add_noise(image_latent,
noise_latents, timesteps = torch.tensor([scheduler.timesteps[start_step]]))
for i, ts in enumerate (tqdm(scheduler.timesteps)): if i >= start_step: # Skip
the batches of noise that don't affect the input image. inp =
scheduler.scale_model_input(torch.cat([latents] * 2 ), ts) with
torch.no_grad(): noise_pred_uncond, noise_pred_text = unet(inp, ts,
encoder_hidden_states = emb).sample.chunk( 2 ) pred = noise_pred_uncond \+
guidance_scale * (noise_pred_text \- noise_pred_uncond) latents =
scheduler.step(pred, ts, latents).prev_sample with torch.no_grad(): return
vae.decode( 1 / 0.18215 * latents).sample prompt = [ 'Wolf howling at the
moon, photorealistic 4K' ] #prompt = ['unicorn'] #prompt = ['a kids drawing of
bacteria, cartoon style'] #prompt = [' Horse looking at the morning sun,
photorealistic 4K'] image = create_sample(prompt,transformed_image,steps = 50
,seed = 1000 ) display(make_image(image[ 0 ])) Back to top

***URL: https://galax.dev/apps.html***

galax.dev - apps playground Logo Header Categories All (0) afterhoursbilly
GitHub Twitter [email protected] No matching items Back to top

***URL: https://galax.dev/index.html***

galax.dev My name is Szymon and, I am an aspiring Machine/Deep Learning
Engineer, currently in my third year of Computer Science. If you are already
here, you can check out my blog posts and demo apps in the playground below.
You can find most of the code for my projects on my GitHub. blog Click here to
check out the all blog posts. LR Schedulers Implementation From Scratch
Implementation of cosine annealing and OneCycle learning rate schedulers from
scratch using tinyai mini-framework Nov 13, 2023 An Image-to-Image
Implementation Demonstation of an image-to-image implementation of the Stable
Diffusion model. Oct 3, 2023 No matching items playground Click here to play
more in the playground. No matching items Contact Information afterhoursbilly
GitHub Twitter [email protected]

***URL: https://galax.dev/posts/00_lr_scheduler_from_scratch.html***

galax.dev - LR Schedulers Implementation From Scratch Importing utilities
(click to show/hide) import torch,math,functools import matplotlib.pyplot as
plt from functools import partial import pdb from tinyai.datasets import *
from tinyai.conv import * from tinyai.learner import * from tinyai.activations
import * from tinyai.init import * from tinyai.sgd import * from datasets
import load_dataset import torchvision.transforms.functional as
TF,torch.nn.functional as F from torch import tensor,nn,optim import fastcore.
all as fc from torch.optim import lr_scheduler from torcheval.metrics import
MulticlassAccuracy x = torch.linspace( 0 , 10 , 10 ) lr = 5 print (x,math.pi)
tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778,
8.8889, 10.0000]) 3.141592653589793 How we want our learning rate to look at.
def plot_thing(f,lr,steps): x = torch.linspace( 0 ,math.pi,steps)
plt.plot(x,(f(x) \+ 1 ) / 2 * lr) plot_thing(partial(torch.cos),lr,steps = 100
) Figure: Plot of Cosine Function Lets try in learner Importing and
transfroming dataset (click to show/hide) xl,yl = 'image' , 'label' # x label,
y label name = "fashion_mnist" bs = 1024 xmean,xstd = 0.28 , 0.35 @inplace def
transformi(b): b[xl] = [(TF.to_tensor(o) \- xmean) / xstd for o in b[xl]] dsd
= load_dataset(name) tds = dsd.with_transform(transformi) dls =
DataLoaders.from_dd(tds, bs, num_workers = 4 ) CosineAnnealingLR First
Version. Cosine Annealing LR implementation from scratch, which had to be
updated for the OneCycleLR This version might be a little faster but take more
memory.(not tested) First Version. (click to show/hide) class CosAnnLR(): def
__init__ ( self ,tmax,optim): self .optim = optim self .tmax = tmax self .lr =
optim.param_groups[ 0 ][ 'lr' ] self .values = self ._init_values() self
.cur_step = 0 def _init_values( self ): return (torch.cos(torch.linspace( 0
,math.pi, self .tmax)) \+ 1 ) / 2 * self .lr def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = self .values[ self .cur_step] self
.cur_step += 1 Second Version CosineAnnealingLR implementation from scratch.
Second Version. (click to show/hide) class CosAnnLR(): def __init__ ( self
,tmax,optim): self .optim = optim self .lr = optim.param_groups[ 0 ][ 'lr' ]
self .tmax = tmax self .cur_step = 0 def step( self ): self
.optim.param_groups[ 0 ][ 'lr' ] = (math.cos( self .cur_step / self .tmax *
math.pi) \+ 1 ) / 2 * self .lr self .cur_step += 1 def _lr(cb): return cb.pg[
'lr' ] # Callback that will allow us to record LR during learning. Preparing
the learner for training. Code for learner. (click to show/hide) act_gr =
partial(GeneralRelu, leak = 0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy =
MulticlassAccuracy()) astats = ActivationStats(fc.risinstance(GeneralRelu))
cbs = [DeviceCB(), metrics, ProgressCB(plot = True ), astats] iw =
partial(init_weights, leaky = 0.1 ) set_seed( 42 ) lr,epochs = 1e-2 , 5 model
= get_model(act_gr, norm = nn.BatchNorm2d). apply (iw) tmax = epochs * len
(dls.train) sched = partial(CosAnnLR,tmax) #sched =
partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) # Testing if it works
with pytorch's CosineAnnealingLR record = RecorderCB(lr = _lr) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) Code learn.fit(epochs)
accuracy loss epoch train 0.806 0.529 0 train 0.853 0.404 0 eval 0.876 0.338 1
train 0.872 0.349 1 eval 0.892 0.295 2 train 0.882 0.326 2 eval 0.904 0.264 3
train 0.887 0.316 3 eval 0.910 0.248 4 train 0.887 0.310 4 eval Plot of
learning rate throughout the learning process Code record.plot()
astats.color_dim() Figure: Plot of Weight’s distribution. astats.plot_stats()
Figure: Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = 0. CosineAnnealing
Summary. After creating my own CosineAnnealing I decided to look for paper
where it was introduced, and I found this paper . Where we can find this
equation. \\[ \eta_{t} = \eta_{min}^{i} +
\frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
\\] If we compared it to our code, it looks completely different.
(math.cos(cur_step / tmax * math.pi) \+ 1 ) / 2 * lr But if we read the paper
further, the η and T could be translated to our code. Where: \\[ \eta \text{
(eta) - is learning rate } \\] \\[ T_{cur} \text{ - is current step }\\] \\[
t_{i} \text{ - is our tmax}\\] \\[ lr_{t} = lr_{min} +
\frac{1}{2}\left(lr_{max}-lr_{min}\right)\left(1+\cos\left(\frac{\text{curstep}}{tmax}\pi\right)\right)
\\] The paper’s equation introduces min & max learning rate, therefore the
difference. But the rest is the same. OneCycleLR CLR should specify minmum and
maximum learning rate boundaries and a step_size , but this implementation
doesn’t do that. Adding minimum and maximum should be pretty straight forward,
tho. You also might want to add a 3rd phase where learning rate is at its
maximum for 5-10% of the training. class OneCycleLR: ''' This version of
OneCycle was create before looking up CosineAnnealing paper. ''' def __init__
( self , tmax, optim, warm_up: float = 0.30 ): self .optim = optim self
.initial_lr = self .optim.param_groups[ 0 ][ 'lr' ] self .beta, self .beta_2 =
self .optim.param_groups[ 0 ][ 'betas' ] self .max_beta, self .min_beta = self
.beta \+ 0.05 , self .beta \- 0.05 self .warm_up = warm_up self .warm_up_steps
= int (tmax * self .warm_up) self .annealing_steps = tmax \- self
.warm_up_steps self .cur_step = 0 def get_beta( self ,phase: float
,warming_up: bool ): if warming_up: return self .min_beta \+ ( self .max_beta
\- self .min_beta) * ((math.cos(math.pi * phase) \+ 1 ) / 2 ) else : return
self .max_beta \+ ( self .min_beta \- self .max_beta) * ((math.cos(math.pi *
phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if self .cur_step <=
self .warm_up_steps: # Increasing learning rate phase = self .cur_step / self
.warm_up_steps adjusted_lr = (math.cos(phase * math.pi \+ math.pi) \+ 1 ) / 2
* self .initial_lr adjusted_beta = self .get_beta(phase, warming_up = True )
else : # Decreasing learning rate phase = ( self .cur_step \- self
.warm_up_steps) / self .annealing_steps adjusted_lr = (math.cos(phase *
math.pi) \+ 1 ) / 2 * self .initial_lr adjusted_beta = self .get_beta(phase,
warming_up = False ) # adjusted_lr min_max self .optim.param_groups[ 0 ][ 'lr'
] = adjusted_lr self .optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta,
self .beta_2) self .cur_step += 1 def _beta1(cb): return cb.pg[ 'betas' ][ 0 ]
rec = RecorderCB(lr = _lr, mom = _beta1) Preparing the learner for training.
Code for learner. (click to show/hide) act_gr = partial(GeneralRelu, leak =
0.1 , sub = 0.4 ) metrics = MetricsCB(accuracy = MulticlassAccuracy()) astats
= ActivationStats(fc.risinstance(GeneralRelu)) cbs = [DeviceCB(), metrics,
ProgressCB(plot = True ), astats] iw = partial(init_weights, leaky = 0.1 )
set_seed( 42 ) lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) #sched = partial(lr_scheduler.OneCycleLR,max_lr =
lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR
record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.723 0.827 0 train 0.822 0.485 0 eval 0.860 0.386 1 train
0.864 0.368 1 eval 0.887 0.310 2 train 0.877 0.338 2 eval 0.902 0.268 3 train
0.882 0.316 3 eval 0.912 0.242 4 train 0.888 0.303 4 eval Note: If you
happened to know why does the learning doesn’t go smoothly at the beginning, u
can dm me on discord @afterhoursbilly Plot of Learning Rate and Momentum
throughout the learning process Code record.plot() astats.plot_stats() Figure:
Plot of Weight’s Means and Stdves throughout the learning process.
astats.dead_chart() Figure: Plot of Weight’s that are = to 0.
astats.color_dim() Figure: Plot of Weight’s distribution. OneCycle Summary
Inspired by paper , & fast.ai 22part course This CLR implements minmum and
maximum learning rate boundaries We could also add a phase where learning rate
is at its maximum for 5-10% of the training. class OneCycleLR: ''' Modified
version after looking up papers. ''' def __init__ ( self , tmax, optim,
warm_up: float = 0.30 ): self .optim = optim self .initial_lr, self .min_lr =
self .optim.param_groups[ 0 ][ 'lr' ], self .optim.param_groups[ 0 ][ 'lr' ]
// 20 self .beta, self .beta_2 = self .optim.param_groups[ 0 ][ 'betas' ] self
.max_beta, self .min_beta = self .beta \+ 0.05 , self .beta \- 0.05 self
.warm_up = warm_up self .warm_up_steps = int (tmax * self .warm_up) self
.annealing_steps = tmax \- self .warm_up_steps self .cur_step = 0 def
cosine_annealing( self ,phase, min , max ): return min \+ ( max \- min ) *
((math.cos(math.pi * phase) \+ 1 ) / 2 ) def step( self ): # warm_up phase if
self .cur_step <= self .warm_up_steps: # Increasing learning rate phase = self
.cur_step / self .warm_up_steps adjusted_lr = self .cosine_annealing(phase,
self .initial_lr, self .min_lr) adjusted_beta = self .cosine_annealing(phase,
self .min_beta, self .max_beta) else : # Decreasing learning rate phase = (
self .cur_step \- self .warm_up_steps) / self .annealing_steps adjusted_lr =
self .cosine_annealing(phase, self .min_lr, self .initial_lr) adjusted_beta =
self .cosine_annealing(phase, self .max_beta, self .min_beta) # adjusted_lr
min_max self .optim.param_groups[ 0 ][ 'lr' ] = adjusted_lr self
.optim.param_groups[ 0 ][ 'betas' ] = (adjusted_beta, self .beta_2) self
.cur_step += 1 lr,epochs = 1e-2 , 5 model = get_model(act_gr, norm =
nn.BatchNorm2d). apply (iw) tmax = epochs * len (dls.train) sched =
partial(OneCycleLR,tmax) record = RecorderCB(lr = _lr, mom = _beta1) xtra =
[BatchSchedCB(sched),record] learn = TrainLearner(model, dls, F.cross_entropy,
lr = lr, cbs = cbs \+ xtra, opt_func = optim.AdamW) learn.fit(epochs) accuracy
loss epoch train 0.696 0.921 0 train 0.825 0.476 0 eval 0.857 0.391 1 train
0.861 0.385 1 eval 0.884 0.317 2 train 0.875 0.348 2 eval 0.900 0.272 3 train
0.882 0.322 3 eval 0.913 0.241 4 train 0.886 0.315 4 eval Back to top

***URL: https://iver56.github.io***

Site not found · GitHub Pages 404 There isn't a GitHub Pages site here. If
you're trying to publish one, read the full documentation to learn how to set
up GitHub Pages for your repository, organization, or user account. GitHub
Status — @githubstatus

