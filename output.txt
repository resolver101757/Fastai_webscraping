[ Engineering notes ](./index.html)

  * [ Home](./index.html)
  * [ Proejcts](./projects.html)
  * [ Blog](./blog.html)

## On this page

  * About
  * Skills
  * Experience
  * Education

![](photo.jpg)

# Viktor Anchutin

[ LinkedIn ](https://www.linkedin.com/in/viktor-anchutin/) [ GitHub
](https://github.coViktorAnchutinm/)

# About

Software engineer passionate about machine learning and data intensive
applications.

I’ve worked as a robotics software developer using C and C++ before
transitioning to backend engineering with Kotlin/Java, where I’ve helped to
build various data intensive systems. I’m currently finishing my second
master’s degree in the field of Data Science.

# Skills

**Programming languages** : Kotlin, Java, Scala, Python, C++,C

**ML** : PyTorch, Deep Learning, Statistical Learning, Numerical Optimization

**Databases** : PostgreSQL, Amazon DynamoDB, ElasticSearch

**Data processing** : Kafka, Spark, AWS SQS, AWS Lambda

**Software engineering** : Object Oriented Programming, Functional
Programming, Software Architecture Patterns, Testing and Monitoring,
Concurrent programming

**Robotics** Real-Time Control Systems, Position Estimation

# Experience

**Java/Kotlin software engineer**  
Grid Dynamics, Belgrade, Serbia, May 2021 - Aug 2022  

**Java software engineer**  
X5 Group, Moscow, Russia, Dec 2019 - Aug 2021  

**Robotics software engineer**  
UVS YURION, Moscow, Russia, Oct 2018 - Dec 2019  

**Embedded software engineer**  
Central Research Institute “Cyclone”, Moscow, Jul 2018 - Oct 2018  

# Education

**Master’s degree in Data Science**  
University Malaya, Kuala Lumpur, Malaysia, Sep 2022 - Jan 2024  
Research project: [Active emotion
detection](./projects/Emotional_speech_detection.html)  

**Master’s degree in Robotics**  
BMSTU, Moscow, Russia, 2016 - 2018  
Final project: [Electric motor control](./projects/motor_control.html)

**Bachelor’s degree in Control Systems and Automation Technology**  
MGSU, Moscow, Russia, 2012 - 2016

[ Engineering notes ](./index.html)

  * [ Home](./index.html)
  * [ Proejcts](./projects.html)
  * [ Blog](./blog.html)

# Proejcts

[

![](./projects/images/ess.png)

#####  Detecting active emotions in speech

Training 5 different models to perform active emotion recognition from speech
](./projects/Emotional_speech_detection.html)

[

![](./projects/images/rns.png)

#####  ResNet from scratch for rice disease classification

Building a custom ResNet model using Pytorch
](./projects/mini_resnet_from_scratch_with_pytorch.html)

[

![](./projects/images/hds.png)

#####  Heart disease prediction

Data analysis, Random Forest and Logistic Regression with heart disease
dataset  ](./projects/Heart_disease_project.html)

[

![](./projects/images/motor_main.png)

#####  Controlling magnetic field in electric motors

How I built BLDC motor control system from scratch in my Robotics masters
degree  ](./projects/motor_control.html)

No matching items

[ Engineering notes ](../index.html)

  * [ Home](../index.html)
  * [ Proejcts](../projects.html)
  * [ Blog](../blog.html)

## On this page

  * Data
    * Creating a single dataset
    * Attributes
    * Exploratory data analysis
      * Numeric data summary
      * Categorical data summary
      * Analysing risk factors for heart disease
      * Numeric attributes relationships
      * EDA colclusion
    * Data quality
      * Incorrect values
      * Missing values
  * Data Preparation
    * Data cleaning
      * Outliers
    * Statistical significance testing
      * Chi square test for categorical data
      * T-test for numeric features
    * Split train/test
      * Data imputation
    * One hot encoding
    * Feature Scaling
  * Modeling
    * Random forest
    * Logistic regression
    * SVM
    * Interpretation

# Heart disease prediction

Data analysis, Random Forest and Logistic Regression with heart disease
dataset

Imports

    
    
    import pandas as pd
    import numpy as np
    import seaborn as sns
    import matplotlib.pyplot as plt
    import scipy.stats as stats
    import palettable
    import seaborn as sns
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.impute import SimpleImputer
    from sklearn.model_selection import cross_val_score __

# Data

The data is available at UCI [machine learning
repository](https://archive.ics.uci.edu/ml/datasets/heart+disease).

The website contains 4 datasets concerning heart disease diagnosis.The data
for these datasets was collected from the four following locations:

    
    
    1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
    2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
    3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
    4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.

There are 2 versions of each dataset:

  * Full dataset with 76 attributes
  * Dataset with 14 attributes

The reduced dataset exists because only the subset of 14 attributes has been
used in prior research and experiments.

Files used for this project:

  * `processed.switzerland.data`
  * `processed.cleveland.data`
  * `processed.hungarian.data`
  * `processed.va.data`

I create a single dataset by combining these four.

Download data

    
    
    !wget https://archive.ics.uci.edu/static/public/45/heart+disease.zip
    !mkdir heart_disease_data
    !unzip heart+disease.zip -d heart_disease_data __

### Creating a single dataset

The datasets have the same columns, they don’t have headers, and missing
values are provided as `?`.

Code

    
    
    columns = ["age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak",
              "slope", "ca", "thal", "disease"]
    sdf = pd.read_csv("heart_disease_data/processed.switzerland.data", header=None, names=columns, na_values='?')
    cdf = pd.read_csv("heart_disease_data/processed.cleveland.data", header=None, names=columns, na_values='?')
    hdf = pd.read_csv("heart_disease_data/processed.hungarian.data", header=None, names=columns, na_values='?')
    vdf = pd.read_csv("heart_disease_data/processed.va.data", header=None, names=columns, na_values='?')
    
    df = pd.concat([sdf, cdf, vdf, hdf], ignore_index=True)
    df.disease = df['disease'].apply(lambda x: 1 if x > 0 else 0)
    df __

| age | sex | cp | trestbps | chol | fbs | restecg | thalach | exang | oldpeak
| slope | ca | thal | disease  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
0 | 32.0 | 1.0 | 1.0 | 95.0 | 0.0 | NaN | 0.0 | 127.0 | 0.0 | 0.7 | 1.0 | NaN
| NaN | 1  
1 | 34.0 | 1.0 | 4.0 | 115.0 | 0.0 | NaN | NaN | 154.0 | 0.0 | 0.2 | 1.0 | NaN
| NaN | 1  
2 | 35.0 | 1.0 | 4.0 | NaN | 0.0 | NaN | 0.0 | 130.0 | 1.0 | NaN | NaN | NaN |
7.0 | 1  
3 | 36.0 | 1.0 | 4.0 | 110.0 | 0.0 | NaN | 0.0 | 125.0 | 1.0 | 1.0 | 2.0 | NaN
| 6.0 | 1  
4 | 38.0 | 0.0 | 4.0 | 105.0 | 0.0 | NaN | 0.0 | 166.0 | 0.0 | 2.8 | 1.0 | NaN
| NaN | 1  
... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |
... | ...  
915 | 52.0 | 1.0 | 4.0 | 160.0 | 331.0 | 0.0 | 0.0 | 94.0 | 1.0 | 2.5 | NaN |
NaN | NaN | 1  
916 | 54.0 | 0.0 | 3.0 | 130.0 | 294.0 | 0.0 | 1.0 | 100.0 | 1.0 | 0.0 | 2.0 |
NaN | NaN | 1  
917 | 56.0 | 1.0 | 4.0 | 155.0 | 342.0 | 1.0 | 0.0 | 150.0 | 1.0 | 3.0 | 2.0 |
NaN | NaN | 1  
918 | 58.0 | 0.0 | 2.0 | 180.0 | 393.0 | 0.0 | 0.0 | 110.0 | 1.0 | 1.0 | 2.0 |
NaN | 7.0 | 1  
919 | 65.0 | 1.0 | 4.0 | 130.0 | 275.0 | 0.0 | 1.0 | 115.0 | 1.0 | 1.0 | 2.0 |
NaN | NaN | 1  
  
920 rows × 14 columns

### Attributes

#### Numerical attributes

  * `age`: age in years, numerical

  * `trestbps` \- resting blood pressure (in mm Hg on admission to the hospital)

  * `chol` \- cholesterol in mg/dl

  * `thalach` \- maximum heart rate achieved

  * `oldpeak` \- ST depression induced by exercise relative to rest. ‘ST’ relates to the positions on the electrocardiographic (ECG) plot.

  * `ca` \- number of major vessels (0-3) colored by flouroscopy. Fluoroscopy is one of the most popular non-invasive coronary artery disease diagnosis. It enables the doctor to see the flow of blood through the coronary arteries in order to evaluate the presence of arterial blockages.

#### Categorical attributes

  * `sex`: sex 
    * 1 = male
    * 0 = female
  * `cp`: chest pain type 
    * 1: typical angina
    * 2: atypical angina
    * 3: non-anginal pain
    * 4: asymptomatic
  * `fbs` \- fasting blood sugar > 120 mg/dl 
    * 1 = true
    * 0 = false
  * `restecg` \- resting electrocardiographic (ECG) results 
    * 0: normal
    * 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
    * 2: showing probable or definite left ventricular hypertrophy by Estes’ criteria
  * `exang` \- exercise induced angina. Angina is a type of chest pain caused by reduced blood flow to the heart. 
    * 1 - yes
    * 0 - no
  * `slope` \- the slope of the peak exercise ST segment. (ECG) 
    * 1: upsloping
    * 2: flat
    * 3: downsloping
  * `thal` \- A blood disorder called thalassemia 
    * 3: normal blood flow
    * 6: fixed defect (no blood flow in some part of the heart)
    * 7: reversable defect (a blood flow is observed but it is not normal)
  * `disease` \- refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.

## Exploratory data analysis

EDA helper functinos

    
    
    df_eda = df.copy()
    
    def plot_categorical(data=pd.DataFrame,
                         column=str,
                         labels=[],
                         target='disease',
                         target_labels=['healthy', 'heart disease'],
                         title='', font = 10, ax = None):
        crosstab = pd.crosstab(data[column], data[target])
        ax = crosstab.plot(kind='bar', figsize=(15, 7), rot = 0, fontsize=font, ax=ax)
        x = np.arange(len(labels))
        ax.set_xticks(x)
        ax.set_xticklabels(labels)
        ax.legend(target_labels)
        plt.ylabel("count", size=14)
        plt.title(title)
    
        bars =  ax.patches
    
        #compute percents
        total_by_category = crosstab.sum(axis=1)
        healthy_perc = round((crosstab[0] / total_by_category ) * 100)
    
    
        for (i, bar) in enumerate(bars):
          prc = healthy_perc.iloc[i] if i < len(healthy_perc) else 100 - healthy_perc.iloc[i % len(healthy_perc)]
          plt.annotate(str(int(prc)) + '%',
                        (bar.get_x() + bar.get_width() / 2., bar.get_height()),
                        ha='center', va='center',
                        xytext=(0, 9),
                        textcoords='offset points')
    
    
    def plot_numeric(data=pd.DataFrame, column=str, title=str):
        fig, ax = plt.subplots(1, 2)
        fig.set_size_inches(20, 7)
    
        #with respect to the target
        healthy = data.loc[data.disease == 0, column]
        sick = data.loc[data.disease == 1, column]
        healthy.plot.density(ax=ax[0])
        sick.plot.density(ax=ax[0])
        ax[0].legend(['healthy', 'heart disease'])
    
        data.boxplot(by='disease', column= [column], ax=ax[1])
    
        fig.suptitle(title, fontsize = 19)
    
    
    def describe_numeric(data=pd.DataFrame, column=str):
        temp = data[[column, 'disease']].copy()
        temp['healthy'] = data[data.disease == 0][column]
        temp['sick'] = data[data.disease == 1][column]
        return temp[[column, 'healthy', 'sick']].describe()
    
    def plot_missing(data = pd.DataFrame):
      na_values_percent = data.isna().sum().sort_values(ascending=False) \
        .apply(lambda x: (x, round(x / data.index.size * 100, 2))) # (count, %)
      na_values_percent.apply(lambda x: x[1]).plot.bar() # (plot %)
      plt.ylabel("Percentage", size=14)
      plt.title('Missing values') __

### Numeric data summary

Code

    
    
    df_numeric=df_eda.loc[:,['age','trestbps','chol','thalach','oldpeak']]
    df_numeric.describe() __

| age | trestbps | chol | thalach | oldpeak  
---|---|---|---|---|---  
count | 920.000000 | 861.000000 | 890.000000 | 865.000000 | 858.000000  
mean | 53.510870 | 132.132404 | 199.130337 | 137.545665 | 0.878788  
std | 9.424685 | 19.066070 | 110.780810 | 25.926276 | 1.091226  
min | 28.000000 | 0.000000 | 0.000000 | 60.000000 | -2.600000  
25% | 47.000000 | 120.000000 | 175.000000 | 120.000000 | 0.000000  
50% | 54.000000 | 130.000000 | 223.000000 | 140.000000 | 0.500000  
75% | 60.000000 | 140.000000 | 268.000000 | 157.000000 | 1.500000  
max | 77.000000 | 200.000000 | 603.000000 | 202.000000 | 6.200000  
  
Code

    
    
    dcorr=df_numeric.corr(method='pearson')
    sns.heatmap(data=dcorr,annot=True,fmt=".2f") __
    
    
    <AxesSubplot: >

![](Heart_disease_project_files/figure-html/cell-7-output-2.png)

### Categorical data summary

Code

    
    
    plt.figure(figsize=(5,5),dpi=200)
    for (i, col) in enumerate(['sex','cp','fbs','restecg','exang', 'disease', 'thal', 'slope']):
      plt.subplot(3,3,i+1)
      df_eda[col].value_counts().sort_index().plot(kind='pie', figsize=(7,5), autopct='%1.1f%%', title = col, textprops={'fontsize': 5}) __

![](Heart_disease_project_files/figure-html/cell-8-output-1.png)

### Analysing risk factors for heart disease

#### Cholesterol (‘chol’)

Cholesterol has a lot of 0 values, which is not possible.

Code

    
    
    df_eda.chol.hist(bins=20)
    df_eda['chol'] = df_eda['chol'].replace({0:np.nan}) __

![](Heart_disease_project_files/figure-html/cell-9-output-1.png)

##### Analysis

People with heart disease on average have a higher cholesterol level.

Code

    
    
    plot_numeric(df_eda, 'chol', 'Cholesterol') __

![](Heart_disease_project_files/figure-html/cell-10-output-1.png)

Code

    
    
    describe_numeric(df_eda, 'chol') __

| chol | healthy | sick  
---|---|---|---  
count | 718.000000 | 372.000000 | 346.000000  
mean | 246.832869 | 240.158602 | 254.008671  
std | 58.527062 | 55.767559 | 60.620439  
min | 85.000000 | 85.000000 | 100.000000  
25% | 210.000000 | 204.000000 | 216.000000  
50% | 239.500000 | 233.000000 | 248.000000  
75% | 276.750000 | 270.250000 | 284.750000  
max | 603.000000 | 564.000000 | 603.000000  
  
Binning shows that people with cholesterol level more than 254 mg/dl have more
than 50% chance of having a heart disease.

Code

    
    
    df_eda['chol_range'] = pd.qcut(df_eda['chol'], 10, duplicates = 'drop')
    plot_categorical(df_eda, 'chol_range', df_eda.chol_range.unique().sort_values(), title = 'Cholesterol intervals') __

![](Heart_disease_project_files/figure-html/cell-12-output-1.png)

#### Age

People with heart disease on average are about 5 years older than healthy
people.

Code

    
    
    df_eda.age.hist(bins=20) __
    
    
    <AxesSubplot: >

![](Heart_disease_project_files/figure-html/cell-13-output-2.png)

Code

    
    
    describe_numeric(df_eda, 'age') __

| age | healthy | sick  
---|---|---|---  
count | 920.000000 | 411.000000 | 509.000000  
mean | 53.510870 | 50.547445 | 55.903733  
std | 9.424685 | 9.433700 | 8.718959  
min | 28.000000 | 28.000000 | 31.000000  
25% | 47.000000 | 43.000000 | 51.000000  
50% | 54.000000 | 51.000000 | 57.000000  
75% | 60.000000 | 57.000000 | 62.000000  
max | 77.000000 | 76.000000 | 77.000000  
  
Code

    
    
    plot_numeric(df_eda, 'age', 'age') __

![](Heart_disease_project_files/figure-html/cell-15-output-1.png)

We can see that the risk of getting heart disease increases after the age of
54.

Code

    
    
    df_eda['age_range'] = pd.qcut(df_eda['age'], 10)
    plot_categorical(df_eda, 'age_range', df_eda.age_range.unique().sort_values()) __

![](Heart_disease_project_files/figure-html/cell-16-output-1.png)

#### Resting blood pressure (‘trestbps’)

Code

    
    
    df_eda.trestbps.hist(bins=20) __
    
    
    <AxesSubplot: >

![](Heart_disease_project_files/figure-html/cell-17-output-2.png)

There’s one unrealistic value of 0, I replace it with na for analysis.

Code

    
    
    df_eda.trestbps.replace({0:np.nan}, inplace=True) __

People with heart disease on average have a slightly higher blood pressure
than healthy patients.

Code

    
    
    plot_numeric(df_eda, 'trestbps', 'Resting blood pressure') __

![](Heart_disease_project_files/figure-html/cell-19-output-1.png)

Code

    
    
    describe_numeric(df_eda, 'trestbps') __

| trestbps | healthy | sick  
---|---|---|---  
count | 860.000000 | 391.000000 | 469.000000  
mean | 132.286047 | 129.913043 | 134.264392  
std | 18.536175 | 16.869867 | 19.617889  
min | 80.000000 | 80.000000 | 92.000000  
25% | 120.000000 | 120.000000 | 120.000000  
50% | 130.000000 | 130.000000 | 130.000000  
75% | 140.000000 | 140.000000 | 145.000000  
max | 200.000000 | 190.000000 | 200.000000  
  
Most patients with normal blood pressure do not have heart disease, while most
patients with elevated blood pressure have heart disease.

Code

    
    
    bins = [0, 120, 130, 140, np.inf]
    labels = ['Normal', 'Elevated', 'High', 'Critically high']
    df_eda['bp_range'] = pd.cut(df_eda['trestbps'], bins)
    plot_categorical(df_eda, 'bp_range', labels) __

![](Heart_disease_project_files/figure-html/cell-21-output-1.png)

#### Exercise induced angina (‘exang’)

Most of the patients with heart disease experienced angina during the
exercise, while the majority in the healthy group had no such symptoms.

Code

    
    
    plot_categorical(df_eda, 'exang', ['no', 'yes']) __

![](Heart_disease_project_files/figure-html/cell-22-output-1.png)

#### Number of Blocked Vessels (‘ca’)

The chance of having heart disease increases proportionally to the number of
blocked vessels. Patients with 0 blocked vessels have only 27% chance of
having heart disease. The value reaches 85% chance for the group with 3
blocked vessels.

Code

    
    
    labels=["0", "1", "2", "3"]
    plot_categorical(df_eda, 'ca', labels) __

![](Heart_disease_project_files/figure-html/cell-23-output-1.png)

#### Gender

The majority of men in the dataset have heart disease, while only 26% of women
are unhealthy.

Code

    
    
    plot_categorical(df_eda, 'sex', ['women', 'men']) __

![](Heart_disease_project_files/figure-html/cell-24-output-1.png)

#### Chest pain type (‘cp’)

Amongst the patients with no chest pain almost 80% had heart disease. Patients
with the atypical angina had the lowest level of heart disease rate. Overall,
we can not say if a chest pain can be considered as a risk factor for a heart
disease.

Code

    
    
    labels=['typical angina', 'atypical angina', 'non-anginal pain', 'no pain']
    plot_categorical(df_eda, 'cp', labels) __

![](Heart_disease_project_files/figure-html/cell-25-output-1.png)

#### Fasting blood sugar

In the dataset most people don’t have a high sugar level. But among those who
do, almost 70% have heart disease.

Code

    
    
    plot_categorical(df_eda, 'fbs', ['Normal', 'High']) __

![](Heart_disease_project_files/figure-html/cell-26-output-1.png)

#### Resting electrocardiographic results (‘restecg’)

Patients in the categories with type 1 and type 2 abnormalities have higher
chance of getting a heart disease than the group with normal ECG.

Code

    
    
    labels=['normal', 'type 1', 'type 2']
    plot_categorical(df_eda, 'restecg', labels) __

![](Heart_disease_project_files/figure-html/cell-27-output-1.png)

#### Maximum heart rate achieved (‘thalach’)

Code

    
    
    df_eda.thalach.plot.hist(bins=20) __
    
    
    <AxesSubplot: ylabel='Frequency'>

![](Heart_disease_project_files/figure-html/cell-28-output-2.png)

Patients without heart disease are able to reach a higher maximum heart rate
than patients with the disease.

Code

    
    
    plot_numeric(df_eda, 'thalach', 'Maximum heart rate achieved') __

![](Heart_disease_project_files/figure-html/cell-29-output-1.png)

#### ST depression (oldpeak)

Code

    
    
    df_eda.oldpeak.hist() __
    
    
    <AxesSubplot: >

![](Heart_disease_project_files/figure-html/cell-30-output-2.png)

There are some negative values. Replacing with nan.

Code

    
    
    df_eda['oldpeak'] = df_eda['oldpeak'].apply(lambda x: np.nan if x < 0 else x)
    df_eda.oldpeak.hist() __
    
    
    <AxesSubplot: >

![](Heart_disease_project_files/figure-html/cell-31-output-2.png)

Oldpeak is another ECG parameter measuring ST depression during the exercise.
It represents a distance on the ECG plot between specific points. There’s a
notable difference in distributions between the groups. Sick people on average
have higher value of the parameter.

Code

    
    
    plot_numeric(df_eda, 'oldpeak', 'oldpeak') __

![](Heart_disease_project_files/figure-html/cell-32-output-1.png)

Code

    
    
    describe_numeric(df_eda, 'oldpeak') __

| oldpeak | healthy | sick  
---|---|---|---  
count | 846.000000 | 387.000000 | 459.000000  
mean | 0.906265 | 0.425840 | 1.311329  
std | 1.071192 | 0.712184 | 1.153295  
min | 0.000000 | 0.000000 | 0.000000  
25% | 0.000000 | 0.000000 | 0.000000  
50% | 0.500000 | 0.000000 | 1.200000  
75% | 1.500000 | 0.800000 | 2.000000  
max | 6.200000 | 4.200000 | 6.200000  
  
Patients with high olpeak values have higher chance of having herat disease.

Code

    
    
    df_eda['oldpeak_range'] = pd.qcut(df_eda['oldpeak'], 5, duplicates = 'drop')
    plot_categorical(df_eda, 'oldpeak_range', df_eda.oldpeak_range.unique().sort_values(), title = 'oldpeak') __

![](Heart_disease_project_files/figure-html/cell-34-output-1.png)

#### The slope of the peak exercise ST segment (‘slope’)

This is another ECG parameter, measured during the exercise. Almost 80% of the
patients with ‘flat’ or ‘downslopping’ slope parameter had heart disease. Most
of the people with the ‘upslopping’ slope were healthy.

Code

    
    
    labels=["upslopping", "flat", "downslopping"]
    plot_categorical(df_eda, 'slope', labels) __

![](Heart_disease_project_files/figure-html/cell-35-output-1.png)

#### Thalassemia blood disorder (‘thal’)

People with this blood disorder are at risk of having heart disease with
almost 80% for both type 1 and type 2 disorders.

Code

    
    
    labels=['normal','type 1','type 2']
    plot_categorical(df_eda, 'thal', labels) __

![](Heart_disease_project_files/figure-html/cell-36-output-1.png)

### Numeric attributes relationships

Younger patients are able to achieve higher maximum heart rate. Patients with
heart disease are older and have lower maximum heart rate.

Code

    
    
    sns.pairplot(df_eda, hue='disease', vars=['age', 'thalach']); __

![](Heart_disease_project_files/figure-html/cell-37-output-1.png)

Blood pressure is higher in older patients. But these factors do not form a
clear separation between healthy and sick patients.

Code

    
    
    sns.pairplot(df_eda, hue='disease', vars=['age', 'trestbps']); __

![](Heart_disease_project_files/figure-html/cell-38-output-1.png)

### EDA colclusion

Exploratory data analysis identified the following groups with a high risk of
heart disease:

  * Patients with high cholesterol
  * Patients older than 54 years old
  * Male patients
  * Patients with high blood sugar
  * Patients with abnormalities in their ECG
  * Patient low maximum heart rate
  * Patients with high oldpeak value
  * Patients with slope that flat or downsloping
  * Patients with blocked heart vessels
  * Patients with thalassemia blood disorder
  * Patients with high blood pressure, more than 120 mmHg

## Data quality

### Incorrect values

There are some incorrect values in the dataset as it was discovered.

  * Cholesterol (‘chol’) has lots of zeros.
  * ST depression (‘oldpeak’) has some negative values. In the existing research this parameter is >= 0.
  * Resting blood pressure (‘trestbps’) has one zero value.

### Missing values

Count incorrect values as missing values.

Code

    
    
    df_missing = df.copy()
    df_missing.chol = df.chol.replace({0:np.nan})
    df_missing.trestbps = df.trestbps.replace({0:np.nan})
    df_missing.loc[df.oldpeak < 0, 'oldpeak'] = np.nan
    
    plot_missing(df_missing) __

![](Heart_disease_project_files/figure-html/cell-39-output-1.png)

# Data Preparation

## Data cleaning

Removing incorrect values.

Code

    
    
    def remove_incorrect(data=pd.DataFrame):
      copy=df.copy()
      copy.chol.replace(0,np.nan,inplace=True)
      copy.trestbps.replace(0,np.nan,inplace=True)
      copy.loc[copy['oldpeak'] < 0, "oldpeak"] = np.nan
      return copy
    
    df_clean = remove_incorrect(df)
    
    #plot the result
    fig, axs = plt.subplots(2, 2, figsize=(10, 10))
    
    #chol
    sns.histplot(data=df, x="chol", ax = axs[0,0])
    axs[0, 0].title.set_text('Cholesterol raw values')
    sns.histplot(data=df_clean, x="chol", ax = axs[0,1])
    axs[0, 1].title.set_text('Cholesterol removed zeros')
    
    #oldpeak
    sns.histplot(data=df, x="oldpeak", ax = axs[1,0])
    axs[1, 0].title.set_text('Oldpeak raw values')
    sns.histplot(data=df_clean, x="oldpeak", ax = axs[1,1])
    axs[1, 1].title.set_text('Oldpeak removed negative values') __

![](Heart_disease_project_files/figure-html/cell-40-output-1.png)

Deleting columns with more than 30% missing data

Code

    
    
    df_reduced = df_clean.drop(['ca', 'thal', 'slope'],axis=1)
    df_reduced.head(1) __

| age | sex | cp | trestbps | chol | fbs | restecg | thalach | exang | oldpeak
| disease  
---|---|---|---|---|---|---|---|---|---|---|---  
0 | 32.0 | 1.0 | 1.0 | 95.0 | NaN | NaN | 0.0 | 127.0 | 0.0 | 0.7 | 1  
  
Removing duplicate rows.

Code

    
    
    duplicates = df_reduced.loc[df_reduced.duplicated(), :] 
    df_no_duplicates = df_reduced.drop_duplicates(keep='first') 
    duplicates __

| age | sex | cp | trestbps | chol | fbs | restecg | thalach | exang | oldpeak
| disease  
---|---|---|---|---|---|---|---|---|---|---|---  
613 | 58.0 | 1.0 | 3.0 | 150.0 | 219.0 | 0.0 | 1.0 | 118.0 | 1.0 | 0.0 | 1  
728 | 49.0 | 0.0 | 2.0 | 110.0 | NaN | 0.0 | 0.0 | 160.0 | 0.0 | 0.0 | 0  
  
### Outliers

Code

    
    
    def find_outliers(data=pd.DataFrame):
        return [col for col in data.columns if has_outliers(data[col])]
    
    def get_box_limits(col=pd.Series):
        q1 = col.quantile(.25)
        q3 = col.quantile(.75)
        IQR = q3 - q1
        ll = q1 - (1.5*IQR)
        ul = q3 + (1.5*IQR)
        return (ll, ul)
    
    def has_outliers(col=pd.Series):
        ll, ul = get_box_limits(col)
        upper_outliers = col[col > ul].count() > 0
        lower_outliers = col[col < ll].count() > 0
        return upper_outliers or lower_outliers
    
    outliers_columns = find_outliers(df_no_duplicates[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']])
    
    fig, axs = plt.subplots(1, len(outliers_columns), figsize = (20, 7))
    for (i, col) in enumerate(outliers_columns):
       df_no_duplicates[col].plot.box(ax = axs[i], fontsize = 18) __

![](Heart_disease_project_files/figure-html/cell-43-output-1.png)

These values are real, and removing them might affect the modeling, so I keep
them.

## Statistical significance testing

The best resource about univariate and bivariate statistical analysis basics:

  * [univariate](https://www.youtube.com/playlist?list=PLqzoL9-eJTNDVNYtKrBKuFnshUoBGY593)
  * [bivariate](https://www.youtube.com/playlist?list=PLqzoL9-eJTNBq-C2sh46hYIlZYJ0Z1cIB)

### Chi square test for categorical data

Code

    
    
    from pandas.core.strings.accessor import isna
    from itertools import combinations
    from scipy.stats import chi2_contingency
    
    def test(col1=pd.Series, col2=pd.Series, alpha=float):
      data_cont=pd.crosstab(col1, col2)
      p_value = chi2_contingency(data_cont)[1]
      return p_value > alpha
    
    def color_df(x):
      if x == True:
        return 'color: %s' % 'red'
      elif isna(x): return None
      else: return 'color: %s' % 'green'
    
    def show_chi_square_results(data=pd.DataFrame):
      categorical = ['sex','cp','fbs','restecg','exang', 'disease']
      cmb = list(combinations(categorical, 2))
    
      chi_square_results_df = pd.DataFrame(index = categorical, columns = categorical)
      for c in cmb:
        res = test(data[c[0]], data[c[1]], 0.05)
        chi_square_results_df.loc[c[0], c[1]] = res
        chi_square_results_df.loc[c[1], c[0]] = res
    
      return chi_square_results_df.style.applymap(color_df)
    
    show_chi_square_results(df_no_duplicates) __

| sex | cp | fbs | restecg | exang | disease  
---|---|---|---|---|---|---  
sex | nan | False | False | True | False | False  
cp | False | nan | True | False | False | False  
fbs | False | True | nan | False | True | False  
restecg | True | False | False | nan | False | False  
exang | False | False | True | False | nan | False  
disease | False | False | False | False | False | nan  
  
The table demonstrates if a variable is statistically independent from another
variable(label ‘True’). There are no independent variables for the target
(disease) column.

### T-test for numeric features

Two-tailed two sample t-testing is performed.

**Null Hypothesis:** The means of features for patients with heart disease and
healthy patients are the same.

**Alternative hypothesis:** There are statistically significant differences in
the feature means for the healthy and sick patients.

Code

    
    
    from scipy.stats import ttest_ind
    
    def test_numeric(data = pd.DataFrame, col=str):
        data = data[~data[col].isna()]
        _, p = ttest_ind(data[col], data['disease'], equal_var=False)
        return p
    
    def show_ttest_results(data=pd.DataFrame):
      results = [(col, test_numeric(data, col))  for col in ['chol', 'trestbps', 'age', 'oldpeak', 'thalach']]
      return pd.DataFrame(results, columns = ['Column', 'p-value']) __

Code

    
    
    show_ttest_results(df_no_duplicates) __

| Column | p-value  
---|---|---  
0 | chol | 0.000000e+00  
1 | trestbps | 0.000000e+00  
2 | age | 0.000000e+00  
3 | oldpeak | 9.389716e-19  
4 | thalach | 0.000000e+00  
  
I reject null hypothesis and state that there are statistically significant
differences in the feature means for the healthy and sick patients. Keeping
all the features.

## Split train/test

70% train, 30% test

Code

    
    
    train_data = df_no_duplicates.sample(frac=0.7, random_state=123)
    test_data = df_no_duplicates[~df_no_duplicates.index.isin(train_data.index)] __

### Data imputation

Code

    
    
    from scipy.stats import mode
    from functools import partial
    
    def impute(data,cols,impute_fn):
        imputed = data.copy()
        for col in cols:
            imputed[col + "_missing"] = imputed[col].isna().astype(int) # flag variable for missing 
            imputed[col] = impute_fn(imputed[col])#.fillna(imputed[col].mode()[0]) 
        return imputed
    
    def impute_categorical(data=pd.DataFrame) -> pd.DataFrame:
        cols = ['restecg', 'exang', 'fbs']
        impute_fn = lambda x: x.fillna(x.mode()[0])
        return impute(data,cols,impute_fn)
        
    
    def impute_numeric(data=pd.DataFrame, cols=list) -> pd.DataFrame:
        cols = ['oldpeak', 'trestbps', 'thalach', 'chol']
        impute_fn = lambda x: x.fillna(x.median())
        return impute(data,cols,impute_fn)
    
    def plot_numeric_imputation(data_before=pd.DataFrame, data_after=pd.DataFrame):
      cols = ['oldpeak', 'trestbps', 'thalach', 'chol']
      fig, axs = plt.subplots(1, len(cols), figsize = (25, 5))
      for i, col in enumerate(cols):
        sns.kdeplot(data_before[col], color='b', fill=True, ax = axs[i], alpha = 0.07)
        sns.kdeplot(data_after[col], color='r', fill=True, ax = axs[i], alpha = 0.07)
        axs[i].legend(['Before imputation', 'After imputation'])
    
    def plot_categorical_imputation(data_before=pd.DataFrame, data_after=pd.DataFrame):
      cols = ['restecg', 'exang', 'fbs']
      fig, axs = plt.subplots(1, len(cols), figsize = (25, 5))
      for i, col in enumerate(cols):
        sns.histplot(data=data_before, x=col, ax=axs[i], color='b')
        sns.histplot(data=data_after, x=col, ax=axs[i], color='r', alpha=0.2)
        
    plot_numeric_imputation(train_data, impute_numeric(train_data))
    plot_categorical_imputation(train_data, impute_categorical(train_data)) __

![](Heart_disease_project_files/figure-html/cell-48-output-1.png)

![](Heart_disease_project_files/figure-html/cell-48-output-2.png)

## One hot encoding

Code

    
    
    def encode_dummy(data = pd.DataFrame, drop_first=True) -> pd.DataFrame:
      df = data.copy()
      for col in ['cp', 'restecg']:
        dummy = pd.get_dummies(df[col], prefix = col,drop_first=drop_first)
        df = pd.concat([df, dummy], axis=1)
        df.drop(col, axis=1, inplace=True)
      return df
    
    encode_dummy(train_data).head(5) __

| age | sex | trestbps | chol | fbs | thalach | exang | oldpeak | disease |
cp_2.0 | cp_3.0 | cp_4.0 | restecg_1.0 | restecg_2.0  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
349 | 47.0 | 1.0 | 112.0 | 204.0 | 0.0 | 143.0 | 0.0 | 0.1 | 0 | 0 | 0 | 1 | 0
| 0  
654 | 38.0 | 1.0 | 140.0 | 297.0 | 0.0 | 150.0 | 0.0 | 0.0 | 0 | 1 | 0 | 0 | 0
| 0  
7 | 38.0 | 1.0 | 115.0 | NaN | 0.0 | 128.0 | 1.0 | 0.0 | 1 | 0 | 1 | 0 | 0 | 0  
571 | 54.0 | 1.0 | NaN | 182.0 | 0.0 | NaN | NaN | NaN | 0 | 1 | 0 | 0 | 1 | 0  
171 | 65.0 | 0.0 | 140.0 | 417.0 | 1.0 | 157.0 | 0.0 | 0.8 | 0 | 0 | 1 | 0 | 0
| 1  
  
## Feature Scaling

Normalization/Standardization

Machine learning algorithms like linear regression, logistic regression,
neural network, etc. that use gradient descent as an optimization technique
require data to be scaled.  
Distance algorithms like KNN, K-means, and SVM are most affected by the range
of features Normalization/Standardization.Therefore, I scale the data before
employing a distance based algorithm so that all the features contribute
equally to the result.

Code

    
    
    from sklearn.preprocessing import MinMaxScaler
    
    def scale(data=pd.DataFrame):
      return pd.DataFrame(MinMaxScaler().fit_transform(data), columns=data.columns)
    
    def plot_normalization(data_before=pd.DataFrame, data_after=pd.DataFrame, cols=str):
      fig, axs = plt.subplots(len(cols), 2, figsize = (25, 15))
      for i, col in enumerate(cols):
        sns.histplot(data_before[col], color='b', ax = axs[i, 0])
        axs[i, 0].title.set_text(col + ' before scaling')
        sns.histplot(data_after[col], color='r', ax = axs[i, 1])
        axs[i, 1].title.set_text(col + ' after scaling')
    
    plot_normalization(train_data, scale(train_data), ['chol', 'age', 'sex']) __

![](Heart_disease_project_files/figure-html/cell-50-output-1.png)

# Modeling

Helper functions

    
    
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    from sklearn.linear_model import LogisticRegression
    
    def run_modeling(model,train_data,test_data):
        x_train = train_data.drop('disease', axis=1)
        y_train = train_data['disease']
        model.fit(x_train, y_train)
          
        x_test = test_data.drop('disease', axis=1)
        y_test = test_data['disease']
        y_model = model.predict(x_test)
        return accuracy_score(y_test, y_model), model
    
    
    def impute_all(data):
        return impute_categorical(impute_numeric(data))
    
    def full_pipeline(data):
        data = impute_all(data)
        data = encode_dummy(data)
        return scale(data) __

## Random forest

    
    
    rf_train_data  = impute_all(train_data)
    rf_test_data  = impute_all(test_data)
    
    acc, rf_model = run_modeling(RandomForestClassifier(random_state=0), rf_train_data,rf_test_data)
    print(f'accuracy: {acc}') __
    
    
    accuracy: 0.7527272727272727

## Logistic regression

    
    
    lr_train_data = full_pipeline(train_data)
    lr_test_data = full_pipeline(test_data)
        
    
    acc, model = run_modeling(LogisticRegression(), lr_train_data, lr_test_data)
    print(f'accuracy: {acc}') __
    
    
    accuracy: 0.7781818181818182

## SVM

There are many model parameters and they are not easy to choose, so I use the
GridSearchCV tool in sklearn to help to complete the parameter selection. The
main parameters include kernel, C and gamma

    
    
    from sklearn import svm
    from sklearn.model_selection import GridSearchCV
    
    parameters = {'kernel':('linear', 'rbf'), 'C':[1], 'gamma':[0.05, 0.07, 0.125, 0.25, 0.5]}
    model = GridSearchCV(svm.SVC(), parameters, scoring='accuracy')
    
    svm_train_data = full_pipeline(train_data)
    svm_test_data = full_pipeline(test_data)
    
    acc, model = run_modeling(model, svm_train_data, svm_test_data)
    print(f'accuracy: {acc}') __
    
    
    accuracy: 0.7781818181818182

## Interpretation

I use information from the random forest feature importance to do
interpretation.

From the random forest feature importance information I conclude that the most
important variables for predicting heart disease are:

  * Chest pain type (cp)
  * Maximum heart rate achieved (thalach)
  * Oldpeak
  * Age
  * Cholesterol

Meanwhile such parameters as blood sugar and the results of the
electrocardiogram contributed the least to the heart disease prediction in the
random forest model.

The most of the missing values flags were not important for the model.

Code

    
    
    (pd.Series(rf_model.feature_importances_, index=rf_train_data.drop(['disease'], axis=1).columns).sort_values().plot(kind='barh', figsize=(10,10))) __
    
    
    <AxesSubplot: >

![](Heart_disease_project_files/figure-html/cell-55-output-2.png)

[ Engineering notes ](./index.html)

  * [ Home](./index.html)
  * [ Proejcts](./projects.html)
  * [ Blog](./blog.html)

## On this page

  * About
  * Skills
  * Experience
  * Education

![](photo.jpg)

# Viktor Anchutin

[ LinkedIn ](https://www.linkedin.com/in/viktor-anchutin/) [ GitHub
](https://github.com/ViktorAnchutin)

# About

Software engineer passionate about machine learning and data intensive
applications.

I’ve worked as a robotics software developer using C and C++ before
transitioning to backend engineering with Kotlin/Java, where I’ve helped to
build various data intensive systems. I’m currently finishing my second
master’s degree in the field of Data Science.

# Skills

**Programming languages** : Kotlin, Java, Scala, Python, C++,C

**ML** : PyTorch, Deep Learning, Statistical Learning, Numerical Optimization

**Databases** : PostgreSQL, Amazon DynamoDB, ElasticSearch

**Data processing** : Kafka, Spark, AWS SQS, AWS Lambda

**Software engineering** : Object Oriented Programming, Functional
Programming, Software Architecture Patterns, Testing and Monitoring,
Concurrent programming

**Robotics** Real-Time Control Systems, Position Estimation

# Experience

**Java/Kotlin software engineer**  
Grid Dynamics, Belgrade, Serbia, May 2021 - Aug 2022  

**Java software engineer**  
X5 Group, Moscow, Russia, Dec 2019 - Aug 2021  

**Robotics software engineer**  
UVS YURION, Moscow, Russia, Oct 2018 - Dec 2019  

**Embedded software engineer**  
Central Research Institute “Cyclone”, Moscow, Jul 2018 - Oct 2018  

# Education

**Master’s degree in Data Science**  
University Malaya, Kuala Lumpur, Malaysia, Sep 2022 - Jan 2024  
Research project: [Active emotion
detection](./projects/Emotional_speech_detection.html)  

**Master’s degree in Robotics**  
BMSTU, Moscow, Russia, 2016 - 2018  
Final project: [Electric motor control](./projects/motor_control.html)

**Bachelor’s degree in Control Systems and Automation Technology**  
MGSU, Moscow, Russia, 2012 - 2016

[ Engineering notes ](../index.html)

  * [ Home](../index.html)
  * [ Proejcts](../projects.html)
  * [ Blog](../blog.html)

# Controlling magnetic field in electric motors

How I built BLDC motor control system from scratch in my Robotics masters
degree

During my time in Robotics master’s program I worked on building software to
control electric motors position. All projects were built in C and C++. Some
hardware parts had to be developed from scratch as well.

The basic elements of a system were:

  * **_Microcontroller (STM32)_**. Executing software to control magnetic field. (STM32)
  * **_Hardware module (Driver)_**. Performing commands from the microcontroller for controlling voltage.
  * **_Magnetic encoder_**. Measuring position of the rotor and sending it back to the microcontroller.
  * **_Motor (BLDC)_**

![](https://raw.githubusercontent.com/ViktorAnchutin/BLDC_CONTROL/master/graph/Structure.JPG)

By the end of my master’s degree I had built various systems from controlling
a single motor to controlling a feedback loop stabilization system.

The first project
[implemented](https://github.com/ViktorAnchutin/BLDC_CONTROL) vector control
or also called field oriented control (FOC) in C language. Later I
[reimplemented](https://github.com/ViktorAnchutin/motorControl) it in C++.
![](https://raw.githubusercontent.com/ViktorAnchutin/motorControl/master/pictures/motorControl.PNG)

One of the fun things to implement was slow rotation of the motor, which was
done using [sinusoidal voltage
control](https://www.youtube.com/shorts/BCIBsAJhNY4), meaning that the
magnetic field was slowly and gradually rotated.

Eventually the [robotics club](https://github.com/bauman-robotics) in my
university built a platform with a more powerful motor from a hoverboard, and
I implemented the same system for it as well.

In my last year I put all the pieces together and
[built](https://github.com/ViktorAnchutin/Stabilized_platform) a stabilization
platform. It used a gyroscope and accelerometer to measure its position.

Finally I gave a seminar about motor control algorithms

[ Engineering notes ](../index.html)

  * [ Home](../index.html)
  * [ Proejcts](../projects.html)
  * [ Blog](../blog.html)

## On this page

  * Framework for running the experiments
  * Data
  * Baseline model | CNN-Transformer
  * Vision transformer
  * Pretrained CNN model
  * Wav2Vec 2.0
  * Hybrid
  * Losses analysis

# Detecting active emotions in speech

Training 5 different models to perform active emotion recognition from speech

[![drawing](./images/gh.png)](https://github.com/ViktorAnchutin/detecting_active_emotion)

Init code

    
    
    !pip install transformers --quiet;
    !pip install timm --quiet;
    
    import pandas as pd
    from functools import partial
    from itertools import chain
    import torch
    from torch import nn
    import gc
    from data.utils import *
    from transformers import Wav2Vec2Model
    from experiments.models import W2V2Model, ConvNextModel
    from data.dataloader import create_dataloader, MutiDataLoader
    from framework.learning_rate import ExponentialLRCalculator,CosineLRCalculator
    from experiments.experiment_runner import Training, TrainingConfig
    from matplotlib import pyplot as plt
    import torchaudio
    from tqdm import tqdm
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    w2v_features_path = 'w2v_features' __

In this project I build an emotional speech detector. The goal is to train the
model which can recognize active emotion in a speech segment.

![](./images/actemo.png)

Emotions are categorized using the arousal-valence model. Arousal axis
measures how active emotion is and valence measures whether emotion is
positive or negative. I build a detector for the arousal axis, targeting the
top region of the space, where emotions are the most active. One of the
applications for such a model is to apply it to detect highlights in media
content from speech. ![](./images/emotionmodel.png)

I experiment with 5 different model architectures. I train a custom CNN-
Transformer model and compare it to the pretrained models from different
domains, finally I train a hybrid model combining different modalities of
data.

![](./images/experiments.png)

The results are in the table below:

![](./images/results.png)

# Framework for running the experiments

The source code for the framework and experiments is published in a [Github
repository](https://github.com/ViktorAnchutin/detection_active_emotion/tree/main).

For this project I developed a custom deep learning framework to configure and
run the experiments, as well as to preprocess and load the data.

Most of the models in my experiments are able to learn from mini-batches of
different size. For this purpose I developed a
[dataloader](https://github.com/ViktorAnchutin/detecting_active_emotion/blob/7a34326ea81658db5f823740b495b967b64aea78/data/dataloader.py#L21)
which builds mini batches from the data of similar length to avoid padding
overhead. ![](./images/vb.png) The details of the dataloader implementation
are in the blog [post](../blog/variable_length_training.html).

To run the experiments I created class
[Training](https://github.com/ViktorAnchutin/detection_active_emotion/blob/8eecbd2ed6c3079f40357a8290727d1ebcaf2b61/experiments/experiment_runner.py#L29)
which is responsible for initializing and running the training from the
provided
[TrainingConfig](https://github.com/ViktorAnchutin/detection_active_emotion/blob/8eecbd2ed6c3079f40357a8290727d1ebcaf2b61/experiments/experiment_runner.py#L11).
Since some of the models are quite big I added the functionality of mixed
precision training and gradient accumulation.

For evaluation purposes I built a
[module](https://github.com/ViktorAnchutin/detection_active_emotion/blob/8eecbd2ed6c3079f40357a8290727d1ebcaf2b61/framework/metrics.py#L52)
which computes such metrics as ROC-AUC, best F1 score and corresponding
precision and recall.

# Data

![](./images/mspds.png)

I am using the [MSP-
Podcast](https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-
Podcast.html) dataset. It is a large naturalistic speech emotional dataset. It
consists of speech segments from podcast recordings, perceptually annotated
using crowdsourcing.

I split labels into 2 categories for a binary classification task. One
category contains labels corresponding to a high level of emotional
activation: somewhat active, active, very active, the other contains the rest
of the labels.

![](./images/labels.png)

Code

    
    
    data = pd.read_csv("labels_consensus.csv").set_index("FileName")
    data = data[data.SpkrID != 'Unknown'] # segments without a speaker id are dropped
    data['is_active'] = data.apply(lambda x: 1. if x['EmoAct'] >= 5 else 0.,axis=1)
    
    def plot_label(data,label,ax=None,title=None):
      barc = data[label].value_counts().plot.bar(ax=ax,title=title)
      barc.bar_label(barc.containers[0], label_type='edge')
    
    plot_label(data, 'is_active', title='active emotions'); __

![](Emotional_speech_detection_files/figure-html/cell-3-output-1.png)

I set a limit of 500 segments for an individual speaker in order to avoid a
potential overfitting to a specific speaker.

Code

    
    
    speaker_counts = data.groupby('SpkrID').apply(lambda x: pd.Series({'cnt':len(x)}))
    
    def downsample_speaker(data,speaker,limit=500):
      sampled_ids = data[data['SpkrID'] == speaker].sample(limit).index
      todrop = data[(data['SpkrID'] == speaker)&(~data.index.isin(sampled_ids))].index
      return data.drop(todrop)
    
    for sid in speaker_counts[speaker_counts.cnt >= 500].index:
      data = downsample_speaker(data,sid) __

I perform a speaker independent data split with 20% of speakers used for the
test set. There’s no overlap between test and test sets.

Code

    
    
    speakers = data.SpkrID.value_counts()
    
    holdout_speakers = speakers.sample(round(len(speakers)*0.2), random_state=123)
    data.loc[:,'custom_partition'] = data.apply(lambda x: 'Test' if x['SpkrID'] in holdout_speakers.index else 'Train', axis=1)
    
    train_data = data[data.custom_partition == 'Train']
    valid_data = data[data.custom_partition == 'Test']
    
    _,ax = plt.subplots(1,2,figsize=(12, 5));
    plot_label(train_data, 'is_active', ax=ax[0], title='Train data');
    plot_label(valid_data, 'is_active', ax=ax[1], title='Validation data');
    
    data.groupby('custom_partition').apply(lambda x: pd.Series({'unique speakers':len(x.SpkrID.unique())})) __

| unique speakers  
---|---  
custom_partition |  
Test | 286  
Train | 1146  
  
![](Emotional_speech_detection_files/figure-html/cell-5-output-2.png)

The information about the length of audio segments in seconds is collected.

Code

    
    
    files = []
    durations = []
    samples = []
    
    for file_name in tqdm(data.index):
          signal, sr = torchaudio.load(f"Audios/Audio/{file_name}")
          files.append(file_name)
          durations.append(signal.shape[1]/sr)
          samples.append(signal.shape[1])
    
    duration_df = pd.DataFrame({"file":files, "samples":samples, "dur":durations}).set_index('file')
    train_data = train_data.join(duration_df)
    valid_data = valid_data.join(duration_df)
    duration_df.dur.hist(); __

![](Emotional_speech_detection_files/figure-html/cell-6-output-1.png)

Data is grouped into buckets of similar length for more efficient batching
during training.

Code

    
    
    bins=20
    
    train_data['bin'] = pd.cut(train_data.dur, bins=bins, labels=range(bins))
    train_data.loc[train_data['bin']==19, 'bin'] = 18 # merge last bin due to only one record
    
    valid_data['bin'] = pd.cut(valid_data.dur, bins=bins, labels=range(bins)) __

The positive class weight is computed and it will be used in the binary cross
entropy function to account for the class imbalance.

    
    
    weight = len(train_data[train_data['is_active'] == 0])/len(train_data[train_data['is_active'] == 1]) __

# Baseline model | CNN-Transformer

It has been [demonstrated](https://arxiv.org/abs/2106.04803) that combining
convolutions and transformers achieves state of the art results under
different data sizes and computational budgets. In the ViT
[paper](https://arxiv.org/abs/2010.11929) the hybrid CNN-Transformer
outperformed the pure transformer model for smaller computational budgets. I
use CNN-Transformer architecture for the baseline model.

The model diagram:

![drawing](./images/cnntrf.png)

The convolutional neural network first processes F×T log mel-spectrogram and
produces F’ × T’ × C feature map volume, where F represents the frequency
dimension of a spectrogram and equals to the number of mel-filterbanks used, T
- represents the time dimension and it equals to the number of time frames in
a spectrogram, (F’,T’) is the resolution of the produced feature map and C is
the number of channels. Mean pooling is then used in the frequency dimension
to produce the T’C sequence of embeddings. The obtained embeddings are then
mapped to dimension D of the transformer encoder. Class token embedding is
added to the beginning of the sequence. The MLP classification head is then
applied to the class token after the transformer encoding stage.

**_Prepare data_**

![](./images/vsd.png)

To create spectrograms I use a window size of 40ms and a hop step of 20ms with
64 mel-filterbanks. I precomputed the mean and standard deviation of the
dataset, which are used for normalization. The model can be trained on a
variable size data, so I split the data into buckets of similar length and
create a dataloader for variable size data.

Prepare data code

    
    
    window = 640 # 40ms
    hop = 320 # 20 ms
    mels=64
    spctr_binned_dataset_norms = (-10.58189139511126, 14.482822057824656)
    
    spctr_dataset_builder = partial(create_spectrogram_ds,norms=spctr_binned_dataset_norms,w=window,hop=hop,n_mels=mels)
    spctr_ds_train_binned = create_binned_datasets(train_data, spctr_dataset_builder)
    spctr_ds_valid_binned = create_binned_datasets(valid_data, spctr_dataset_builder)
    
    train_dl = MutiDataLoader(spctr_ds_train_binned, partial(create_dataloader, bs=64))
    valid_dl = MutiDataLoader(spctr_ds_valid_binned, partial(create_dataloader, bs=64)) __

Prefetch data to RAM to accelerate training:

Prefetch data code

    
    
    for ds in chain(spctr_ds_train_binned,spctr_ds_valid_binned):
      ds.prefetch(1) __

**_Running the experiment_**

Create CNN

    
    
    from experiments.models import convolution_block, ResidualBlock, CustomConvTransformer
    
    cnn = nn.Sequential(
            convolution_block(1,32),
            nn.MaxPool2d(2),
            ResidualBlock(32, 32),
            nn.MaxPool2d(2),
            ResidualBlock(32, 64),
            nn.MaxPool2d(2),
            ResidualBlock(64, 64),
            nn.MaxPool2d(2),
            ResidualBlock(64, 128),
            nn.MaxPool2d(2)
        ) __

Creating the
[model](https://github.com/ViktorAnchutin/detection_active_emotion/blob/8eecbd2ed6c3079f40357a8290727d1ebcaf2b61/experiments/models.py#L172).
Here, the CNN will produce a volume with 128 channels, which means the
obtained embeddings will have the size of 128. I map them to 256.

    
    
    model = CustomConvTransformer(cnn,conv_emb_dim=128,target_dim=256,layers=3) __

Creating the training configuration and running the experiment:

    
    
    training_config = TrainingConfig(
        fine_tune = False,
        device = device,
        model = model,
        train_dl = train_dl,
        valid_dl = valid_dl,
        optimizer = torch.optim.AdamW,
        weight_decay = 3,
        positive_class_weight = weight,
        lr_calculator = ExponentialLRCalculator(factor=0.5),
        epochs = 5,
        learning_rate = 1e-4/2,
        mixed_precision = True
    )
    
    training = Training(training_config)
    
    training.run() __
    
    
    100%|██████████| 880/880 [00:57<00:00, 15.37it/s]
    100%|██████████| 230/230 [00:07<00:00, 32.28it/s]
    100%|██████████| 880/880 [00:52<00:00, 16.84it/s]
    100%|██████████| 230/230 [00:07<00:00, 31.47it/s]
    100%|██████████| 880/880 [00:51<00:00, 17.14it/s]
    100%|██████████| 230/230 [00:06<00:00, 36.48it/s]
    100%|██████████| 880/880 [00:53<00:00, 16.37it/s]
    100%|██████████| 230/230 [00:05<00:00, 40.80it/s]
    100%|██████████| 880/880 [00:49<00:00, 17.73it/s]
    100%|██████████| 230/230 [00:06<00:00, 33.63it/s]

| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
0 | 0.814238 | 0.773414 | 0.793025 | 0.624 | 0.719368 | 0.550959  
  
| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
1 | 0.771719 | 0.758556 | 0.801348 | 0.633006 | 0.728151 | 0.559851  
  
| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
2 | 0.753767 | 0.758195 | 0.802045 | 0.635189 | 0.73408 | 0.559779  
  
| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
3 | 0.734888 | 0.754444 | 0.803696 | 0.635551 | 0.741546 | 0.556068  
  
| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
4 | 0.725415 | 0.754049 | 0.802665 | 0.635757 | 0.731664 | 0.562078  
  
![](Emotional_speech_detection_files/figure-html/cell-13-output-7.png)

![](Emotional_speech_detection_files/figure-html/cell-13-output-8.png)

# Vision transformer

Similar to [Audio Spectrogram Trasnformer](https://arxiv.org/abs/2104.01778)
approach I use DeiT model. To apply vision transformer to spectrogram data
several adjustments are required.

First, spectrogram data has only 1 channel, but ViT is pretrained on images
with 3 channels. It is adjusted by
[summing](https://github.com/huggingface/pytorch-image-
models/blob/68b2824e49c8a4442ca1c7fb29d3de8135e87c64/timm/models/_manipulate.py#L267C13-L267C63)
the weights of the patch embedding layer to produce the single channel.

Second, ViT is pretrained on images of different resolution, which means
position embeddings need to be adjusted to the new size before fine-tuning. In
the timm library it is [done](https://github.com/huggingface/pytorch-image-
models/blob/68b2824e49c8a4442ca1c7fb29d3de8135e87c64/timm/layers/pos_embed.py#L46)
using 2d interpolation.

And finally, the both DeiT heads are replaced with new ones. The prediction is
computed using late fusion as an average of the predictions from both heads.

More about applying vision models to spectrogram data in my [blog
post](../blog/Applying_vision_models_to_audio_classification.html)

![](./images/adjuststeps.png)

**_Prepare data_**

![](./images/fld.png)

Vision transformer requires data to have fixed size due to positional
embeddings. Positional embeddings have fixed size and theoretically can be
interpolated during forward pass (e.g.
[DINOv2](https://github.com/facebookresearch/dinov2/blob/44abdbe27c0a5d4a826eff72b7c8ab0203d02328/dinov2/models/vision_transformer.py#L200C16-L200C16)),
but generally it is [not recommended](https://github.com/huggingface/pytorch-
image-models/issues/908).

All spectrograms are padded to the same length.

Prepare data code

    
    
    window = 640 # 40ms
    hop = 320 # 20 ms
    mels=64
    maxl = max(get_max_spectrogram_length(train_data,hop), get_max_spectrogram_length(valid_data,hop))
    spctr_norms = (-5.612719667630828, 12.146062730351977)
    spctr_dataset_train_fixed = create_spectrogram_ds(train_data, maxl=maxl,norms=spctr_norms,w=window,hop=hop,n_mels=mels)
    spctr_dataset_valid_fixed = create_spectrogram_ds(valid_data, maxl = maxl,norms=spctr_norms,w=window,n_mels=mels) __

Prefetch data to RAM

Prefetch data code

    
    
    spctr_dataset_train_fixed.prefetch(0.5)
    spctr_dataset_valid_fixed.prefetch(0.5) __

**_Training_**

Mixed precision is used to accelerate the training.

    
    
    from experiments.models import ViTModel
    
    model = ViTModel(im_size=(mels, maxl),
                     name = 'deit_tiny_distilled_patch16_224',
                     dropout=0.3)
    
    vit_training_config = TrainingConfig(
        fine_tune = True,
        device = device,
        model = model,
        train_dl = create_dataloader(spctr_dataset_train_fixed, bs=64),
        valid_dl = create_dataloader(spctr_dataset_valid_fixed, bs=64),
        optimizer = torch.optim.AdamW,
        weight_decay = 1,
        positive_class_weight = weight,
        lr_calculator = ExponentialLRCalculator(factor=0.5),
        epochs = 2,
        head_pretrain_epochs = 1,
        learning_rate = 1e-5,
        head_pretrain_learning_rate = 1e-3,
        mixed_precision = True
    )
    
    training = Training(vit_training_config)
    
    training.run() __
    
    
    100%|██████████| 436/436 [03:28<00:00,  2.09it/s]
    100%|██████████| 111/111 [00:44<00:00,  2.47it/s]
    100%|██████████| 436/436 [03:59<00:00,  1.82it/s]
    100%|██████████| 111/111 [00:44<00:00,  2.49it/s]
    100%|██████████| 436/436 [03:50<00:00,  1.89it/s]
    100%|██████████| 111/111 [00:44<00:00,  2.48it/s]

| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
0 | 0.832366 | 0.810435 | 0.766863 | 0.60327 | 0.704875 | 0.527267  
  
![](Emotional_speech_detection_files/figure-html/cell-16-output-4.png)

![](Emotional_speech_detection_files/figure-html/cell-16-output-5.png)

| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
0 | 0.786857 | 0.841773 | 0.797649 | 0.628731 | 0.742424 | 0.545235  
  
| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
1 | 0.751145 | 0.801809 | 0.796527 | 0.626177 | 0.74484 | 0.540127  
  
![](Emotional_speech_detection_files/figure-html/cell-16-output-8.png)

![](Emotional_speech_detection_files/figure-html/cell-16-output-9.png)

# Pretrained CNN model

I’m using the ‘ConvNeXt’ model. The first layer is adjusted to spectrogram
data by [summing](https://github.com/huggingface/pytorch-image-
models/blob/68b2824e49c8a4442ca1c7fb29d3de8135e87c64/timm/models/_manipulate.py#L267C13-L267C63)
the weights of the input channels to produce a single channel. The head of the
model is replaced with the fully connected layer which computes a single value
for binary classification.

**_Prepare data_**

![](./images/vsd.png)

CNN models can be trained on variable size data, so similar to CNN-Transformer
model I split the data into buckets of similar length and create a dataloader
for variable size data.

Prepare data code

    
    
    window = 640 # 40ms
    hop = 320 # 20 ms
    mels=64
    spctr_binned_dataset_norms = (-10.58189139511126, 14.482822057824656)
    
    spctr_dataset_builder = partial(create_spectrogram_ds,norms=spctr_binned_dataset_norms,w=window,hop=hop,n_mels=mels)
    spctr_ds_train_binned = create_binned_datasets(train_data, spctr_dataset_builder)
    spctr_ds_valid_binned = create_binned_datasets(valid_data, spctr_dataset_builder)
    
    train_dl = MutiDataLoader(spctr_ds_train_binned, partial(create_dataloader, bs=64))
    valid_dl = MutiDataLoader(spctr_ds_valid_binned, partial(create_dataloader, bs=64)) __

Prefetch data to RAM to accelerate the training

Prefetch data code

    
    
    for ds in chain(spctr_ds_train_binned,spctr_ds_valid_binned):
      ds.prefetch(1) __

**_Training_**

    
    
    model = ConvNextModel(name = 'convnext_tiny', dropout=0.2)
    
    training_config = TrainingConfig(
        fine_tune = True,
        device = device,
        model = model,
        train_dl = MutiDataLoader(spctr_ds_train_binned, partial(create_dataloader, bs=64)),
        valid_dl = MutiDataLoader(spctr_ds_valid_binned, partial(create_dataloader, bs=64)),
        optimizer = torch.optim.AdamW,
        weight_decay = 4,
        positive_class_weight = weight,
        lr_calculator = ExponentialLRCalculator(factor=0.5),
        epochs = 3,
        learning_rate = 1e-4/2,
        head_pretrain_learning_rate = 1e-3,
        mixed_precision = True
    )
    
    training = Training(training_config)
    
    training.run() __
    
    
    100%|██████████| 880/880 [00:47<00:00, 18.63it/s]
    100%|██████████| 230/230 [00:11<00:00, 19.28it/s]
    100%|██████████| 880/880 [02:00<00:00,  7.32it/s]
    100%|██████████| 230/230 [00:12<00:00, 18.80it/s]
    100%|██████████| 880/880 [02:00<00:00,  7.28it/s]
    100%|██████████| 230/230 [00:12<00:00, 19.08it/s]
    100%|██████████| 880/880 [01:59<00:00,  7.34it/s]
    100%|██████████| 230/230 [00:11<00:00, 19.31it/s]

| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
0 | 0.878943 | 0.907094 | 0.716213 | 0.571478 | 0.726834 | 0.470839  
  
![](Emotional_speech_detection_files/figure-html/cell-19-output-4.png)

![](Emotional_speech_detection_files/figure-html/cell-19-output-5.png)

| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
0 | 0.800668 | 0.803675 | 0.804821 | 0.641816 | 0.74484 | 0.56383  
  
| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
1 | 0.762559 | 0.750714 | 0.811969 | 0.644576 | 0.741107 | 0.570294  
  
| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
2 | 0.740888 | 0.757503 | 0.808494 | 0.640108 | 0.726834 | 0.571873  
  
![](Emotional_speech_detection_files/figure-html/cell-19-output-9.png)

![](Emotional_speech_detection_files/figure-html/cell-19-output-10.png)

# Wav2Vec 2.0

I fine tune Wav2Vec 2.0 model. The final representation vector of an audio
segment is computed as an average of the sequence of vectors produced by the
model.

I add an MLP classification head on top. I keep the weights of the CNN feature
encoder layer frozen, as it has been [shown](https://arxiv.org/abs/2111.02735)
that it yields better results.

![](./images/wav2vec.png)

**_Prepare data_**

![](./images/w2vp.png)

**_Precompute CNN features_**

The convolutional encoder step consumes a lot of GPU. To avoid it during
training, I preprocess audio segments with CNN encoder beforehand. It is done
by loading a Wav2Vec CNN feature extractor and running a forward pass on it
with batches of data. The results are
[saved](https://github.com/ViktorAnchutin/detection_active_emotion/blob/8eecbd2ed6c3079f40357a8290727d1ebcaf2b61/data/utils.py#L87)
on disk.

Code

    
    
    audios_mean,audios_sdt = (-0.00011539726044066538, 0.07812332811748154)
    raw_audio_datasets_train = create_binned_datasets(train_data, partial(create_audio_ds, norms=(audios_mean,audios_sdt)))
    raw_audio_datasets_valid = create_binned_datasets(valid_data, partial(create_audio_ds, norms=(audios_mean,audios_sdt)))
    
    w2v_feature_extractor = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h").feature_extractor
    
    !mkdir {w2v_features_path}
    
    for ds in chain(raw_audio_datasets_train, raw_audio_datasets_valid):
      extract_w2v_features(ds, w2v_feature_extractor, w2v_features_path, bs=32, device=device) __

**_Create dataset and dataloader_**

Wav2Vec 2.0 transformer encoder is using realtive positional embeddings which
allows it to process data of variable length. The preprocessed audio files
were saved on disk and here I create a dataset which groups these files by
length. The corresponding dataloader is created. The batch size is only 16 as
it is the maximum power of 2 size which fits into Colab’s 16GB GPU memory.

Code

    
    
    w2v2_datasets_train = create_binned_datasets(train_data, partial(create_file_ds, path=w2v_features_path))
    w2v2_datasets_valid = create_binned_datasets(valid_data, partial(create_file_ds, path=w2v_features_path))
    
    w2v2dl_train = MutiDataLoader(w2v2_datasets_train, partial(create_dataloader, bs=16))
    w2v2dl_valid = MutiDataLoader(w2v2_datasets_valid, partial(create_dataloader, bs=16)) __

**_Training_**

I use gradient accumulation since my batch size is only 16. The model expects
CNN encoded audio features as an input.

    
    
    w2v_training_config = TrainingConfig(
        fine_tune = True,
        device = device,
        model = W2V2Model(),
        train_dl = w2v2dl_train,
        valid_dl = w2v2dl_valid,
        optimizer = torch.optim.AdamW,
        weight_decay = 1,
        positive_class_weight = weight,
        lr_calculator = ExponentialLRCalculator(factor=0.5),
        epochs = 2,
        head_pretrain_epochs = 1,
        learning_rate = 1e-5,
        head_pretrain_learning_rate = 1e-3,
        mixed_precision = True,
        gradient_accumulation_size = 64
    )
    
    training = Training(w2v_training_config)
    
    training.run() __
    
    
    Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
    100%|██████████| 3491/3491 [10:30<00:00,  5.54it/s]
    100%|██████████| 892/892 [02:42<00:00,  5.50it/s]
    100%|██████████| 3491/3491 [14:19<00:00,  4.06it/s]
    100%|██████████| 892/892 [02:42<00:00,  5.48it/s]
    100%|██████████| 3491/3491 [14:18<00:00,  4.07it/s]
    100%|██████████| 892/892 [02:40<00:00,  5.56it/s]

| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
0 | 0.932333 | 0.92618 | 0.65685 | 0.512447 | 0.671278 | 0.414396  
  
![](Emotional_speech_detection_files/figure-html/cell-22-output-3.png)

![](Emotional_speech_detection_files/figure-html/cell-22-output-4.png)

| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
0 | 0.78635 | 0.738586 | 0.817297 | 0.654125 | 0.752086 | 0.578743  
  
| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
1 | 0.724516 | 0.74012 | 0.81747 | 0.651802 | 0.72859 | 0.589657  
  
![](Emotional_speech_detection_files/figure-html/cell-22-output-7.png)

![](Emotional_speech_detection_files/figure-html/cell-22-output-8.png)

# Hybrid

The hybrid model uses both: log mel-spectrograms and raw audio data. Log mel-
spectrogram modality is processed by ConvNeXt model, raw audio data is
processed by the Wav2Vec 2.0 model.

Both channels produce 1D vector representations of the input data. In the case
of the ConvNeXt model I apply global average pooling for the obtained feature
maps, for the Wav2Vec 2.0 model I average the representation vectors it
produces. The obtained 1D vectors are then projected to a lower dimension,
concatenated into a single vector and sent to the classification head to
produce the final result. For the Wav2Vec 2.0 model I keep CNN feature encoder
weights frozen.

![](./images/hybrid.png)

**_Prepare data_**

I create variable length datasets for both spectrogram and wav2vec data, since
both models can process variable length data.

![](./images/preprocesshybrid.png)

Create a dataset for wav2vec. First create raw audio dataset, then precompute
CNN encoder layer outputs.

precompute CNN features

    
    
    audios_mean,audios_sdt = (-0.00011539726044066538, 0.07812332811748154)
    raw_audio_datasets_train = create_binned_datasets(train_data, partial(create_audio_ds, norms=(audios_mean,audios_sdt)))
    raw_audio_datasets_valid = create_binned_datasets(valid_data, partial(create_audio_ds, norms=(audios_mean,audios_sdt)))
    
    w2v_feature_extractor = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h").feature_extractor
    
    !mkdir {w2v_features_path}
    
    for ds in chain(raw_audio_datasets_train, raw_audio_datasets_valid):
      extract_w2v_features(ds, w2v_feature_extractor, w2v_features_path, bs=32, device=device) __

Create a dataset from the precomputed features.

Create w2v dataset

    
    
    w2v2_datasets_train = create_binned_datasets(train_data, partial(create_file_ds, path=w2v_features_path))
    w2v2_datasets_valid = create_binned_datasets(valid_data, partial(create_file_ds, path=w2v_features_path)) __

Create a spectrogram dataset

Create spectrogram dataset

    
    
    window = 640 # 40ms
    hop = 320 # 20 ms
    mels=64
    spctr_binned_dataset_norms = (-10.58189139511126, 14.482822057824656)
    
    spctr_dataset_builder = partial(create_spectrogram_ds,norms=spctr_binned_dataset_norms,w=window,hop=hop,n_mels=mels)
    spctr_ds_train_binned = create_binned_datasets(train_data, spctr_dataset_builder)
    spctr_ds_valid_binned = create_binned_datasets(valid_data, spctr_dataset_builder) __

The hybrid dataset wraps spectrogram dataset and Wav2Vec dataset to produce
items with 2 modalities. A variable length dataloader is created. Batch size
is set to only eight items due to GPU memory limitations.

Create hybrid dataset

    
    
    from data.utils import create_hybrid_datasets_binned
    from data.dataloader import collate_hybrid
    
    hybrid_datasets_train = create_hybrid_datasets_binned(w2v2_datasets_train, spctr_ds_train_binned)
    hybrid_datasets_valid = create_hybrid_datasets_binned(w2v2_datasets_valid, spctr_ds_valid_binned)
    
    hybrid_dl_train = MutiDataLoader(hybrid_datasets_train, partial(create_dataloader, collate_fn=collate_hybrid, bs=8))
    hybrid_dl_valid = MutiDataLoader(hybrid_datasets_valid, partial(create_dataloader, collate_fn=collate_hybrid, bs=8)) __

Prefetch spectrogram part of the dataset (wav2vec features take too much
memory) into RAM.

Code

    
    
    for ds in chain(hybrid_datasets_train,hybrid_datasets_valid):
      ds.prefetch(1) __

**_Run the training_**

Gradient accumulation is enabled.

    
    
    from experiments.models import HybridModel, ConvNextModel
    
    spctr_model = ConvNextModel(name = 'convnext_tiny')
    model = HybridModel(spctr_model=spctr_model)
    
    hybrid_training_config = TrainingConfig(
        fine_tune = True,
        device = device,
        model = model,
        train_dl = hybrid_dl_train,
        valid_dl = hybrid_dl_valid,
        optimizer = torch.optim.AdamW,
        weight_decay = 1,
        positive_class_weight = weight,
        lr_calculator = ExponentialLRCalculator(factor=0.5),
        epochs = 2,
        head_pretrain_epochs = 1,
        learning_rate = 1e-5,
        head_pretrain_learning_rate = 1e-3,
        mixed_precision = True,
        gradient_accumulation_size = 64
    )
    
    training = Training(hybrid_training_config)
    
    training.run() __
    
    
    Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
    100%|██████████| 6972/6972 [15:24<00:00,  7.54it/s]
    100%|██████████| 1775/1775 [03:54<00:00,  7.57it/s]
    100%|██████████| 6972/6972 [33:03<00:00,  3.52it/s]
    100%|██████████| 1775/1775 [04:00<00:00,  7.39it/s]
    100%|██████████| 6972/6972 [32:59<00:00,  3.52it/s]
    100%|██████████| 1775/1775 [04:01<00:00,  7.34it/s]

| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
0 | 0.847191 | 0.816372 | 0.766683 | 0.607242 | 0.71805 | 0.526062  
  
![](Emotional_speech_detection_files/figure-html/cell-28-output-3.png)

![](Emotional_speech_detection_files/figure-html/cell-28-output-4.png)

| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
0 | 0.769939 | 0.750812 | 0.82529 | 0.659073 | 0.71783 | 0.609206  
  
| train_loss | test_loss | auc | f1 | recall | precision  
---|---|---|---|---|---|---  
1 | 0.712602 | 0.732861 | 0.821689 | 0.658245 | 0.75516 | 0.583376  
  
![](Emotional_speech_detection_files/figure-html/cell-28-output-7.png)

![](Emotional_speech_detection_files/figure-html/cell-28-output-8.png)

Save metrics for further analysis

Save metrics

    
    
    errors_report = training.trainer.cbs.callbacks[-3].get_errors_report()
    errors_report.to_csv('errors_report.csv') __

# Losses analysis

I analysed loss values for individual segments. Loss values are computed from
the outputs of the hybrid model on the validation dataset.

Code

    
    
    import numpy as np
    import seaborn as sns
    errors_report =  pd.read_csv('errors_report.csv', index_col=0)
    errors_report['errors'] = np.where(errors_report.targ, 1-errors_report.pred, errors_report.pred)
    errors_report = errors_report.set_index('id')
    report = errors_report.join(valid_data) __

Average loss values for different lengths of audio segments

Code

    
    
    sns.relplot(data=report, x=round(report.dur), y='errors', kind="line") __

![](Emotional_speech_detection_files/figure-html/cell-31-output-1.png)

Average loss values for different emotional arousal levels

Code

    
    
    sns.relplot(data=report, x="EmoAct", y="errors", kind="line", hue='is_active')
    ax = plt.gca()
    ax.axvline(x=5, color='r', linewidth=3)
    ax.set_xticks([1,2,3,4,5,6,7])
    ax.set_xticklabels(['Very Calm', 'Calm', 'Somewhat \n Calm', 'Neutral', 'Somewhat \n Active', 'Active', 'Very \n active'], rotation=30); __

![](Emotional_speech_detection_files/figure-html/cell-32-output-1.png)

Code

    
    
    sns.histplot(data=report, x="EmoAct", y="errors", hue='is_active') __
    
    
    <Axes: xlabel='EmoAct', ylabel='errors'>

![](Emotional_speech_detection_files/figure-html/cell-33-output-2.png)

[ Engineering notes ](../index.html)

  * [ Home](../index.html)
  * [ Proejcts](../projects.html)
  * [ Blog](../blog.html)

## On this page

  * 1\. Splitting the data
  * 2\. Create dataloaders
  * 3\. Create random iterator

# Training with variable length data

Building a dataloader to train deep learning models on variable length data

There are several different ways we can deal with variable length data when
training deep learning models:

  * Cut or pad all the samples to the maximum length in the whole dataset
  * Cut or pad samples to the maximum length within a mini-batch
  * Split the dataset into multiple buckets with samples of similar length.

I will describe the third option as it imposes the least memory and
computational overhead. This option can be used to train CNNs, RNNs or
transformers with relative positional encoding, since they can be trained on
variable length data. For example we can train Wav2Vec 2.0 model with audio
samples of different length as it encodes audio with CNN and is using
convolutional relative positional encoding as well.

![](./images/dl.png)

# 1\. Splitting the data

Generating dataset with variable length items.

    
    
    from random import randint
    import torch
    import pandas as pd
    
    # generate dataset
    min_length = 2
    max_length = 20
    size = 1000
    dummy_y = 0
    
    dataset = [(torch.randn(randint(min_length,max_length)),dummy_y) for _ in range(size)] __

Create a dataframe with information about items length

    
    
    df = pd.DataFrame([(id,len(x)) for id,(x,y) in enumerate(dataset)], columns=['id','length'])
    df.length.plot(kind='hist',title="Length distribution"); __

![](variable_length_training_files/figure-html/cell-3-output-1.png)

Split data into bukets

    
    
    nbuckets=10
    df['bucket'] = pd.cut(df.length, bins=nbuckets, labels=range(nbuckets)) __

# 2\. Create dataloaders

Create DataSet class, which is using a dataframe with items ids to retrieve
them from the original dataset.

    
    
    from torch.utils.data import DataLoader
    from torch.nn.functional import pad
    
    class DataSet:
        def __init__(self,dataframe,data):
            self.df = dataframe.reset_index(drop=True) # items ids
            self.data = data
        
        def __getitem__(self,index):
            id = self.df.iloc[index].id # get item by id from the original dataset
            return self.data[id]
        
        def __len__(self): return len(self.df) __

Collate function adds padding according to the maximum length in a batch

    
    
    def collate_fn(batch):
        xs,ys = [list(b) for b in (zip(*batch))]
        maxl = max([len(x) for x in xs]) # maxl in a batch
        for i in range(len(xs)):
            xs[i] = pad(xs[i],(0,maxl-len(xs[i]))) # pad to maxl
        x = torch.stack(xs)
        y = torch.tensor(ys)
        return (x,y) __

Create dataloaders for each bucket

    
    
    def create_dataloader(dataframe,bs=4): 
        return DataLoader(DataSet(dataframe, dataset), bs, shuffle = True, collate_fn=collate_fn)
    
    dataloaders = []
    for bucket_id in df.bucket.unique():
        dl = create_dataloader(df[df.bucket==bucket_id])
        dataloaders.append(dl) __

# 3\. Create random iterator

The iterator takes iterators from the dataloaders and randomly chooses one at
the each `next` call

    
    
    from random import choice
    
    class DLIterator:
        def __init__(self, dls) -> None:
            self.iters = [iter(dl) for dl in dls]
    
        def __iter__(self): return self
    
        def __next__(self):
                for _ in range(len(self.iters)): # iterate in case some are empty
                    try:
                        it = choice(self.iters)
                        return next(it)
                    except StopIteration:
                        self.iters.remove(it)
                raise StopIteration
    
                
    class MultiDataLoader:
        '''Combining multiple dataloaders.'''
        def __init__(self,dataloaders) -> None:
            self.dls=dataloaders
    
        def __iter__(self):
            return DLIterator(self.dls)
    
        def __len__(self):
            return sum(map(len, self.dls)) __

Check the distribution of batch lengths for the obtained dataloader

    
    
    import matplotlib.pyplot as plt
    
    batch_sizes = [xb.shape[1] for xb,_ in MultiDataLoader(dataloaders)]
    plt.hist(batch_sizes); __

![](variable_length_training_files/figure-html/cell-9-output-1.png)

Visualize batch lengths:

    
    
    it = iter(MultiDataLoader(dataloaders))
    _,ax = plt.subplots(5)
    for i in range(5):
        ax[i].imshow(next(it)[0]) __

![](variable_length_training_files/figure-html/cell-10-output-1.png)

[ Engineering notes ](../index.html)

  * [ Home](../index.html)
  * [ Proejcts](../projects.html)
  * [ Blog](../blog.html)

## On this page

  * Vision transformers
    * Adjusting the embedding layer
      * Computing patch embeddings
      * Adjusting position embeddings
    * Replacing the head
    * Example
  * CNNs

# Applying vision models to audio data

Fine-tuning ViTs and ConvNets on spectrogram data

![](./images/AIclub.png)

Several research have demonstrated that vision models pretrained on large
datasets of images can be successfully applied for audio classification tasks.
Both vision transformers and convolutional neural networks.
![](https://github.com/ViktorAnchutin/umd/blob/main/Screenshot%20from%202023-10-15%2011-08-13.png?raw=true)

# Vision transformers

The are 3 main modules in the vision transformer which are important for
transfer learning:

  * **The embedding module**. Transforms images to a sequence of vector embeddings
  * **Transformer encoder module**. Contains stacked transformer encoder layers.
  * **Head**. Computes the final prediction from the obtained representation vector.

![](./images/adjuststeps.png)

## Adjusting the embedding layer

Embedding layer processing:

  1. Split an image into patches
  2. Compute patch embeddings
  3. Add position encoding

![](https://github.com/ViktorAnchutin/umd/blob/main/image.png?raw=true)

### Computing patch embeddings

To compute an embedding from a patch, vision transformer applies a convolution
operation with 3 input channels and the number of output channels equal to the
specified embedding dimension.

![](./images/patch_embed.png)

Here’s how it is done in the timm library, from the
[timm.layers.PatchEmbed](https://github.com/huggingface/pytorch-image-
models/blob/68b2824e49c8a4442ca1c7fb29d3de8135e87c64/timm/layers/patch_embed.py#L65C9-L65C105)
class:

    
    
    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias) __

When we work with spectrograms we have only 1 channel of information and
that’s why the embeddings layer has to be adjusted.
![](./images/patchembedspctr.png)

The adjustment can be done by copying spectrogram channel 2 times, averaging
or taking a sum of the input channels of the embedding layer.

![](./images/spctrmean.png)

Timm library sums the weights of the convolution channels. From the
_adapt_input_conv_ function in the
[timm.models._manipulate](https://github.com/huggingface/pytorch-image-
models/blob/68b2824e49c8a4442ca1c7fb29d3de8135e87c64/timm/models/_manipulate.py#L267C13-L267C63):

    
    
    conv_weight = conv_weight.sum(dim=1, keepdim=True) __

The same is happening in the [Audio Spectrogram
Transformer](https://github.com/YuanGongND/ast/blob/31088be8a3f6ef96416145c4b8d43c81f99eba7a/src/models/ast_models.py#L86C17-L86C116):

    
    
    new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1)) __

### Adjusting position embeddings

Position embeddings added to encode spatial information about the location of
each patch in the image. Visual transformer models are pretrained on a fixed
size images, which means they learn positional embeddings of a fixed size as
well. To apply it to a different size images or spectrograms, position
embeddings need to be somehow adjusted to transfer the learned relationships
to different resolutions.

![](./images/pos_embed_problem.png)

A popular solution - 2D interpolation.

![](./images/interpolation.png)

From the original [ViT paper](https://arxiv.org/abs/2010.11929), ‘Fine-tuning
and higher resolution’ section:

![](./images/vitpaper.png)

Timm library also implements interpolation. From the github issue
[discussion](https://github.com/huggingface/pytorch-image-models/issues/908):

![](./images/timminterpolation.png)

From the
[timm.layers._pos_embed.resample_abs_pos_embed](https://github.com/huggingface/pytorch-
image-
models/blob/68b2824e49c8a4442ca1c7fb29d3de8135e87c64/timm/layers/pos_embed.py#L46):

    
    
    posemb = F.interpolate(posemb, size=new_size, mode=interpolation, antialias=antialias) __

## Replacing the head

The final adjustment is simply changing the final layer for the specific task.
The head layer in ViT consumes CLS token. So the new head should have the
input dimension equal to the CLS token dimension (or just embeddings
dimension).

![](./images/vithead.png)

* DeiT models have 2 CLS tokens, so there are 2 heads to adjust.

## Example

Here’s how we can get a pretrained ViT model with adjusted embedding layer and
the head layer for further fine-tunung:

    
    
    m = timm.create_model('vit_base_patch16_224', img_size=size, pretrained=True, in_chans=1, num_classes=num_classes) __

# CNNs

For CNNs the process is mostly the same except they can work with variable
size inputs, so we only need to adjust the number of input channels and the
head. When we choose `in_chans = 1` in timm library, under the hood
[adapt_input_conv](https://github.com/huggingface/pytorch-image-
models/blob/68b2824e49c8a4442ca1c7fb29d3de8135e87c64/timm/models/_manipulate.py#L267C13-L267C63)
function is called to adjust input layer as it was shown earlier:

`conv_weight = conv_weight.sum(dim=1, keepdim=True)`

    
    
    m = timm.create_model('convnext_tiny', pretrained=True, in_chans=1, num_classes=num_classes) __

[ Engineering notes ](./index.html)

  * [ Home](./index.html)
  * [ Proejcts](./projects.html)
  * [ Blog](./blog.html)

# Blog

[

![](./blog/images/rdd.png)

#####  Spark RDD with distributed machine learning

Understanding Spark computational model with logistic regression and
clustering  ](./blog/Spark RDD.html)

[

![](./blog/images/Aiclubs.png)

#####  Applying vision models to audio data

Fine-tuning ViTs and ConvNets on spectrogram data
](./blog/Applying_vision_models_to_audio_classification.html)

[

![](./blog/images/gacs.png)

#####  Gradient accumulation

Toy example for gradient accumulation understanding
](./blog/gradient_accumulation.html)

[

![](./blog/images/vld.png)

#####  Training with variable length data

Building a dataloader to train deep learning models on variable length data
](./blog/variable_length_training.html)

No matching items

[ Engineering notes ](../index.html)

  * [ Home](../index.html)
  * [ Proejcts](../projects.html)
  * [ Blog](../blog.html)

## On this page

  * K-Means clustering
    * Data
    * Initialize centroids
    * Define computational graph
    * Running the algorithm
    * Execution analysis
  * Logistic regression
    * Data
    * Model
    * Cost function and gradient
    * Define computational graph
    * Running the training
    * Execution analysis

# Spark RDD with distributed machine learning

Understanding Spark computational model with logistic regression and
clustering

In this blog post I take a close look into the Spark computational model by
implementing 2 machine learning algorithms: Logistic Regression and K-Means
clustering.

The code is executed in the Databricks environment using Scala. Python is used
for visualization.

Databricks [notebook](https://databricks-prod-
cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3756302453323559/2672913114616530/2768646450526944/latest.html)
[![drawing](./images/databricks.jpeg)](https://databricks-prod-
cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3756302453323559/2672913114616530/2768646450526944/latest.html)

* * *

Spark was developed to address iterative big data algorithms like logistic
regression (gradient descent) or k-means clustering. From the RDDs
[paper](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf):

![](./images/sparkmotivation.png)

# K-Means clustering

## Data

For K-means clustering let’s generate 5 clusters of data.

Generate data (Python)

    
    
    import matplotlib.pyplot as plt
    from sklearn.datasets import make_blobs
    import numpy as np
    import csv
    
    n_samples = 10000
    n_features = 2
    n_clusters = 5
    
    data, labels = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=12345)
    
    
    def plot_centroids(centroids,ax):
        for i, centroid in enumerate(centroids):
            samples = data[i*n_samples:(i+1)*n_samples]
            ax.plot(*centroid, markersize=10, marker="x", color='k', mew=5)
            ax.plot(*centroid, markersize=5, marker="x", color='m', mew=2)
    
    _,ax = plt.subplots()
    ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')
    
    centroids = []
    for cluster_label in range(n_clusters):
        cluster_points = data[labels == cluster_label]  # Select data points in the current cluster
        cluster_centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid for the cluster
        centroids.append(cluster_centroid)
    
    plot_centroids(centroids,ax)
    print(centroids)
    
    
    #save data
    with open('data.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerows(data) __
    
    
    [array([ 8.57032532, -3.64137432]), array([-6.34270089, -5.91962725]), array([1.3294662 , 1.87901003]), array([9.31274039, 3.05338878]), array([4.96798439, 3.09705577])]

![](Spark RDD_files/figure-html/cell-2-output-2.png)

Let’s read the data and create an RDD of data points,

Also important to make sure that RDD of data points will be cached, so we
don’t need to recompute it for each iteration.

    
    
    import scala.io.Source
    
    def getPointsRDD(): RDD[(Float,Float)] = {
        val source = Source.fromFile("data.csv")
        val linesRDD = sc.parallelize(source.getLines().toList)
        return linesRDD.map(_.split(',').map(_.toFloat)).map({case Array(x,y) => (x,y)})
    }
    val points = getPointsRDD().cache() __

We can see that data points were split by Spark in 8 partitions:

    
    
    points.getNumPartitions __
    
    
    res5: Int = 8

## Initialize centroids

The first step of k-means clustering is to initialize the first estimates of
the centroids. For this example I randomly sample 5 points, but in real
applications this initialization step usually involves more sophisticated
sampling.

    
    
    val randomMeans = points.takeSample(withReplacement=false, num=5, seed=10) __

save centroids for plotting

    
    
    import java.io.PrintWriter
    
    def saveResult(means: Array[(Float,Float)]) = {
        val writer = new PrintWriter("means.txt")
        try {
            means.foreach(writer.println)
        } finally {
            writer.close()
        }
    }
    
    saveResult(randomMeans) __

Plot centroids (Python)

    
    
    # Open the file in read mode
    def readResults()->list:
        with open('means.txt', 'r') as file:
            lines = file.readlines()
        return [tuple(map(float, line.strip('()\n').split(','))) for line in lines]
    
    def plot_results(centroids,ax):
        for i, centroid in enumerate(centroids):
            samples = data[i*n_samples:(i+1)*n_samples]
            ax.plot(*centroid, markersize=5, marker="*", color='r', mew=5)
    
    first_state = readResults()
    _,ax = plt.subplots()
    plot_results(first_state,ax)
    plot_centroids(centroids,ax)
    ax.set_title('Initial centroids against true centroids') __
    
    
    Out[5]: Text(0.5, 1.0, 'Initial centroids against true centrids')

![](Spark RDD_files/figure-html/cell-3-output-2.png)

## Define computational graph

Let’s implement k-means clustering.

The main function `update` \- conceptually performs 2 steps:

  1. Groups the points by the closest centroids
  2. Finds the centre of the groups, effectively obtaining the new estimate for centroids

    
    
    def euclideanDistance(v1: (Float, Float), v2: (Float, Float)): Double =
        (v1._1 - v2._1) * (v1._1 - v2._1) + (v1._2 - v2._2) * (v1._2 - v2._2)
    
      /** Return the center that is the closest to `p` */
    def findClosest(p: (Float, Float), centers: Array[(Float, Float)]): (Float, Float) =
        centers.minBy(euclideanDistance(_,p))
    
    def updateMeans(means: Array[(Float,Float)], points: RDD[(Float,Float)]): Array[(Float,Float)] =
      return points
                .map(point => (findClosest(point,means),point)) // pair (closest mean, point)
                .mapValues(point => (point,1))  // add counter for aggregation
                .reduceByKey({case ((p1,cnt1),(p2,cnt2)) => ((p1._1 + p2._1,p1._2+p2._2),cnt1+cnt2)}) // sum all the points around a centroid 
                .mapValues({case (sum,count) => (sum._1/count, sum._2/count)}) // average aggregated points -> new centroid 
                .map({case (oldMean, newMean) => newMean})
                .collect() __

## Running the algorithm

Let’s run 10 iterations and look at the result.

    
    
    var means = randomMeans
    for(i <- 0 to 10){
      means = updateMeans(means,points) 
    } __

save results for plotting

    
    
    saveResult(means) __

The algorithm successfully found true centroids of clusters.

Plot results (Python)

    
    
    results = readResults()
    _,ax = plt.subplots(1,3,figsize=(10,3))
    plot_results(results,ax[0])
    plot_centroids(centroids,ax[0])
    plot_results(results,ax[1])
    plot_centroids(centroids,ax[2])
    ax[0].set_title('True centroids and estimations')
    ax[1].set_title('Estimations')
    ax[2].set_title('True centroids') __
    
    
    Out[5]: Text(0.5, 1.0, 'True centroids')

![](Spark RDD_files/figure-html/cell-4-output-2.png)

## Execution analysis

First, Spark builds a graph of computations and only when we call `action`
functions such as .collect() it executes the graph.

Let’s look at the diagram of the execution process.

![](./images/kmns.png)

Spark driver creates closures with centroids and sends them to the executors.
Executors apply closures received by the driver to the partitions.

From the spark
[paper](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf):
_“..users provide arguments to RDD operations like map by passing closures
(function literals). Scala represents each closure as a Java object, and these
objects can be serialized and loaded on another node to pass the closure
across the network. Scala also saves any variables bound in the closure as
fields in the Java object.”_

First each partition of points is transformed to the pairs of points and the
corresponding closest centroid.

Then we have reduceByKey followed by shuffle and the average. It is
conceptually the same as grouping the points by key and taking the average,
but computationally more optimal. If we used groupByKey, then the shuffle
operation would have to send 10000 points over the network in the worst case.
With reduceByKey operation, reduction happens before shuffle occurs,
significantly reducing the amount of data to send. In this case for each
cluster data points are reduced to a single pair of sum and counts, which
means that at most 5 (number of clusters) * 8 (number of partitions) = 40
pairs would need to be sent over the network.

The obtained centroids are then sent back to the driver.

Next iteration driver sends updated centroids back to the executors for
recomputation.

# Logistic regression

## Data

Let’s generate data for binary classification problem

generate data (Python)

    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.datasets import make_classification
    
    
    X, y = make_classification(n_samples=10000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=123)
    
    # Plot the dataset
    plt.figure(figsize=(8, 6))
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', marker='o', edgecolors='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2') __
    
    
    Out[15]: Text(0, 0.5, 'Feature 2')

![](Spark RDD_files/figure-html/cell-5-output-2.png)

save data (Python)

    
    
    import numpy as np
    data = np.column_stack((X.astype(str), y.astype(str)))
    np.savetxt('classification_data.txt', data, fmt='%s') __

reading data

    
    
    import scala.io.Source
    
    case class Point(coordinates:List[Double], label:Int)
    
    def readData(): List[Point] = {
        val filePath = "classification_data.txt"
        val lines = Source.fromFile(filePath).getLines().toList
        val points: List[Point] = lines.map { line =>
            val fields = line.split("\\s+")
            val x = fields.init.map(_.toDouble).toList
            val y = fields.last.toInt
            Point(x, y)
        }
        return points
    }
    
    val points = readData() __

## Model

For a 2 dimensional problem binary classification model would look like this:

\\(p = \sigma(\beta_0 + \beta_1*x_1 + \beta_2*x_2)\\)

where p is the probability of a data point to belong to class 1, \\(\beta_i\\)
\- learnable model parameters

We can implement it in Scala in the following way:

    
    
    import math.exp
    
    def linear(x: List[Double], beta: List[Double]): Double =
        return (1.0 :: x).zip(beta).map({case (a, b) => a * b}).reduce(_ + _) // dot product x*beta, append 1 for beta_0
    
    def sigmoid(x: Double): Double =
        1 / (1 + math.exp(-x).toFloat)
    
    def model(x: List[Double], beta: List[Double]): Double =
        sigmoid(linear(x, beta)) __

## Cost function and gradient

We need to minimize the cost function \\(J\\):

\\(J = - \frac{1}{N} * \sum_{k=0}^{N}[y_k*log(p_k) + (1-y_k)*log(1-p_k)]\\)

where \\(N\\) \- total number of points

And the corresponding partial derivative with respect to each parameter is:

\\(\frac{dJ}{d{\beta}_i} = \frac{1}{N} * \sum_{k=0}^{N} x_i*(p_k - y_k)\\)

For gradient descent we need to compute gradient at each iteration and update
parameters.

Implementation of the gradient \\([\frac{dJ}{d{{\beta}}_0},
\frac{dJ}{d{{\beta}}_1}, \frac{dJ}{d{\beta}_2}]\\) computation and loss
function:

    
    
    def partial_derivative(y:Int,pred:Double,x:Double): Double = (pred-y)*x    
    
    def gradient(xs:List[Double], pred:Double, y:Int): List[Double] = {
        (1.0::xs).map(x => partial_derivative(y,pred,x)) // add 1 to xs to acount for b0
    }
    
    def loss(pred:Double,y:Int): Double = {
        -(y*log(pred) + (1-y)*log(1-pred))
    } __

## Define computational graph

Let’s define our computation graph for one iteration of logistic regression.
We want to compute gradients for all data points and find their average. For
monitoring also let’s compute loss for each iteration as well.

    
    
    import math.log
    
    /* Sum gradients and loss values */
    def sum(a:(List[Double],Double,Int), b:(List[Double],Double,Int)): (List[Double],Double,Int) = {
        val (grad1,loss1,cnt1) = a
        val (grad2,loss2,cnt2) = b
        val grad_sum = grad1.zip(grad2).map({case (g1,g2) => g1+g2})
        val loss_sum = loss1+loss2
        val count = cnt1 + cnt2
        (grad_sum,loss_sum,count)
    }
    
    /*Compute average for gradients and the loss value*/
    def average(grad_sum:List[Double], loss_sum:Double,counts:Int): (List[Double],Double) = {
        (grad_sum.map(_/counts),loss_sum/counts)
    }
    
    /*Compute gradient and loss value*/
    def compute(points:RDD[Point],params:List[Double]): (List[Double],Double,Int) = 
        points.map({case Point(xs,y) => (Point(xs,y), model(xs,params))}) // get predictions
        .map({case (Point(xs,y),pred) => (gradient(xs,pred,y),loss(pred,y),1)}) // compute gradient and loss
        .reduce(sum) __

## Running the training

Running the training. 10 iterations. Compute average gradient and loss for
each iteration and perform gradient descent step

\\(parameters_n = parameters_{n-1} - \alpha * \nabla J\\)

    
    
    val pointsRDD = sc.parallelize(points)
    
    var parameters = List(0.1,0.1,0.1) // b0,b1,b2 random
    
    val step_size = 3
    for(i <- 0 to 10){
        val (gradSum,lossSum,cnt) = compute(pointsRDD,parameters)
        val (grad,loss) = average(gradSum,lossSum,cnt)
        parameters = parameters.zip(grad).map({case (param,g) => param - step_size*g})
        println(loss)
    } __

loss output

    
    
    0.7985616355198335
    0.07500786607886242
    0.06578866845350452
    0.05999996600024983
    0.05603349371988779
    0.05315514097244599
    0.05097938292867146
    0.04928355967304257
    0.04792987072323888
    0.046828371238424885
    0.04591785644633872

Computing accuracy:

    
    
    val accuracy = pointsRDD.map(point => (point.label,model(point.coordinates,parameters)))
    .map({case (y,prediction) => if( (prediction > 0.5) == (y==1) ) 1 else 0})
    .mean __
    
    
    accuracy = 0.9945

Let’s plot the decision boundary for the obtained model:

save results

    
    
      import java.io.PrintWriter
    
      def saveParameters(parameters: List[Double]) = {
          new PrintWriter("parameters.txt") {
          // Iterate through the array and write each element to the file
          parameters.foreach(println)
          close()
        }
      }
    
      saveParameters(parameters) __

plot results (Python)

    
    
    with open('parameters.txt', 'r') as file:
        parameters = [float(line.strip()) for line in file]
    
    def get_decision_line(b0,b1,b2):
        c = -b0/b2
        m = -b1/b2
        return lambda x: m*x + c
    
    xs = np.linspace(-5,5)
    ys = get_decision_line(*parameters)(xs)
    
    plt.figure(figsize=(8, 6))
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', marker='o', edgecolors='k')
    plt.plot(xs, ys) __
    
    
    Out[17]: [<matplotlib.lines.Line2D at 0x7eff0c05adf0>]

![](Spark RDD_files/figure-html/cell-7-output-2.png)

## Execution analysis

Logistic regression iteration doesn’t require shuffle operation. We simply
apply the sequence of `map` operations to compute gradients and losses for
each point and then do `reduce` to aggregate. The final step of the iteration
happens on the driver. The Driver receives aggregated gradients and losses
from the executors, computes the average and performs gradient descent update
step. The updated parameters are then sent to the executors for the next
iteration.

![](./images/logreg.png)

